---
title: Last chance to be. 
description: TK
growthStage: stub
tags:
  - ai
  - culture
  - tech
cover_image: 
featured: false
---

It's 2026 now and the challenge is to fail at what the mechanisms of power desire of you. Everywhere we are asked to be incurious, uncreative, forgiving of the powerful, and cruel to the powerless. 

There is pressure at almost every digital surface of interaction to step back into Plato's cave, where you can chain yourself to the wall and amuse yourself only with shadows of reality. 

I ask you to consider an alternative. 

We must prioritize humans, our community, the people around us and what they create. There's no future in shadows. There are no choices that can be made when chained to cave wall. 

We still have to function in the here and now, acknowledging that is just one part of taking the real world head on. 

We can only take on the politics and issues we're capable of. Right now, in the field of technology, an issue I feel capable of talking about is the hollow homunculus of AI produced by companies like OpenAI and popularized as AI, [even though AI isn't real and we're no where near it being real](https://context.center/topics/generative-ai-and-chatgpt/#does-ai-exist-no.). 

{% image 'src/images/star-trek-its-a-fake.gif' "Romulan in Star Trek holding a data drive up saying it's a fake vehemently" 'in-post-gif center-in-text' %}

It is a signal of our times that AI is as popular as it is right now. Everywhere we're being pushed into watching and interacting with shadows, it's no surprise that the most popular product on the market is a shadows generator. 

Large Models and "Neural" Networks are fundamentally tools for finding similarities in big data sets. This makes them both useful and interesting for a variety of tasks. They are also able to [trace their current iterations and even origins to defense research](https://youtu.be/A0X4O49cY4o). The foundations of such technologies are fundamentally compromised in ways that we cannot really conceive of until the biases of the design realize themselves into reality as they do all too often now, facial recognition being the most common example. 

Such systems are being rendered even worse by the instance that there's some sort of future in pumping ever more energy, resources, and compute into them. This seems likely to produce ever diminishing returns, [has a significant environmental impact](https://context.center/topics/generative-ai-and-chatgpt/#ai-and-climate), and will absolutely not solve the nature of these systems to produce errors. At the core of ever LLM-style system is essentially an engine for finding similarities to whatever you feed into it by basically (very very basic explanation incoming) taking your input, turning it into a cluster of dots on a graph populated by every other piece of content entered into the model and turned into dots, and then trying to find the dot closest to your cluster of dots. This is the fundamental function of the software. It will never stop "hallucinating". Error rates can go down, but they can never **ever** be eliminated with the shape of the tools that have been popularized. 

We will burn the planet to a cinder before we ever eliminate the fundamental unpredictability of these systems. Which calls into question if any of this massive market-deforming paradise-paving data-center construction is worth it; even if you were ok with disposition of land from communities, a less stable power grid, and *some significant environmental impact*, the most aggressive techno optimist should only want to do all of that if it **delivers** *something* instead of the very likely chance that it will accomplish statistically insignificant improvements. 

I mean the goal here isn't just to fuck over communities of poor and less-white people right? Oh wait... according to Wikipedia one of the inspirations of the a16z techno-optimist manifesto is a writer who promotes:

> "scientific racism and eugenics, or what he has called "hyper-racism"

[Maybe it is](/glossary/tescreal/) huh? 

Well, we're getting away from the point, which is AI is here, it is fucked up, it is [inherently political](/noteworthy/defining-ai-as-a-political-project/) and, while it is not [inevitable](/glossary/inevitabilism/), it is currently unavoidable. There's a lot of reasons why you should try and avoid it as much as possible anyway. 

If you are a successful AI resister, that's great. Stop reading here and we'll 100% be in agreement. 

## Good AI--or: What can we do about a cave that's built around us? 

I don't think we can avoid it though. Here's the thing: my search doesn't work anymore. Even using search engines that avoid or allow deactivation of AI like DuckDuckGo there's no avoiding the endless slop that is being layered on to the internet, faster than we can remove it from any search algorithm or database. I like searching Stack Overflow for answers about code, especially because people talk about the choices they make there, but it is harder to search for answers there. It is harder to search GitHub for examples. It is harder to search for blog posts by people doing the things I'm trying to do and who could give me advice. My spellcheck works worse. My autocomplete is crazy. My music recommendations keep trying to route me to some bullshit AI artist. My YouTube algo is funky. On the rare occasions I log into Facebook to communicate with family my feed is slop-city. Everything that once worked is either being deformed or destroyed to force you into using a chatbot. 

Like everything that happens in technology in the modern day, our hand is being forced. The way we interacted with the internet is undergoing a sort of forced planned obsolescence. I think we can build our way out of it, good intentioned people, indieweb folks, and practical decentralization are collectively a pathway out of the city and [into the woods](/noteworthy/we-need-to-rewild-the-internet/). Between then and now is a gap, and the web is being destroyed by the big tech overlords some thought we could trust it with, intentionally, as part of a cascading series of dark patterns and manipulations intended to force you into using "AI." They [want to wear you down](https://bsky.app/profile/did:plc:t5xmf33p5kqgkbznx22p7d7g/post/3ma55nrzfo22u), and they have a pretty good track record of succeeding. 

So maybe there is a reason to look for "[Good AI](https://www.anildash.com/2025/11/14/wanting-not-to-want-ai/)". 

[What would that even be though](https://bsky.app/profile/chronotope.aramzs.xyz/post/3mb6t36mmwc2b)?

I think there's a reasonable argument to be made that "Good AI" already exists. It's the useful things that we've been using LLMs for all along. Dealing with finding patterns in big data. Experts using it to extrapolate patterns to research further. Big approximations of huge patterns that can be verified by humans. Normal algorithm stuff. 

But that's not what this is all about is it? The entire information landscape is being defored to force us to use fucking chatbots. So what does a "good" chatbot look like? 

It would be nice if it is built on top of consented data, in community server farms powered by solar, that contribute to and are controlled by the communities they sit in. Networked small server farms that are the silicon equivalent of "Community Supported Agriculture"--Community Supported Compute farms and using only completely excess water. All the externalities imagined gone. 

Like the re-wilded indie web, I think such a thing is possible, but we've got a ways to go until we get there. Let's be honest, you won't be using a CSC Farm network for your chatbot training any time soon. I wish. We're also going to end up using the same models that are out there. Training a model is expensive and I think that this is a used car scenario, better to use a bad thing that's already finished then re-do all the work in a better way.

It's going to use stolen data from the foundation model level. I hope we can get to a real useful foundation model built purely on permitted data but you'll need all the foundational stuff first. You can't train a model without the compute and those CSC Farms still aren't in my neighborhood. As much as server management should be a community skill, we ain't there yet. So it's a chatbot built on bad data, bad ecological impact, and a long history of being used to bomb people--badly.

What do you use it for? That's the big question. 

The question isn't can we have Good AI then, at least not right now. It's can we have *good AI users*. What are the affordances of a system built this way and what can we do to shape such a product to force good design and allow for healthy use (both for the individual and the community)? 

## Who to be when you're being AI-assisted 






I imagine search, especially with your personal data, but also the web. 
