---
title: C2PA is just the beginning. We can't yet see the end. 
tags:
  - Notable Articles
  - ai
  - media
  - journalism
  - misinformation
  - disinformation
date: '2026-02-23T18:36:11.000-05:00'
cite:
  name: Does Big Tech actually care about fighting AI slop?
  author: Jess Weatherbed
  href: >-
    https://www.theverge.com/ai-artificial-intelligence/882956/ai-deepfake-detection-labels-c2pa-instagram-youtube

---

This great piece doesn't even get into the biggest problem with C2PA, which is more images and videos we see will start with legit photos or videos with all the right provenance markers and pass through AI software at some point for both light touch and total transforms

C2PA theoretically provides signal on *what* the changes are, but in reality the message is pretty similar if it is a request to an AI to correct lighting in a shot or a more substantial misleading transform. Especially considering tools to create total fraud images are now built in to professional tools. Adobe, as one example, ships AI-transformation tools in its professional software that can create massively different images or simple touch-ups with basically the same action. 

As is correctly identified in this article and others, C2PA is a system concerned with trustworthiness and provenance, but it started development a long time ago, far before any of these problems were even on the horizon. It really is struggling with the informational-landscape concerns that AI brings to the table.

It is also entirely unable to attack another major issue, which is that sometimes the photo metadata is *dangerous* to the photographer, especially in the most sensitive situations. The people leaking data, pushing out front-line reports from protests and wars, some times you don't want metadata on.

The real problem can only be solved by AI systems stamping their images with provenance, but the nature of how these systems are made to be reused by a million customers who are apps reselling their capabilities means that this is almost impossible to enforce.

This also doesn't address one of the core problems: C2PA arose in a time where we were having a hard time determining fraud because bad actors were sharing real images without clear information about where they came from. The solution at the time was trustworthy messaging from known gatekeepers for news. But fewer people trust mainstream journalists now than ever. All the fancy digital metadata in the world cannot defeat this problem where the sources are not trusted by the audience, especially at times when they must enact confidentiality, which are often the moments it is most important to get this right. 

> Even if more synthetic content is embedded with C2PA information, everyday people are still largely expected to manually hunt for it themselves across the images and videos they see online, despite many not even being aware that C2PA exists. If anything, it seems like AI providers are using C2PA to distance themselves from the problem, while continuing work on their own slop factories.

It's hard to see how even the best intentions here could give us something better than real, significant, guardrails at the level of the image or video generator. The AI systems really need to do more work at a deeper level, but it doesn't seem like they are willing.

> In fact, anything that ensures synthetic materials won’t be mistaken for something human-made goes against the business interests of every company that’s throwing money into AI, especially if it paints the technology in a bad light. How much responsibility can you really take with such a conflict of interest?
