---
author: Robert Booth
cover_image: >-
  https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=e97802cd7a088f7e81071845389a5302
date: '2024-12-06T18:21:07.000Z'
dateFolder: 2024/12/06
description: >-
  A biased AI system used by the UK government to detect welfare fraud has been
  found to unfairly target people based on their age, disability, marital
  status, and nationality. Critics argue that this leads to a “hurt first, fix
  later” approach, risking harm to marginalized groups. The government has been
  urged to improve transparency and assess the fairness of these automated
  systems.
isBasedOn: >-
  https://www.theguardian.com/society/2024/dec/06/revealed-bias-found-in-ai-system-used-to-detect-uk-benefits
link: >-
  https://www.theguardian.com/society/2024/dec/06/revealed-bias-found-in-ai-system-used-to-detect-uk-benefits
slug: >-
  2024-12-06-httpswwwtheguardiancomsociety2024dec06revealed-bias-found-in-ai-system-used-to-detect-uk-benefits
tags:
  - ai
  - law and order
title: 'Revealed: Bias Found in AI System Used to Detect UK Benefits Fraud'
---
<figure><picture><source media="(min-width: 980px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 980px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 980px)" srcset="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 740px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 740px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=700&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 740px)" srcset="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=700&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 660px)" srcset="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=645&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 480px)" srcset="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=645&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=465&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 320px)" srcset="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=465&amp;dpr=1&amp;s=none&amp;crop=none"/><img alt="Person walks past a job centre in Borough, London" src="https://i.guim.co.uk/img/media/52d3c80c0959c656da707331b163c217d7f9b869/0_183_5500_3300/master/5500.jpg?width=465&amp;dpr=1&amp;s=none&amp;crop=none"/></picture><figcaption data-spacefinder-role="inline">Claims for advances on universal credit payments are being examined by a biased AI system designed to detect fraud, it has emerged. Photograph: Mina Kim/Reuters</figcaption></figure>
<h2 data-gu-name="standfirst">Exclusive: Age, disability, marital status and nationality influence decisions to investigate claims, prompting fears of ‘hurt first, fix later’ approach</h2>
<p>An artificial intelligence system used by the UK government to detect welfare fraud is showing bias according to people’s age, disability, marital status and nationality, the Guardian can reveal.</p>
<p>An internal assessment of a <a data-link-name="in body link" href="https://www.theguardian.com/society/2023/jul/11/use-of-artificial-intelligence-widened-to-assess-universal-credit-applications-and-tackle">machine-learning programme used to vet thousands of claims for universal credit payments across England</a> found it incorrectly selected people from some groups more than others when recommending whom to investigate for possible fraud.</p>
<p>The admission was made in <a data-link-name="in body link" href="https://www.whatdotheyknow.com/request/ai_strategy_information/response/2748592/attach/6/Advances%20Fairness%20Analysis%20February%2024%20redacted%201.pdf?cookie_passthrough=1">documents released</a> under the Freedom of Information Act by the Department for Work and Pensions (DWP). The “statistically significant outcome disparity” emerged in a “fairness analysis” of the automated system for universal credit advances carried out in February this year.</p>
<p>The emergence of the bias comes after the DWP this summer claimed the AI system “does not present any immediate concerns of discrimination, unfair treatment or detrimental impact on customers”.</p>
<p>This assurance came in part because the final decision on whether a person gets a welfare payment is still made by a human, and officials believe the continued use of the system – which is attempting to help cut an estimated £8bn a year lost in fraud and error – is “reasonable and proportionate”.</p>
<p>But no fairness analysis has yet been undertaken in respect of potential bias centring on race, sex, sexual orientation and religion, or pregnancy, maternity and gender reassignment status, the disclosures reveal.</p>
<p>Campaigners responded by accusing the government of a “hurt first, fix later” policy and called on ministers to be more open about which groups were likely to be wrongly suspected by the algorithm of trying to cheat the system.</p>
<p>“It is clear that in a vast majority of cases the DWP did not assess whether their automated processes risked unfairly targeting marginalised groups,” said Caroline Selman, senior research fellow at the Public Law Project, which first obtained the analysis.</p>
<p>“DWP must put an end to this ‘hurt first, fix later’ approach and stop rolling out tools when it is not able to properly understand the risk of harm they represent.”</p>
<p>The acknowledgement of disparities in how the automated system assesses fraud risks is also likely to increase scrutiny of the rapidly expanding government use of AI systems and fuel calls for greater transparency.</p>
<p>By one independent count, there are <a data-link-name="in body link" href="https://publiclawproject.org.uk/resources/the-tracking-automated-government-register/">at least 55 automated tools being used by public authorities in the UK</a> potentially affecting decisions about millions of people, although the government’s own register includes only nine.</p>
<p>Last month, the Guardian <a data-link-name="in body link" href="https://www.theguardian.com/technology/2024/nov/28/uk-government-failing-to-list-use-of-ai-on-mandatory-register">revealed</a> that not a single Whitehall department had registered the use of AI systems since the government said <a data-link-name="in body link" href="https://www.gov.uk/government/consultations/ai-regulation-a-pro-innovation-approach-policy-proposals/outcome/a-pro-innovation-approach-to-ai-regulation-government-response">it would become mandatory</a> earlier this year.</p>
<p>Records show public bodies have awarded dozens of contracts for AI and algorithmic services. A contract for facial recognition software, worth up to £20m, was put up for grabs last month by a police procurement body set up by the <a data-link-name="in body link" href="https://www.theguardian.com/politics/home-office">Home Office</a>, reigniting concerns about “mass biometric surveillance”.</p>
<p>Peter Kyle, the secretary of state for science and technology, has previously <a data-link-name="in body link" href="https://www.theguardian.com/technology/2024/nov/28/uk-government-failing-to-list-use-of-ai-on-mandatory-register">told the Guardian</a> that the public sector “hasn’t taken seriously enough the need to be transparent in the way that the government uses algorithms”.</p>
<p>Government departments, including the Home Office and the DWP have, in recent years, been reluctant to disclose more about their use of AI, citing concerns that to do so could allow bad actors to manipulate systems.</p>
<p>It is not clear which age groups are more likely to be wrongly targeted for fraud checks by the algorithm, as the DWP redacted that part of the fairness analysis.</p>
<p>Neither did it reveal whether disabled people are more or less likely to be wrongly singled out for investigation by the algorithm than non-disabled people, or the difference between the way the algorithm treats different nationalities. Officials said this was to prevent fraudsters gaming the system.</p>
<p>A DWP spokesperson said: “Our AI tool does not replace human judgment, and a caseworker will always look at all available information to make a decision. We are taking bold and decisive action to tackle benefit fraud – our fraud and error bill will enable more efficient and effective investigations to identify criminals exploiting the benefits system faster.”</p>
