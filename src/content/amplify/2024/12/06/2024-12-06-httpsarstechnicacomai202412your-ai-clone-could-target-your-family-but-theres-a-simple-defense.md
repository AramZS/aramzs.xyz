---
author: Benj Edwards
cover_image: >-
  https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-1152x648.jpg
date: '2024-12-06T23:41:42.000Z'
dateFolder: 2024/12/06
description: >-
  The FBI advises families to create a secret password to protect against AI
  voice cloning scams. This password can help verify identity during suspicious
  calls that may impersonate loved ones. Criminals are increasingly using AI to
  create realistic voice clones and deceptive content, making this precaution
  necessary.
isBasedOn: >-
  https://arstechnica.com/ai/2024/12/your-ai-clone-could-target-your-family-but-theres-a-simple-defense/
link: >-
  https://arstechnica.com/ai/2024/12/your-ai-clone-could-target-your-family-but-theres-a-simple-defense/
slug: >-
  2024-12-06-httpsarstechnicacomai202412your-ai-clone-could-target-your-family-but-theres-a-simple-defense
tags:
  - ai
  - tech
title: FBI recommends secret passwords to thwart AI voice clones
---
<figure></figure><p> <em> Klaatu Barada Nikto </em></p>
<p>The FBI now recommends choosing a secret password to thwart AI voice clones from tricking people.</p>
<p><a href="https://arstechnica.com/author/benjedwards/"> Benj Edwards </a> –  Dec 6, 2024 7:22 PM |</p>
<figure><a data-cropped="true" data-pswp-height="675" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot.jpg 1200w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-980x551.jpg 980w" data-pswp-width="1200" href="https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot.jpg"><img alt="Man looks through letter flap" sizes="(max-width: 1152px) 100vw, 1152px" src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot.jpg" srcset="https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2024/12/peeping_through_door_slot.jpg 1200w"/></a><figcaption>/ Credit:GSO Images via Getty Images</figcaption></figure>
<figure></figure><figure></figure><p>On Tuesday, the US Federal Bureau of Investigation <a href="https://www.ic3.gov/PSA/2024/PSA241203">advised</a> Americans to share a secret word or phrase with their family members to protect against AI-powered voice-cloning scams, as criminals <a href="https://arstechnica.com/tech-policy/2023/03/rising-scams-use-ai-to-mimic-voices-of-loved-ones-in-financial-distress/">increasingly use voice synthesis</a> to impersonate loved ones in crisis.</p>
<p>"Create a secret word or phrase with your family to verify their identity," wrote the FBI in an official public service announcement (<a href="https://www.ic3.gov/PSA/2024/PSA241203">I-120324-PSA</a>).</p>
<p>For example, you could tell your parents, children, or spouse to ask for a word or phrase to verify your identity if something seems suspicious, such as "The sparrow flies at midnight," "Greg is the king of burritos," or simply "flibbertigibbet." (As fun as these sound, your password should be secret and not the same as these.)</p>
<p>The bureau also recommends that people listen carefully to the tone and word choices in unexpected calls claiming to be from family members. The FBI reports that criminals use AI-generated audio to create convincing voice clips of relatives pleading for emergency financial help or ransom payments.</p>
<p>The recommendation comes as part of a broader service announcement detailing how criminal groups now use generative AI models in their fraud operations, which we've <a href="https://arstechnica.com/ai/2024/10/deepfake-lovers-swindle-victims-out-of-46m-in-hong-kong-ai-scam/">reported on</a> in the past. AI technology now makes creating <a href="https://arstechnica.com/information-technology/2023/01/microsofts-new-ai-can-simulate-anyones-voice-with-3-seconds-of-audio/">realistic voice clones</a> trivial.</p>
<p>It's worth noting that these types of fraudulent clones typically rely on having samples of your speaking voice publicly available (such as in a podcast or recorded interview), so if you aren't a semi-public figure, it's far less likely your voice will be cloned.</p>
<p>The warning extends beyond voice scams. The FBI announcement details how criminals also use AI models to generate convincing profile photos, identification documents, and chatbots embedded in fraudulent websites. These tools automate the creation of deceptive content while reducing previously obvious signs of humans behind the scams, like poor grammar or obviously fake photos.</p>
<p>Much like we <a href="https://arstechnica.com/information-technology/2022/12/thanks-to-ai-its-probably-time-to-take-your-photos-off-the-internet/">warned way back in 2022</a> in a piece about life-wrecking deepfakes based on publicly available photos, the FBI also recommends limiting public access to recordings of your voice and images online. The bureau suggests making social media accounts private and restricting followers to known contacts.</p>
<h2>Origin of the secret word in AI</h2>
<p>To our knowledge, we can trace the first appearance of the secret word in the context of modern AI voice synthesis and deepfakes back to an AI developer named Asara Near, who <a href="https://x.com/nearcyan/status/1640447061035307008">first announced</a> the idea on Twitter on March 27, 2023.</p>
<p>"(I)t may be useful to establish a 'proof of humanity' word, which your trusted contacts can ask you for," Near wrote. "(I)n case they get a strange and urgent voice or video call from you this can help assure them they are actually speaking with you, and not a deepfaked/deepcloned version of you."</p>
<p>Since then, the idea has spread widely. In February, Rachel Metz <a href="https://www.bloomberg.com/news/newsletters/2024-02-14/researchers-embrace-a-low-tech-solution-for-ai-extortion-attempts">covered the topic</a> for Bloomberg, writing, "The idea is becoming common in the AI research community, one founder told me. It’s also simple and free."</p>
<p>Of course, passwords have been used <a href="https://en.wikipedia.org/wiki/Password">since ancient times</a> to verify someone's identity, and it seems likely some science fiction story has dealt with the issue of passwords and robot clones in the past. It's interesting that, in this new age of high-tech AI identity fraud, this ancient invention—a special word or phrase known to few—can still prove so useful.</p>
