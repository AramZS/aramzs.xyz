---
author: Renee DiResta
cover_image: >-
  https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/191:100/w_1280,c_limit/Ideas_Art_Trump-Algorithums.jpg
date: '2019-07-12T16:35:49.000Z'
dateFolder: 2019/07/12
description: >-
  The algorithms that determine what we see online influence our information
  consumption and can spread misinformation. Politicians often mischaracterize
  these algorithms as censorship, distracting from the need to fix them. We
  should focus on increasing transparency and understanding of algorithms to
  address their negative impacts on society.
isBasedOn: 'https://www.wired.com/story/free-speech-is-not-the-same-as-free-reach/'
link: 'https://www.wired.com/story/free-speech-is-not-the-same-as-free-reach/'
slug: 2019-07-12-httpswwwwiredcomstoryfree-speech-is-not-the-same-as-free-reach
tags:
  - favorite
title: Free Speech Is Not the Same As Free Reach
---
<h2>Bad faith politicking about the way search algorithms work makes it harder for tech companies to solve the real problems.</h2>
<figure><picture><source media="(max-width: 767px)" sizes="100vw" srcset="https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_120,c_limit/Ideas_Art_Trump-Algorithums.jpg 120w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_240,c_limit/Ideas_Art_Trump-Algorithums.jpg 240w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_320,c_limit/Ideas_Art_Trump-Algorithums.jpg 320w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_640,c_limit/Ideas_Art_Trump-Algorithums.jpg 640w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_960,c_limit/Ideas_Art_Trump-Algorithums.jpg 960w"/><source media="(min-width: 768px)" sizes="100vw" srcset="https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_120,c_limit/Ideas_Art_Trump-Algorithums.jpg 120w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_240,c_limit/Ideas_Art_Trump-Algorithums.jpg 240w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_320,c_limit/Ideas_Art_Trump-Algorithums.jpg 320w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_640,c_limit/Ideas_Art_Trump-Algorithums.jpg 640w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_960,c_limit/Ideas_Art_Trump-Algorithums.jpg 960w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_1280,c_limit/Ideas_Art_Trump-Algorithums.jpg 1280w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_1600,c_limit/Ideas_Art_Trump-Algorithums.jpg 1600w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_1920,c_limit/Ideas_Art_Trump-Algorithums.jpg 1920w, https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_2240,c_limit/Ideas_Art_Trump-Algorithums.jpg 2240w"/><img alt="Image may contain Human Crowd Audience Person Speech Coat Suit Clothing Overcoat Apparel Flag and Symbol" src="https://media.wired.com/photos/5b8858f418f4c336bb6dd4a7/master/w_2560%2Cc_limit/Ideas_Art_Trump-Algorithums.jpg"/></picture><figcaption>Jabin Botsford/The Washington Post/Getty Images</figcaption></figure>
<p>The algorithms that govern how we find information online are once again in the news—but you have to squint to find them.</p>
<p>“Trump Accuses Google of Burying Conservative News in Search Results,” <a href="https://www.nytimes.com/2018/08/28/business/media/google-trump-news-results.html?action=click&amp;module=Top%20Stories&amp;pgtype=Homepage">reads</a> an August 28 <em>New York Times</em> headline. The piece features a bombastic president, a <a data-event-click='{"element":"ExternalLink","outgoingURL":"https://twitter.com/realDonaldTrump/status/1034456273306243076"}' data-offer-url="https://twitter.com/realDonaldTrump/status/1034456273306243076" href="https://twitter.com/realDonaldTrump/status/1034456273306243076">string</a> of bitter tweets, and accusations of censorship. “Algorithms” are mentioned, but not until the twelfth paragraph.</p>
<p>Trump—like so many other politicians and pundits—has found search and social media companies to be convenient targets in the debate over free speech and censorship online. “They have it RIGGED, for me &amp; others, so that almost all stories &amp; news is BAD,” the president <a data-event-click='{"element":"ExternalLink","outgoingURL":"https://twitter.com/realDonaldTrump/status/1034456273306243076"}' data-offer-url="https://twitter.com/realDonaldTrump/status/1034456273306243076" href="https://twitter.com/realDonaldTrump/status/1034456273306243076">recently tweeted</a>. He <a data-event-click='{"element":"ExternalLink","outgoingURL":"https://twitter.com/realDonaldTrump/status/1034456281120206848"}' data-offer-url="https://twitter.com/realDonaldTrump/status/1034456281120206848" href="https://twitter.com/realDonaldTrump/status/1034456281120206848">added</a>: “They are controlling what we can &amp; cannot see. This is a very serious situation---will be addressed!”</p>
<p>Trump is partly right: They are controlling what we can and cannot see. But “they” aren’t the executives leading Google, Facebook, and other technology companies. “They” are the opaque, influential algorithms that determine what content billions of internet users read, watch, and share next.</p>
<p>These algorithms are invisible, but they have an outsized impact on shaping individuals’ experience online and society at large. Indeed, YouTube’s video-recommendation algorithm inspires <a href="https://www.youtube.com/yt/about/press/">700,000,000 hours of watch time per day</a>—and can spread misinformation, disrupt elections, and incite violence. Algorithms like this need fixing.</p>
<p>But in this moment, the conversation we should be having—how can we fix the algorithms?—is instead being co-opted and twisted by politicians and pundits howling about censorship and miscasting content moderation as the demise of free speech online. It would be good to remind them that free <em>speech</em> does not mean free <em>reach</em>. There is no right to algorithmic amplification. In fact, that’s the very problem that needs fixing.</p>
<p>To see how this algorithm amplification works, simply look to RT, or <em>Russia Today</em>, a Russian state-owned propaganda outlet that’s also among the most popular YouTube presences. RT has amassed more than 6 billion views across 22 channels, more than MSNBC and Fox News combined. According to YouTube chief product officer Neal Mohan, <a href="https://qz.com/1178125/youtubes-recommendations-drive-70-of-what-we-watch/">70 percent of views on YouTube are from recommendations</a>—so the site’s algorithms are largely responsible for amplifying RT’s propaganda hundreds of millions of times.</p>
<p>How? Most RT viewers don’t set out in search of Russian propaganda. The videos that rack up the views are <a href="https://www.google.com/url?q=https://www.washingtonpost.com/news/monkey-cage/wp/2015/03/23/how-russia-today-is-using-youtube/?noredirect=on&amp;utm_term=.aec69b556318&amp;sa=D&amp;ust=1535660886144000&amp;usg=AFQjCNGUmnP8CZVChUCHXhHgbN0YKQsL6A">RT’s clickbait-y, gateway content</a>: videos of towering tsunamis, meteors striking buildings, shark attacks, amusement park accidents, some that are years old but have comments from within an hour ago. This disaster porn is highly engaging; the videos have been viewed tens of millions of times and are likely watched until the end. As a result, YouTube’s algorithm likely believes other RT content is worth suggesting to the viewers of that content—and so, quickly, an American YouTube user looking for news finds themselves watching Russia’s take on Hillary Clinton, immigration, and current events. These videos are served up in autoplay playlists alongside content from legitimate news organizations, giving RT itself increased legitimacy by association.</p>
<p>The social internet is mediated by algorithms: recommendation engines, search, trending, autocomplete, and other mechanisms that predict what we want to see next. The algorithms don’t understand what is propaganda and what isn’t, or what is “fake news” and what is fact-checked. Their job is to surface relevant content (relevant to the user, of course), and they do it exceedingly well. So well, in fact, that the engineers who built these algorithms are sometimes baffled: “Even the creators don’t always understand why it recommends one video instead of another,” says Guillaume Chaslot, an ex-YouTube engineer who worked on the site’s algorithm.</p>
<p>These opaque algorithms with their singular purpose—“keep watching”—coupled with billions of users is a dangerous recipe. In recent years, we’ve seen how dire the consequences can be. Propaganda like RT content is circulated far and wide to disinform and worsen polarization, especially during democratic elections. YouTube’s algorithms can also radicalize by suggesting “white supremacist rants, Holocaust denials, and other disturbing content,” Zeynep Tufekci <a href="https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html">recently wrote</a> in the <em>Times.</em> “YouTube may be one of the most powerful radicalizing instruments of the 21st century.”</p>
<p>The problem extends beyond YouTube, though. On Google search, dangerous anti-vaccine misinformation can <a href="https://www.wired.com/story/the-complexity-of-simply-searching-for-medical-advice/">commandeer</a> the top results. And on Facebook, hate speech can thrive and <a href="https://www.theverge.com/2018/8/28/17789202/facebook-myanmar-ban-genocide-military-leadership">fuel genocide</a>. A United Nations report about the genocide in Myanmar <a data-event-click='{"element":"ExternalLink","outgoingURL":"https://www.ohchr.org/EN/HRBodies/HRC/MyanmarFFM/Pages/ReportoftheMyanmarFFM.aspx"}' data-offer-url="https://www.ohchr.org/EN/HRBodies/HRC/MyanmarFFM/Pages/ReportoftheMyanmarFFM.aspx" href="https://www.ohchr.org/EN/HRBodies/HRC/MyanmarFFM/Pages/ReportoftheMyanmarFFM.aspx">reads</a>: “The role of social media is significant. Facebook has been a useful instrument for those seeking to spread hate, in a context where for most users Facebook is the Internet … The extent to which Facebook posts and messages have led to real-world discrimination and violence must be independently and thoroughly examined.”</p>
<p>So what can we do about it? The solution isn’t to outlaw algorithmic ranking or make noise about legislating what results Google can return. Algorithms are an invaluable tool for making sense of the immense universe of information online. There’s an overwhelming amount of content available to fill any given person’s feed or search query; sorting and ranking is a necessity, and there has never been evidence indicating that the results display systemic partisan bias. That said, unconscious bias is a concern in any algorithm; this is why tech companies have investigated conservative claims of bias since the <a href="https://www.theguardian.com/technology/2016/may/24/facebook-changes-trending-topics-anti-conservative-bias">Facebook Trending News debacle</a> of 2016. There hasn’t been any credible evidence. But there is a trust problem, and a lack of understanding of how rankings and feeds work, and that allows bad-faith politicking to gain traction. The best solution to that is to increase transparency and internet literacy, enabling users to have a better understanding of why they see what they see—and to build these powerful curatorial systems <a href="https://www.wired.com/story/creating-ethical-recommendation-engines/">with a sense of responsibility</a> for what they return.</p>
<p>There have been positive steps in this direction. The examples of harms mentioned above have sparked congressional investigations aimed at understanding how tech platforms shape our conversations and our media consumption. In an upcoming Senate hearing next week, the Senate Intelligence Committee will ask Jack Dorsey of Twitter and Sheryl Sandberg of Facebook to provide an accounting of how, specifically, they are taking steps to address computational propaganda.</p>
<p>It’s imperative that we focus on solutions, not politics. We need to build on those initial investigations. We need more nuanced conversations and education about algorithmic curation, its strange incentives, and its occasionally unfortunate outcomes. We need to hold tech companies accountable—for irresponsible tech, not evidence-free allegations of censorship—and demand transparency into how their algorithms and moderation policies work. By focusing on the real problem here, we can begin addressing the real issues that are disrupting the internet—and democracy.</p>
<p><em>Aza Raskin from the Center for Humane Technology contributed to this story.</em></p>
<h3>More Great WIRED Stories</h3>
<ul><li>It's time to stop <a href="https://www.wired.com/story/venmo-alternatives/?mbid=BottomRelatedStories_Sections_3">sending money on Venmo</a></li><li>How to <a href="https://www.wired.com/story/tips-instagram-merge-shared-accounts/?mbid=BottomRelatedStories_Sections_3">share an Instagram account</a> with your partner</li><li>The JavaScript developer <a href="https://www.wired.com/story/the-solo-javascript-developer-challenging-google-facebook/?mbid=BottomRelatedStories_Sections_3">taking on Google and Facebook</a></li><li>Say hello to the <a href="https://www.wired.com/story/stratolaunch-airplane-burt-rutan-paul-allen/?mbid=BottomRelatedStories_Sections_3">most audacious flying machine</a> ever</li><li>Why universities need <a href="https://www.wired.com/story/universities-public-interest-technology-courses-programs/?mbid=BottomRelatedStories_Sections_3">"public interest technology"</a> courses</li><li>Looking for more? <a href="https://www.wired.com/newsletter/?name=daily&amp;sourceCode=BottomStories">Sign up for our daily newsletter</a> and never miss our latest and greatest stories</li></ul>
