---
author: Anil Dash
cover_image: 'https://www.anildash.com/images/chair-in-field.jpg'
date: '2026-02-28T23:12:46.929Z'
dateFolder: 2026/02/28
description: A blog about making culture. Since 1999.
isBasedOn: 'https://www.anildash.com/2026/02/23/taking-action-ai-harms/'
link: 'https://www.anildash.com/2026/02/23/taking-action-ai-harms/'
slug: 2026-02-28-httpswwwanildashcom20260223taking-action-ai-harms
tags:
  - ai
  - tech
  - politics
title: Taking action against AI harms
---
<figure><picture><source sizes="(min-width: 1024px) 1200px, 100vw" srcset="https://www.anildash.com/img/cfAgw3H46K-600.webp 600w," type="image/webp"/><img alt="Taking action against AI harms" sizes="(min-width: 1024px) 1200px, 100vw" src="https://www.anildash.com/img/cfAgw3H46K-1600.jpeg" srcset="https://www.anildash.com/img/cfAgw3H46K-600.jpeg 600w,"/></picture></figure>
<p>Taking action on AI harms against kids</p>
<p>In my last piece, I talked about <a href="https://www.anildash.com/2026/02/18/threatening-kids-with-ai/">the harms that AI is visiting on children</a> through the irresponsible choices made by the platforms creating those products. While we dove a bit into the incentives and institutional pressures that cause those companies to make such wildly irresponsible decisions, what we haven’t yet reckoned with is how we hold these companies accountable.</p>
<p>Often, people tell me they feel overwhelmed at the idea of trying to engage with getting laws passed, or fighting a big political campaign to rein in the giant tech companies that are causing so much harm. And grassroots, local organizing can be <a href="https://patch.com/new-jersey/newbrunswick/new-brunswick-city-council-kills-proposal-build-ai-data-center-100-jersey">extraordinarily effective</a> in standing up for the values of your community against the agenda of the Big AI companies.</p>
<p>But while I think it’s vital that we pursue systemic justice (and it’s the only way to stop many kinds of harm), I do understand the desire for something more immediate and human-scale. So, I wanted to share some direct, personal actions that you can take to respond to the threats that Big AI has made against kids. Each of these tactics have been proven effective by others who have used the same strategies, so you can feel confident when adapting these for your own use.</p>
<h2>Get your company off of Twitter / X</h2>
<p>If your company or organization maintains a presence on Twitter (or X, as they have tried to rename themselves), it is important to protect yourself, your coworkers, and also your employer from the risks of being on the platform. Many times, leadership in organizations have an outdated view of the platform that is uninformed about the current level of danger and harm presented by participating on the social network, and an accurate description of the problem can often be effective in driving a decision to make a change.</p>
<p>Here is some dialogue you can use or modify to catalyze a productive conversation at work:</p>
<blockquote> <p>Hi, [name]. I saw a while ago that Twitter is being investigated in multiple countries around the world for having generated explicit imagery of women and children. The story even said that their CEO reinstated the account of a user who had shared child exploitation pictures on the site, and monetized the account that had shared the pictures.</p> </blockquote>
<blockquote> <p>Can you verify that our team is required to be on the service even though there is child abuse imagery on the site? I know that Musk’s account is shown to everyone on Twitter, so I’m concerned we’ll see whatever content he shares or retweets. Should I forward any of the child abuse material that I encounter in the course of carrying out the duties of my role to HR or legal, or both? And what is our reporting process for reporting this kind of material to the authorities, as I haven’t been trained in any procedures around these kinds of sensitive materials?</p> </blockquote>
<p>That should be enough to trigger a useful conversation at your workplace. (You can share <a href="https://www.cnbc.com/2026/01/05/india-eu-investigate-musks-x-after-grok-created-deepfake-child-porn.html">this link</a> if they want a credible, business-minded link to reference.) If they need more context about the burden on workers, you can also mention the fact that content moderators who have to interact with this kind of content have had <a href="https://citizensandtech.org/2024/02/measuring-trauma-among-the-internets-first-responders/">serious issues with trauma</a>, according to many academic studies. There is also the risk of employees and partners having concerns about nonconsensual imagery being generated from their images if the company posts anything on Twitter that features their faces or bodies. As <a href="https://www.liberalcurrents.com/the-new-epstein-island-is-right-in-your-pocket-its-time-to-abandon-elon-musks-paradise-of-abuse/">some articles have noted</a>, the Grok AI tool that Twitter uses is even designed to permit the creation of imagery that makes its targets look like the victims of violence, including targets who are underage.</p>
<p>As a result, your emails to your manager should CC your HR team, and should make explicit that you don’t wish to be liable for the risks the company is taking on by remaining on the platform. Talk to your coworkers, and share this information with them, and see if they will join you in the conversation. If you’re able to, it’s not a bad idea to look up a local labor lawyer and see if they’re willing to talk to you for free in case you need someone to CC on an email while discussing these topics. Make your employers say to you, explicitly, that the decision to remain on the platform is theirs, that they’re aware of the risks, that they indemnify you of those risks. You should ask that they take on accountability for burdens like legal costs or even psychological counseling for the real and severe impacts that come from enduring the harms that crimes like those enabled by Twitter can cause.</p>
<p>All of these strategies can also apply to products that integrate with Twitter’s service at a technical level, for sharing content or posting tweets, or for technical platforms that try to use Grok’s AI features. If you are a product manager, or know a product manager, that is considering connecting to a platform that makes child abuse material, you have failed at the most fundamental tenet of your craft. If you work at a company that has incorporated these technologies, file a bug mentioning the issues listed above, and again, CC your legal team and mention these concerns. “Our product might plug in to a platform that generates CSAM” is a show-stopping bug for any product, and any organization that doesn’t understand that is fundamentally broken.</p>
<p>Once you catalyze this conversation, you can begin mapping out a broader communication strategy that takes advantage of the many excellent options for replacing this legacy social media channel.</p>
<h2>Stop your school from using ChatGPT</h2>
<p>An increasing number of schools are falling prey to the “AI is inevitable!” rhetoric and desperately chasing the idea of putting AI tools into kids’ hands. Worse, a lot of schools think that the only kinds of technology that exist are the kinds made by giant tech companies. And because many of the adults making the decisions about AI are not necessarily experts in every detail of every technology, the decision about <em>which</em> AI platforms to use often comes down to which ones people have heard about the most. For most people, that means ChatGPT, since it’s gotten the most free hype from media.</p>
<p>As a result, many schools and educational institutions are considering the deployment of a platform that has told multiple children to self-harm, including several who have taken their own lives. This is something that you can take action about at your kid’s school.</p>
<p>First, you can begin simply by gathering resources. There are <a href="https://www.nytimes.com/2025/08/26/technology/chatgpt-openai-suicide.html?unlocked_article_code=1.M1A.S4zx.M-CdIbTK0GGI&amp;smid=url-share">many</a> <a href="https://www.nytimes.com/2025/11/06/technology/chatgpt-lawsuit-suicides-delusions.html?unlocked_article_code=1.M1A.-92e.rGfKZMgP6nE9&amp;smid=url-share">credible</a> stories which you can share to illustrate the risk to administrators, and to other parents. Typically, apologists for this product will raise a few objections, which you can respond to in a thoughtful way:</p>
<ul> <li>“Maybe those kids were already depressed?” Several of the children who have been impacted by these tools were introduced to them as homework assistants, and only evolved into using them as emotional crutches at the prompting of the responses from the tool. Also: your school has children in it who are depressed, why are you willing to endanger them?</li> <li>“Doesn’t every tool cause this?” No, this is extreme and unusual behavior. Your email software or word processor have never incited your children to commit violence against anyone, let alone themselves. Not even other LLMs prompt this behavior. And again, even if this <em>did</em> happen with every tool in this category, why would that make it okay? If every pill in a bottle is poisonous, does that make it okay to give the bottle of pills to our kids?</li> <li>“They’ll be missing out on the future.” Ask the parents of the children impacted in these stories about their kids’ futures.</li> <li>“We should just roll it out as a test.” Who will pay for monitoring all usage by all students in the test?</li> <li>“It’s a parent’s responsibility.” Forcing a parent to invest hours of time into learning a cutting-edge technology that is being constantly updated is a full-time job. If you are going to burden them with that level of responsibility, how will you provide resources to support them? What is your plan to communicate this responsibility to them and get their consent so they can agree to take on this responsibility?</li> <li>“The company said it’s working on the problem.” They can change their technology so that it only incites violence against their executives, or publish a notice when it has gone a full year without costing any children their lives. At that point, they may be considered for re-evaluation.</li> </ul>
<p>With these responses in hand, you can provide some basic facts about the risks of the specific tool or platform that is being recommended, and help present a cogent argument against its deployment. It’s important to frame the argument in terms of child safety — the conventional arguments against LLMs, grounded in concerns like environmental impact, labor impact, intellectual property rights, or other similar issues tend to be dismissed out of hand due to effective propagandizing by Big AI advocates.</p>
<p>If, instead, you ignore the debate about LLMs and focus on real-world safety concerns based on actual threats that have happened to actual children, you should be able to have a very direct impact. And these are messages that others will generally pick up and amplify as well, whether they are fellow parents, or local media.</p>
<p>From here, you can begin a conversation that re-evaluates the <em>goals</em> of the initiative from first principles. "Everyone else is doing it" is not a valid way of advocating for technology, and even if they feel that LLMs are a technology that students should become familiar with, they should begin by engaging with the many resources on the topic created by academics who are not tied to the Big AI companies.</p>
<h2>You have power</h2>
<p>The key reason I wanted to capture some specific actions that people can take around responding to the harms that Big AI poses towards children is to remind us all that the power to take action lies in everyone’s hands. It’s not an abstract concept, or a theoretical thing that we have to wait for someone else to do.</p>
<p>We are in an outrageous place, where the actions of some of the biggest and most influential technology companies in the world are so beyond the pale that we can’t even discuss the things that they are doing in polite company. The actions that take place on these platforms used to mean that simply <em>accessing</em> these kinds of sites during one’s workday would be a firing offense. Now we have employers and schools trying to <em>require</em> people to use these things.</p>
<p>The pushback has to come at every level. Do talk to your elected officials. Do organize with others at your local level. If you work in tech, make sure to resist every attempt at normalizing these platforms, or incorporating their technologies into your own.</p>
<p>Finally, use your voice and your courage, and trust in your sense of basic decency. It might only take you a few minutes to draft up an email and send it to the right people. If you need help figuring out who to send it to, or how to phrase it, let me know and I’ll help! But these things that feel small can be quite enormous when they all add up together. And that’s exactly what our kids deserve.</p>
