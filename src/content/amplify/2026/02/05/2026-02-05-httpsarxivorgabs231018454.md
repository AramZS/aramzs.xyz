---
author: 'Rebecca M. M. Hicke, David Mimno'
cover_image: 'https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png'
date: '2026-02-05T20:26:08.967Z'
dateFolder: 2026/02/05
description: >-
  Large language models have shown breakthrough potential in many NLP domains.
  Here we consider their use for stylometry, specifically authorship
  identification in Early Modern English drama. We find both promising and
  concerning results; LLMs are able to accurately predict the author of
  surprisingly short passages but are also prone to confidently misattribute
  texts to specific authors. A fine-tuned t5-large model outperforms all tested
  baselines, including logistic regression, SVM with a linear kernel, and cosine
  delta, at attributing small passages. However, we see indications that the
  presence of certain authors in the model's pre-training data affects
  predictive results in ways that are difficult to assess.
isBasedOn: 'https://arxiv.org/abs/2310.18454'
link: 'https://arxiv.org/abs/2310.18454'
slug: 2026-02-05-httpsarxivorgabs231018454
tags:
  - ai
  - books
title: >-
  Title:T5 meets Tybalt: Author Attribution in Early Modern English Drama Using
  Large Language Models
---
<p>[Submitted on 27 Oct 2023]</p>
<p>Authors:<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hicke,+R+M+M">Rebecca M. M. Hicke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mimno,+D">David Mimno</a></p>
<p>View a PDF of the paper titled T5 meets Tybalt: Author Attribution in Early Modern English Drama Using Large Language Models, by Rebecca M. M. Hicke and David Mimno</p>
<p><a href="https://arxiv.org/pdf/2310.18454">View PDF</a></p>
<blockquote> Abstract:Large language models have shown breakthrough potential in many NLP domains. Here we consider their use for stylometry, specifically authorship identification in Early Modern English drama. We find both promising and concerning results; LLMs are able to accurately predict the author of surprisingly short passages but are also prone to confidently misattribute texts to specific authors. A fine-tuned t5-large model outperforms all tested baselines, including logistic regression, SVM with a linear kernel, and cosine delta, at attributing small passages. However, we see indications that the presence of certain authors in the model's pre-training data affects predictive results in ways that are difficult to assess. </blockquote>
<table> <tbody><tr> <td>Comments:</td> <td>Published in CHR 2023</td> </tr> <tr> <td>Subjects:</td> <td> Computation and Language (cs.CL); Machine Learning (cs.LG)</td> </tr><tr> <td>Cite as:</td> <td><a href="https://arxiv.org/abs/2310.18454">arXiv:2310.18454</a> [cs.CL]</td> </tr> <tr> <td></td> <td>(or  <a href="https://arxiv.org/abs/2310.18454v1">arXiv:2310.18454v1</a> [cs.CL] for this version) </td> </tr> <tr> <td></td> <td> <a href="https://doi.org/10.48550/arXiv.2310.18454">https://doi.org/10.48550/arXiv.2310.18454</a>   Focus to learn more    arXiv-issued DOI via DataCite<br/> </td> </tr></tbody></table>
