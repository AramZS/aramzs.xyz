---
author: cannoneyed.com
cover_image: ''
date: '2026-02-26T17:43:44.559Z'
dateFolder: 2026/02/26
description: >-
  A few months ago I was standing on the 13th floor balcony of the Google New
  York 9th St office staring out at Lower Manhattan.
isBasedOn: 'https://cannoneyed.com/projects/isometric-nyc'
link: 'https://cannoneyed.com/projects/isometric-nyc'
slug: 2026-02-26-httpscannoneyedcomprojectsisometric-nyc
tags:
  - tech
title: / Isometric NYC
---
<h3>isometric nyc</h3>
<figure><img alt="isometric nyc" src="https://cannoneyed.com/img/projects/thumbnail-isometric-nyc.jpg"/><figcaption>isometric nyc</figcaption></figure>
<p>A few months ago I was standing on the 13th floor balcony of the Google New York 9th St office staring out at Lower Manhattan. I‚Äôd been deep in the weeds of a secret project using Nano Banana and Veo and was thinking deeply about what these new models mean for the future of creativity.</p>
<p>I find the usual conversations about AI and creativity to be pretty boring - we‚Äôve been talking about <em>cameras</em> and <em>sampling</em> for <em>years</em> now, and I‚Äôm not particularly interested in getting mired down in the muck of the morality and economics of it all. I‚Äôm really only interested in one question:</p>
<p>What‚Äôs possible now that was impossible before?</p>
<h2>/ The Idea</h2>
<p>Growing up, I played <em>a lot</em> of video games, and my favorites were world building games like <em>SimCity 2000</em> and <em>Rollercoaster Tycoon</em>. As a core millennial rapidly approaching middle age, I‚Äôm a sucker for the nostalgic vibes of those late 90s / early 2000s games. As I stared out at the city, I couldn‚Äôt help but imagine what it would look like in the style of those childhood memories.</p>
<p>So here‚Äôs the idea: I‚Äôm going to make a <strong>giant isometric pixel-art map of New York City</strong>. And I‚Äôm going to use it as an excuse to push hard on the limits of the latest and greatest generative models and coding agents.</p>
<p>Best case scenario, I‚Äôll make something cool, and worst case scenario, I‚Äôll learn a lot.</p>
<blockquote> üìí <strong>Note</strong> - From here on out I‚Äôll refer to all AI coding tools collectively as the ‚Äúagent‚Äù - I switched back and forth a lot between Claude Code, Gemini CLI, and Cursor (using both Opus 4.5 and Gemini 3 Pro) and as far as I‚Äôm concerned they all worked pretty well.<br/> </blockquote>
<h2>/ The Process</h2>
<p>I‚Äôm going to lead with my biggest takeaway: I wound up writing almost no code for this project. I couldn‚Äôt have picked a better time to start this project - the release of <a href="https://blog.google/products-and-platforms/products/gemini/gemini-3/">Gemini 3</a> and <a href="https://www.anthropic.com/news/claude-opus-4-5">Opus 4.5</a> along with the maturing platforms of <a href="https://cursor.com/">Cursor</a> and <a href="https://www.anthropic.com/claude-code">Claude Code</a> marked a true inflection point for the craft of software.</p>
<p>I‚Äôd been deep into agentic coding for the past few years and I‚Äôve had to dramatically recalibrate my understanding of software after this project.</p>
<p>But before there was a single line of code there was an idea, and it went something like this:</p>
<p><strong>Let‚Äôs use Nano Banana to generate a pixel art map from satellite imagery tile-by-tile.</strong></p>
<p>And so I started this project as I start almost every project now - <a href="https://cannoneyed.com/chats/isometric-nyc-initial-gemini">by asking Gemini</a>.</p>
<blockquote> üí≠ A brief aside - One of my favorite ways to use AI is simply performing tasks at a scale that were previously impossible. Napoleon may have once said ‚Äúquantity has a quality all its own‚Äù. As a former electronic musician who‚Äôs spent at least 10,000 hours precisely moving around audio clips, I'm particularly interested in scaling up the grindy repetitive tasks that make many ideas practically impossible.<br/> </blockquote>
<h3>/ NYC City Data</h3>
<p>My initial strategy was to use 3D CityGML data from a variety of sources to render a ‚Äúwhitebox‚Äù view of individual tiles. I found a few sources (<a href="https://www.asg.ed.tum.de/gis/projekte/new-york-city-3d/">NYC 3D</a>, <a href="https://github.com/3dcitydb">3DCityDB</a>, <a href="http://www.3dcitydb.net/3dcitydb/fileadmin/public/datasets/NYC/NYC_buildings_CityGML_LoD2/NYC_Buildings_LoD2_CityGML.zip">NYC CityGML</a>), downloaded some data, and then set cursor up to build out a renderer. And very quickly I had something that worked pretty well.</p>
<p>I can‚Äôt stress how big of a deal this is - I‚Äôve never worked with CityGML data before and GIS is notoriously finicky and complex. With a lot of back and forth with the agent correcting for coordinate projection systems and geometry labeling schema, I was pretty quickly able to get a renderer set up that could output an isometric (orthographic) render of real city geometry superimposed on a satellite image.</p>
<figure><img alt="whitebox render of city geometry plus satellite texture" src="https://cannoneyed.com/img/projects/isometric-nyc/whitebox.png"/><figcaption>whitebox render of city geometry plus satellite texture</figcaption></figure>
<p>I quickly set up a <a href="https://marimo.io/">marimo</a> notebook to start testing out my plan with Nano Banana Pro and discovered a number of issues. Long story short, there was a bit too much inconsistency between the ‚Äúwhitebox‚Äù geometry and the top-down satellite imagery, and Nano Banana was prone to too much hallucination in resolving these differences.</p>
<p>So I started digging again (in consultation with Gemini), and it turns out the Google Maps 3D tiles API is basically exactly what I needed - precise geometry <em>and</em> textures in one renderer. Of course, I needed a way to download the geometry for a precise tile, render it in a web renderer (using an orthographic camera) and export those tiles precisely matched with the existing whitebox renders. And sure enough, with a bit of back and forth, the agent was able to build it for me.</p>
<figure><img alt="isometric web render of city geometry plus from Google Maps 3D tiles" src="https://cannoneyed.com/img/projects/isometric-nyc/web_render.png"/><figcaption>isometric web render of city geometry plus from Google Maps 3D tiles</figcaption></figure>
<h3>/ Image Generation</h3>
<p>After a bit of prompt hacking, I was able to get Nano Banana Pro to generate tiles in my preferred style fairly reliably.</p>
<figure><img alt="generated pixel art image from Nano Banana Pro" src="https://cannoneyed.com/img/projects/isometric-nyc/nano_banana_output.png"/><figcaption>generated pixel art image from Nano Banana Pro</figcaption></figure>
<p>But generating image assets with Nano Banana has a few big issues:</p>
<ul> <li><strong>Consistency:</strong> Even with reference images, examples, and tons of prompt engineering, Nano Banana still struggles mightily to generate images with the preferred style consistently. I‚Äôd guess that it‚Äôs <em>at best</em> 50/50, which is far from good enough for the estimated 40k tiles I‚Äôll need to generate.</li> <li><strong>Cost &amp; Speed:</strong> Simply put, Nano Banana is slow and quite expensive. It simply won‚Äôt be possible to generate all of the tiles I‚Äôll need to generate given the cost and speed of a powerful model.</li> </ul>
<p>So I decided to fine-tune a smaller, faster, cheaper model. I opted to try fine-tuning a Qwen/Image-Edit model on the (wonderful) <a href="http://oxen.ai">oxen.ai</a> service and created a training dataset of ~40 input/output pairs. The fine-tuning took ~4 hours and cost ~12 bucks, and I was pretty happy with the results!</p>
<figure><img alt="input/output pairs for fine-tuning Qwen/Image-Edit" src="https://cannoneyed.com/img/projects/isometric-nyc/training_pairs_gen.png"/><figcaption>input/output pairs for fine-tuning Qwen/Image-Edit</figcaption></figure>
<h3>/ Infill</h3>
<p>Knowing that the Qwen/Image-Edit model can learn to generate tiles in the preferred style, I then began to plan the approach to generating all of the tiles. Because all tiles must be seamless, I decided to implement an ‚Äúinfill‚Äù strategy - rather than simply going from full 1024x1024 web render ‚Üí generated pixel art tile, we created a dataset with input images that have a certain percentage of the target generated image ‚Äúmasked‚Äù out. This way we can ‚Äústagger‚Äù generation by generating the tile content adjacent to already-generated tiles. And once again, it seemed like Qwen/Image-Edit was able to learn this task.</p>
<figure><img alt="infill pair images / diagram" src="https://cannoneyed.com/img/projects/isometric-nyc/training_data_infill.png"/><figcaption>infill pair images / diagram</figcaption></figure>
<h3>/ Generation</h3>
<p>Even if you‚Äôre not writing code by hand, it‚Äôs critical to follow software engineering best practices - and in fact, because code is so cheap/fast, it‚Äôs easier than ever to. This is probably worth its own essay, but in short:</p>
<ul> <li>Make small, isolated changes and test them</li> <li>Domain modeling and data storage are critical</li> <li>Simple and boring tech is better</li> <li>Iteration is better than up-front design</li> </ul>
<p>Keeping this in mind, I opted to design an end-to-end generation application to facilitate the process of generating the tiles. Experience shows that you‚Äôll hit edge cases and hit them fast (and oh did I), so it‚Äôs good practice to start small before scaling things up. I whipped up a spec and had the agent generate a simple system for driving generation:</p>
<ul> <li>A schema centered around 512x512 pixel ‚Äúquadrants‚Äù, with a given model call generating a 2x2 quadrant image from an input with a mask.</li> <li>A Sqlite database to store all quadrants along with their coordinates + optional metadata</li> <li>A web application for displaying the generated quadrants and selecting quadrants to generate.</li> </ul>
<p><a href="https://cannoneyed.com/tasks/007_e2e_generation">tasks/007_e2e_generation.md</a><br/>
<a href="https://cannoneyed.com/tasks/008_e2e_generation_pixels">tasks/008_e2e_generation_pixels.md</a><br/>
<a href="https://cannoneyed.com/tasks/012_generation_rules">tasks/012_generation_rules.md</a><br/>
<a href="https://cannoneyed.com/tasks/013_generation_app">tasks/013_generation_app.md</a></p>
<p>And lo and behold, with a bit of back-and-forth I had a working web application for progressively generating tile data.</p>
<figure><video autoplay="" loop="" muted="" src="https://cannoneyed.com/img/projects/isometric-nyc/gen_app_screencast.webm" width="100%"></video></figure>
<h3>/ Micro-tools</h3>
<p>Everyone who‚Äôs built software before AI knows the feeling of needing a tool to make it easier to analyze or debug some part of the system. You stop what you‚Äôre doing and get ready to hammer something out, and realize that it‚Äôs a much more thorny problem than you‚Äôd thought. You dig a bit deeper and accept that it‚Äôll take hours or days or weeks to build, and you go back to the main task dejected. Maybe you file a todo that you know deep down will never get done.</p>
<p>AI agents change everything. Any micro-tool you can imagine is just a few instructions away. Hell, the agent can even build it in a background thread in an isolated work branch. And it‚Äôll be done in minutes.</p>
<p>I built a wide variety of these micro-tools across the application. Here are a few off the top of my head</p>
<ul> <li><strong>Bounds app</strong> to visualize generated/in-progress tiles superimposed on a real map of NYC. Eventually this evolved into a full-fledged boundary polygon editor to determine the edges of the final exported tiles</li> <li><strong>Water classifier</strong> to classify whether or not a given quadrant partially or completely contained water.</li> <li><strong>Training data generator</strong> to generate training data for the Qwen/Image-Edit model.</li> </ul>
<figure><img alt="debug map showing the tiles that have been generated" src="https://cannoneyed.com/img/projects/isometric-nyc/debug_map.png"/><figcaption>debug map showing the tiles that have been generated</figcaption></figure>
<p>A pattern that I‚Äôve noticed when building out this set of tools is something like the following:</p>
<p><strong>CLI tool ‚Üí Library ‚Üí Application</strong></p>
<p>CLI tools are very easy for the agent to use, test, and debug. They also encourage simple boundaries and discourage tight coupling. When the time comes to integrate them into a bigger system or application, it‚Äôs trivial to ask the agent to abstract the functionality into a shared library.</p>
<h3>/ Edge Cases</h3>
<p>Everyone who‚Äôs worked in software knows the feeling: you‚Äôve just built the killer tool that‚Äôs going to solve all your problems. You‚Äôve thrown a bunch of test cases at it and it just keeps working, and you‚Äôre certain that you‚Äôre 90% done with the project. And then you hit an edge case, and realize, once again, that the last 10% of the work takes 90% of the time.</p>
<p>There were too many edge cases to give justice to here, but I‚Äôll focus on two particular issues that caused a lot of downstream challenges: <strong>water</strong> and <strong>trees</strong>.</p>
<p>See, New York City has <em>a lot</em> of water - the Hudson and East Rivers empty into the New York Harbor and Bay, and Jamaica Bay and the Long Island Sound are both large bodies of water with lots of marine topography like islands, sand bars, and marshlands.</p>
<p>I‚Äôd not anticipated both the shear amount of water I‚Äôd be need to generate and, more importantly, how difficult it would be for my fine-tuned models to handle this.</p>
<figure><img alt="water and trees caused lots of issues for the models" src="https://cannoneyed.com/img/projects/isometric-nyc/water_tree_issues.png"/><figcaption>water and trees caused lots of issues for the models</figcaption></figure>
<p>As a brief aside, fine-tuning models is <em>hard</em>. In particular, image models have a lot of quirks that make it very difficult to handle certain tasks - separating structure from texture is a classic issue.</p>
<p>No matter what I did to retrain my fine-tuned image models, I couldn‚Äôt get them to reliably generate water. And <em>trees</em> were much worse - almost a perfect pathological use case for these models.</p>
<p>At some point in almost every creative AI project, you hit a point where the models just <em>can‚Äôt</em> do what you need them to. You‚Äôll need to deploy your own intelligence and grind through these edge cases, and at this point it becomes imperative to use tools to make it as easy and consistent as possible. Fortunately, we live in an age where an agent can build you almost any tool you can imagine.</p>
<p>I built a number of micro-tools to help make this work easier, including:</p>
<ul> <li>Automatic color-picker based water correction in the generation app</li> <li>Custom prompting + negative prompting and model-swappability for running generation</li> <li>Export/import to/from Affinity (photo editing software) for the most manual fixes</li> </ul>
<p>But at the end of the day, the last 10% always takes up 90% of the time and, as always, the difference between good enough and great is the amount of love you put into the work. So in the end, I rolled up my sleeves and threw a lot of time into manually fixing these edge cases.</p>
<h3>/ Scaling up</h3>
<p><a href="http://Oxen.ai">Oxen.ai</a> is a wonderful service - automating and abstracting away all of the fiddly bits of fine-tuning and deploying models and managing training data. But inference through the platform was rather expensive and rather slow - if I wanted to scale up the process to generate <em>all</em> of the NYC map we‚Äôd need to make the models significantly faster (or more parallelizable) and cheaper.</p>
<p>So I opted to export the weights from Oxen to my own rented GPU+VMs using <a href="https://lambda.ai/">Lambda AI</a> (another fantastic service). I‚Äôve been training and deploying models for a <em>long</em> time, and I remember the horrors of getting models to run on commodity hardware. But add this process to the long list of things that AI coding agents <em>magically solve</em>.</p>
<p>I simply booted up a VM with an H100, ssh‚Äôd into it with Cursor, and prompted the agent to set up an inference server that I could ping from my web generation app. What used to take hours or days of painful, slow debugging now takes literally minutes.</p>
<p>Now I could run <code>n</code> models in parallel and generate large spans of the map. Every night, I‚Äôd spend a few minutes setting up a plan for which tiles to generate and then let the models run overnight. For less than $3 an hour and more than 200 generations/hour, the project became tractable both in terms of time <em>and</em> cost.</p>
<p>Of course, now I needed to build a lot of tooling to manage this scale, including retry logic, parallel model queues, and tile planning infrastructure, but the agents took care of this as easily as anything else.</p>
<blockquote> üí≠ Again, software engineering doesn‚Äôt go away in the age of AI - it just moves up the ladder of abstraction. I still had to spec out the behavior of the generation queues and logic which incorporated all of the subtle domain-specific logic, but I no longer cared about <em>any</em> of the code that implemented it. I‚Äôm serious - I‚Äôve never even looked at it.<br/> </blockquote>
<h3>/ Automating</h3>
<p>Now that I‚Äôd scaled up generation and addressed some of the edge cases, I set off to automate as much of the work as I could. Unfortunately, this is where the project (and the models) failed the hardest.</p>
<p>Interestingly, it was <em>extremely</em> difficult to get the agents to implement and understand an efficient tiling algorithm. The rules for generation are fairly simple - no quadrant may be generated such that a ‚Äúseam‚Äù will be present.</p>
<figure><img alt="a sample of tile generation rules to avoid seams" src="https://cannoneyed.com/img/projects/isometric-nyc/gen_rules.png"/><figcaption>a sample of tile generation rules to avoid seams</figcaption></figure>
<p>But despite the simplicity of the constraint, it was very difficult to specify the generation logic, test that logic, and then make it useful by higher-level planning / optimization algorithms. All in all, I did a lot of iterating here and spent a lot of fruitless efforts at getting the agent to understand how to build a planning algorithm. But after a lot of attempts and iterations, I eventually wound up with something that worked well enough with a bit of manual guidance.</p>
<p>One takeaway is that some algorithms are irreducibly complex, and it can still be very difficult to get these otherwise extremely smart models to understand the core logic behind them via the crappy medium of specification documents and instructions.</p>
<p><a href="https://cannoneyed.com/tasks/015_automatic_generation">015_automatic_generation.md</a><br/>
<a href="https://cannoneyed.com/tasks/016_generation_playbook">016_generation_playbook.md</a><br/>
<a href="https://cannoneyed.com/tasks/019_strip_plan">019_strip_plan.md</a></p>
<p>Once I got the app to reliably generate "plans" for large spans of the map, I kicked off a large batch of generations and ran them overnight. The results were mostly good, but the model still demonstrated a number of failure modes (especially around water and terrain).</p>
<p>In a perfect world, I'd add some kind of AI review process to ensure that the generations were up to par. But in most cases, even the smartest image models like Gemini 3 pro couldn‚Äôt reliably assess most of the failure modes (such as seams and incorrect tree generation). And even when the model could assess these issues, there was no way to deploy it reliably at a scale and speed that wouldn‚Äôt make the process intractable.</p>
<p>So I wound up accepting that I‚Äôd simply need to put in the effort to manually review, flag, and correct the generations across the map. And while it did take a <em>lot</em> of work (way more than I‚Äôd planned to spend on the project), the AI agents‚Äô ability to build custom bespoke micro-tools to make it easier proved invaluable.</p>
<h3>/ The app</h3>
<p>Now that the tile generation process was humming along nicely, I wanted to build out the final application to display the generated tiles at all of the zoom levels. This seemed like it would be simple, but wound up being one of the more difficult tasks for the coding agents to handle.</p>
<p>By a stroke of luck, I spent my first year at Google Brain building a custom tiled gigapixel image viewer, so I intimately know the challenges in the problem space. And while I opted to use the open source OpenSeaDragon library, I had to rely on my expertise for countless zoom/coordinate space and caching/performance issues that arose. This kind of app in particular seems like a very pathological challenge for today‚Äôs generation of coding agents - high performance graphics with a lot of manual touch interaction are not handled very well by any of the browser control tools.</p>
<p>But after quite a bit of debugging I was able to get the app up, running, and deployed.</p>
<h2>/ Takeaways</h2>
<h3>/ Cheap, fast software</h3>
<p>The biggest joy of this project was the ability to build tools at the speed of thought. As a software developer, I think of a million little tools I‚Äôd <em>like</em> to have but would take a day or a week to build. With Claude or Cursor, I can whip them up in 5 minutes. This is absolutely transformational - it‚Äôs like having an infinite toolbox.</p>
<p>Of course, software engineering rules still apply. Entropy is everything; as you add features, complexity grows, and without architecture, you accumulate tech debt. But here‚Äôs the thing: for throwaway tools‚Äîdebuggers, visualizers, script runners‚Äî<strong>code quality doesn't really matter.</strong></p>
<p>I know how crappy the code for my generation app is. It‚Äôs a mess of imperative JavaScript and spaghetti event listeners that I‚Äôd never write by hand. But it‚Äôs not going out to customers. It doesn‚Äôt need to scale. I‚Äôm the only user, and the bugs are tolerable given how little it cost me. The fact that it is cheap and fast more than makes up for the fact that it isn't all that great.</p>
<p>In general, composability is huge and even more valuable in the context of vibe coding. The Unix philosophy of small, modular programs that do one thing well means that we can easily compose smaller tools into utility functions that can be reused by higher-level applications. By designing pieces of functionality in a modular way, you can most effectively leverage the coding agent - it‚Äôs easier for you to specify simple behavior, it‚Äôs easier for the agent to build, debug, and test these modular pieces, and it‚Äôs easier to stitch them together into higher level apps later on. This is standard software engineering best practices, and it‚Äôs more relevant now that the cost of code is approaching zero.</p>
<h3>/ Image models aren‚Äôt there yet</h3>
<p>This project also highlighted a massive gap between text/code generation and image generation.</p>
<p>If I ask an agent to write software, it can run the code, read the stack trace, see the error, and correct itself. It has a tight feedback loop. It understands the system it is building.</p>
<p>Image models just aren't there yet. If you were managing a human artist, you could say, "Hey, make sure the trees are this specific style," and they would execute. While models <em>can</em> do this, they can't do it <em>reliably</em>. Even a model as smart as Gemini 3 Pro cannot reliably look at an output and say, "There is a seam here," or "This tree texture is wrong." Because they can‚Äôt reliably "see" the failure modes, I couldn't automate the QA process. I had to give up on fully automated generation because the models simply couldn't understand their own mistakes.</p>
<p>Fine-tuning remains as flimsy as ever - anyone who‚Äôs ever trained a model understands at a deep level that these are alien intelligences. Models often learn things in a deeply counterintuitive way, and in many cases you need to have a deep understanding of ML theory and strong intuitions about model implementations in order to reliably train them to accomplish the task at hand.</p>
<p>There‚Äôs something fundamentally broken here - people are more than capable of contrastive learning (learning from their mistakes) and continuous learning (learning as they go) yet most AI agents are trained purely via association and are completely stateless. I‚Äôm optimistic that we‚Äôll make progress here, though it‚Äôll require some fundamental changes to our model architectures and training regimes.</p>
<p>These failure modes are <em>especially</em> apparent with image models - it might take you a minute to read and assess the output of a pdf extraction task, but you can see incorrect details in generated images in milliseconds.</p>
<h3>/ The edit problem</h3>
<p>Finally, the interface for generative models is quite flimsy compared to text.</p>
<p>With code, I can point to a specific line. Because everything is text, prompts can be self-referential. With images, I can't reliably say, "Look at Image C and copy that tree." The model has no concept of "that tree." - I can‚Äôt point to it and I can‚Äôt reliably refer to it via text. It may not even know which image I mean by ‚ÄúImage C‚Äù</p>
<p>Even worse, it can't really edit the image. If I tell a coding agent to fix a bug, it modifies the file. If I tell an image model to fix a tree, it has to dream up the entire image from scratch again via diffusion. There is no reliable way to reach into the tokens and tweak just one variable.</p>
<p>There‚Äôs no way to do basic instruction techniques like few-shot prompting, and there‚Äôs no way to annotate images for editing. Masking doesn‚Äôt exist, transparency doesn‚Äôt exist. We‚Äôre still so early in the evolution of generative image models, and while they can already do so much, we‚Äôve got a long way to go.</p>
<h3>/ <strong>AI for artists</strong></h3>
<p><strong>The end of drudgery</strong></p>
<p>I spent a decade as an electronic musician, spending literally thousands of hours dragging little boxes around on a screen. So much of creative work is defined by this kind of tedious grind.</p>
<p>For example, after recording a multi-part vocal harmony you change something in the mix and now it feels like one of the phrases is off by 15 milliseconds. To fix it, you need to adjust <em>every</em> layer - and this gets more convoluted if you‚Äôre using plugins or other processing on the material.</p>
<p>This isn't creative. It's just a slog. Every creative field - animation, video, software - is full of these tedious tasks. Of course, there‚Äôs a case to be made that the very act of doing this manual work is what refines your instincts - but I think it‚Äôs more of a ‚ÄúJust So‚Äù story than anything else. In the end, the quality of art is defined by the quality of your decisions - how much work you put into something is just a proxy for how much you care and how much you have to say.</p>
<p><strong>Unlocking Scale</strong></p>
<p>This project is far from perfect, but without generative models, it <em>couldn‚Äôt</em> exist. There‚Äôs simply no way to do this much work on your own, and hiring a team of artists large enough to hand-draw pixel art for every building in New York City would be impossible.</p>
<p>AI agents unlock a universe of creative projects that were previously unimaginable.</p>
<p><strong>Slop vs. Art</strong></p>
<p>If you can push a button and get content, then that content is a commodity. Its value is next to zero.</p>
<p>Counterintuitively, that‚Äôs my biggest reason to be optimistic about AI and creativity. When hard parts become easy, the differentiator becomes love.</p>
