---
author: Ian Wesley-Smith
cover_image: >-
  https://substackcdn.com/image/fetch/$s_!Ybxd!,w_1200,h_675,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff590eeaf-58c9-4c4a-9760-0e8edebfee27_528x514.png
date: '2026-02-24T07:20:48.486Z'
dateFolder: 2026/02/24
description: 'The past, present, and future of personalization of the Discover feed'
isBasedOn: 'https://recsysml.substack.com/p/personalization-at-bluesky?triedRedirect=true'
link: 'https://recsysml.substack.com/p/personalization-at-bluesky?triedRedirect=true'
slug: >-
  2026-02-24-httpsrecsysmlsubstackcomppersonalization-at-blueskytriedredirecttrue
tags:
  - tech
  - social media
  - decentralization
title: Personalization at Bluesky
---
<h3>The past, present, and future of personalization of the Discover feed</h3>

<p>At <a href="https://bsky.app/">Bluesky</a>, we are building an open foundation for the social internet, where anyone can create a feed, such as the<a href="https://bsky.app/profile/did:plc:jfhpnnst6flqway4eaeqzj2a/feed/for-science"> Science feed</a>,<a href="https://bsky.app/profile/spacecowboy17.bsky.social/feed/for-you"> For You feed by spacecowboy</a>, or<a href="https://bsky.app/profile/did:plc:rea3amxwqfkfzhilivubtrib/feed/aaabfz334lr66"> GLAMS feed</a>. We also aim to provide a great default Discover feed. This post discusses personalization of the Discover feed, from historical attempts, to current deployment, and a path forward inspired by Pinterest’s work. If interested, come work with me at <a href="https://jobs.gem.com/bluesky/am9icG9zdDpJ8TCGYh93XAC00AkK4gXz">Bluesky</a>!</p>
<figure><a data-component-name="Image2ToDOM" href="https://substackcdn.com/image/fetch/$s_!Ybxd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff590eeaf-58c9-4c4a-9760-0e8edebfee27_528x514.png"><picture><img alt="A screenshot of the Bluesky Discover feed." data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/f590eeaf-58c9-4c4a-9760-0e8edebfee27_528x514.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":514,"width":528,"resizeWidth":null,"bytes":185213,"alt":"A screenshot of the Bluesky Discover feed.","title":null,"type":"image/png","href":null,"belowTheFold":false,"topImage":true,"internalRedirect":"https://recsysml.substack.com/i/188820301?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff590eeaf-58c9-4c4a-9760-0e8edebfee27_528x514.png","isProcessing":false,"align":null,"offset":false}' sizes="100vw" src="https://substackcdn.com/image/fetch/$s_!Ybxd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff590eeaf-58c9-4c4a-9760-0e8edebfee27_528x514.png" srcset="https://substackcdn.com/image/fetch/$s_!Ybxd!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff590eeaf-58c9-4c4a-9760-0e8edebfee27_528x514.png 424w, https://substackcdn.com/image/fetch/$s_!Ybxd!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff590eeaf-58c9-4c4a-9760-0e8edebfee27_528x514.png 848w, https://substackcdn.com/image/fetch/$s_!Ybxd!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff590eeaf-58c9-4c4a-9760-0e8edebfee27_528x514.png 1272w, https://substackcdn.com/image/fetch/$s_!Ybxd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff590eeaf-58c9-4c4a-9760-0e8edebfee27_528x514.png 1456w"/></picture></a><figcaption>An example of the Bluesky Discover feed</figcaption></figure>
<p>As the first MLE at Bluesky, I initially attempted a <a href="https://recsysml.substack.com/p/two-tower-models-for-retrieval-of">two tower model</a> but it failed to converge, possibly due to insufficient data or being a poor fit for Bluesky’s short-lifetime items and skewed interaction distributions. Bluesky was (and still is) a small team, so I couldn’t spend forever debugging this issue. Instead I switched to building a system that would generate post embeddings based on the content of a post, with the idea that I could build a personalization system on top of that.</p>
<p>Currently, posts are embedded using<a href="https://arxiv.org/abs/2301.12597"> BLIP2</a>, a variant of CLIP, which powers our topic models (27 topics users select during onboarding). While this topic model is accurate it is also quite broad, which hurts the user experience. I’ve also run <a href="https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html">HDBSCAN</a> over a sample of the post embedding space to generate ~600 clusters which provide finer grained grouping of content. By measuring a user’s interaction with content from these clusters or topics we have a rudimentary personalization system that can help users find content they might be interested in.</p>
<p>My goal is to substantially improve personalization of the Discover feed. After reviewing papers, I chose to investigate techniques from Pinterest, specifically their <a href="https://arxiv.org/abs/2007.03634">PinnerSage</a> paper. This choice was based on budget fit, simplicity, avoiding extensive fine-tuning, and the requirement to treat user and post representations separately. There are a lot of similarities between the papers published by Pinterest and Twitter, but I choose to use the Pinterest papers because they’ve continued publishing, providing a path to utilize more advanced models as the ML team at Bluesky grows.</p>
<h2>Bluesky is hiring!</h2>
<p>Speaking of growing the team, are you a mid-senior MLE with experience in recommender systems? Do you want to join a team laying the groundwork for how ML will operate at a fast growing social media platform? Do you want to increase your scope of work? Want to experiment with new, unconventional ideas? Think distributed social media is the way of the future? Then come work with me at <a href="https://jobs.gem.com/bluesky/am9icG9zdDpJ8TCGYh93XAC00AkK4gXz">Bluesky</a>!</p>
<h2>PinnerSage</h2>
<p>Published in 2020, <a href="https://arxiv.org/abs/2007.03634">PinnerSage</a> addresses the issue of a single user preference embedding failing to capture a user’s full range of interests, especially short and long-term interests. It does this by generating several (10-100) user preference embeddings via an offline path (last 90 days) and an online path (today’s interactions). This resulted in a 2% increase in user engagement propensity and a 4% increase in engagement volume in online A/B tests.</p>
<h3>How it works</h3>
<p>PinnerSage is a rather simple approach to the problem, with intentional design choices that match my own. They specifically mention that item embeddings should be fixed, which is a requirement for me.</p>
<h4>Step 1: Cluster User Interactions</h4>
<p>First, for a given user they take the last 90-days of item interactions (i.e. action pins) and gather the item embeddings. Next they cluster these embeddings using <a href="https://en.wikipedia.org/wiki/Ward%27s_method">Ward clustering</a>, generating a ‘small number’ (10-100) of clusters for a user. Their specific Ward implementation is based on the <a href="https://en.wikipedia.org/wiki/Ward%27s_method#Lance%E2%80%93Williams_algorithms">Lance-Williams algorithm</a>, and has a complexity of <em>O(n^2)</em> where <em>n</em> is the number of items being clustered.</p>
<h4>Step 2: Calculate the Medoid</h4>
<p>Second, for each cluster, a medoid—an actual member of the cluster that minimizes the sum of the squared distances with other members—is calculated. This simplifies deployment by allowing Pinterest to reuse existing pin infrastructure.</p>
<h4>Step 3: Importance Scoring</h4>
<p>Finally, they calculate a user-cluster importance score. Since a user can have 10-100 clusters we need a way to choose which clusters to use during retrieval. They use a simple time decay average model:</p>
<p>Importance(C,λ)=∑i∈Ce−λ(τnow−τ[i])</p>
<p><em>lambda</em> is a hyper-parameter that controls recency, with 0 ignoring time effects and 0.1 emphasizing recent interaction. Pinterest found 0.01 to be a good balance.</p>
<p>With these three steps we now have a set of per-user interest medoids (i.e. pins) and weights for how much a user interacts with those pins.</p>
<h3>Integrating with your Recommender System</h3>
<p>Applying this to retrieval is fairly straightforward. The medoids can be sampled, weighted by importance, and used as candidate sources for an ANN-based candidate generator. Pinterest sampled up to 3 medoids at a time, and applied additional (though unspecified) filtering to remove near duplicates and poor quality candidates.</p>
<p>One weakness with PinnerSage is the difficulty in using these user preference embeddings during ranking. Traditionally you create a feature for each item that is the similarity of that item’s embedding and the user preference embedding. With PinnerSage there are anywhere from 10-100 preference embeddings for each user, so it is unclear which of these embeddings you should choose. You could try using all of them, and taking the max score for a given item and the user preference embeddings, but this is expensive to do at runtime (i.e. 100 embeddings x 1,000 items = 100,000 ops). Another option is to take a weighted average of the user preference embeddings to combine them into a single user-preference embedding, but this naive approach will likely result in loss of accuracy due to smearing the users preferences.</p>
<p>The difficulty of integrating multiple user preference embeddings into ranking was a key motivator for <a href="http://arxiv.org/abs/2205.04507">PinnerFormer (Pancha et al., “PinnerFormer.”)</a>, which Pinterest developed to generate a single user preference embedding using Transformers to better capture user interests. We will discuss PinnerFormer in a future blogpost.</p>
<h3>Short Term Interests &amp; Item Embeddings</h3>
<figure><a data-component-name="Image2ToDOM" href="https://substackcdn.com/image/fetch/$s_!puhE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0a2d8c-c0ac-4460-82e3-848b55c6a788_790x626.png"><picture><img alt="A diagram depicting the PinnerSage recommendation system architecture. It shows the batch and real-time systems, with the batch feeding into a Key-Value store while the real time feeds directly into the candidate generator." data-attrs='{"src":"https://substack-post-media.s3.amazonaws.com/public/images/ca0a2d8c-c0ac-4460-82e3-848b55c6a788_790x626.png","srcNoWatermark":null,"fullscreen":null,"imageSize":null,"height":626,"width":790,"resizeWidth":null,"bytes":null,"alt":"A diagram depicting the PinnerSage recommendation system architecture. It shows the batch and real-time systems, with the batch feeding into a Key-Value store while the real time feeds directly into the candidate generator.","title":null,"type":null,"href":null,"belowTheFold":true,"topImage":false,"internalRedirect":null,"isProcessing":false,"align":null,"offset":false}' sizes="100vw" src="https://substackcdn.com/image/fetch/$s_!puhE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0a2d8c-c0ac-4460-82e3-848b55c6a788_790x626.png" srcset="https://substackcdn.com/image/fetch/$s_!puhE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0a2d8c-c0ac-4460-82e3-848b55c6a788_790x626.png 424w, https://substackcdn.com/image/fetch/$s_!puhE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0a2d8c-c0ac-4460-82e3-848b55c6a788_790x626.png 848w, https://substackcdn.com/image/fetch/$s_!puhE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0a2d8c-c0ac-4460-82e3-848b55c6a788_790x626.png 1272w, https://substackcdn.com/image/fetch/$s_!puhE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca0a2d8c-c0ac-4460-82e3-848b55c6a788_790x626.png 1456w"/></picture></a><figcaption>PinnerSage architecture diagram from Pal et al., “PinnerSage.”</figcaption></figure>
<p>Earlier we alluded to an online system that captures short term interests. An event-based streaming system captures short-term interests by performing the same clustering and importance estimation steps on the twenty most recent actions since the last batch job. These results are combined with the batch results.</p>
<p>One thing not discussed in this paper is how item embeddings are generated. At the time of publication (2020), Pinterest used a sophisticated graph based embedding model called <a href="http://arxiv.org/abs/1706.02216">PinSage (Hamilton et al., “Inductive Representation Learning on Large Graphs.”)</a>. At BlueSky we are using BLIP2 to generate post embeddings. If you don’t already have an item embedding model then you can’t deploy PinnerSage.</p>
<h2>Conclusion</h2>
<p>This blog post presented an overview of PinnerSage, a clustering based approach to generating user preference embeddings while keeping item embeddings fixed. I also discussed a brief history of personalization at Bluesky, and provided my motivation for investigating PinnerSage. My current plans are to implement PinnerSage as a candidate generator, then move to PinnerFormer to generate a single user preference embedding for ranking. As we make progress on various parts of the stack we will share our work.</p>
<h2>Bibliography</h2>
<p>Pal, Aditya, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles Rosenberg, and Jure Leskovec. “PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest.” <em>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, August 23, 2020, 2311–20.<a href="https://doi.org/10.1145/3394486.3403280"> https://doi.org/10.1145/3394486.3403280</a>.</p>
<p>Hamilton, William L., Rex Ying, and Jure Leskovec. “Inductive Representation Learning on Large Graphs.” <em>arXiv:1706.02216 [Cs, Stat]</em>, June 7, 2017.<a href="http://arxiv.org/abs/1706.02216"> http://arxiv.org/abs/1706.02216</a>.</p>
<p>Pancha, Nikil, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. “PinnerFormer: Sequence Modeling for User Representation at Pinterest.” arXiv:2205.04507. Preprint, arXiv, May 9, 2022.<a href="http://arxiv.org/abs/2205.04507">http://arxiv.org/abs/2205.04507</a>.</p>
<h4>Subscribe to RecsysML + LLMs</h4>
<p>Launched 5 years ago</p>
<p>State of the art advances in applied machine learning with a focus on recommender systems</p>
<p>By subscribing, you agree Substack's <a href="https://substack.com/tos">Terms of Use</a>, and acknowledge its <a href="https://substack.com/ccpa#personal-data-collected">Information Collection Notice</a> and <a href="https://substack.com/privacy">Privacy Policy</a>.</p>
<p>Previous</p>
<h3>Ready for more?</h3>
<p>Subscribe</p>
