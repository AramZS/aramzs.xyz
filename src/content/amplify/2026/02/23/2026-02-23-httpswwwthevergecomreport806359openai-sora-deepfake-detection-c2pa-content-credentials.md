---
author: Jess Weatherbed
cover_image: >-
  https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&strip=all&crop=0%2C10.732984293194%2C100%2C78.534031413613&w=1200
date: '2026-02-23T17:21:26.865Z'
dateFolder: 2026/02/23
description: We’re living in a world of fake realities.
isBasedOn: >-
  https://www.theverge.com/report/806359/openai-sora-deepfake-detection-c2pa-content-credentials
link: >-
  https://www.theverge.com/report/806359/openai-sora-deepfake-detection-c2pa-content-credentials
slug: >-
  2026-02-23-httpswwwthevergecomreport806359openai-sora-deepfake-detection-c2pa-content-credentials
tags:
  - ai
  - tech
title: Sora is showing us how broken deepfake detection is
---
<p>OpenAI’s video generator uses a system that’s supposed to help online platforms tag deepfakes — so why aren’t we seeing them?</p>
<figure><img alt="STK419_DEEPFAKE_CVIRGINIA_C" data-chromatic="ignore" data-nimg="fill" sizes="(max-width: 768px) 100vw, 700px" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK419_DEEPFAKE_CVIRGINIA_C.gif?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400 2400w"/><figcaption>C2PA has merit, but it isn’t enough to protect online users from being misled by OpenAI’s deepfake factory. Illustration by Cath Virginia / The Verge | Photos from Brandon Bell, Getty Images</figcaption></figure>
<p>OpenAI’s new deepfake machine, Sora, has proven that artificial intelligence is alarmingly good at faking reality. The AI-generated video platform, powered by OpenAI’s new Sora 2 model, has churned out detailed (and <a href="https://globalextremism.org/post/openais-sora-2-used-to-spread-holocaust-denial-and-glorify-hitler/">often offensive or harmful</a>) videos of famous people like <a href="https://www.theverge.com/news/801539/open-ai-sora-mlk">Martin Luther King Jr.</a>, Michael Jackson, and <a href="https://www.theverge.com/news/803141/openai-sora-bryan-cranston-deepfakes">Bryan Cranston</a>, as well as copyrighted characters like <a href="https://www.404media.co/openais-sora-2-copyright-infringement-machine-features-nazi-spongebobs-and-criminal-pikachus/">SpongeBob and Pikachu</a>. Users of the app who voluntarily shared their likenesses have seen themselves shouting racial slurs or turned into fuel for fetish accounts.</p>
<p>On Sora, there’s a clear understanding that everything you see and hear isn’t real. But like any piece of social content, videos made on Sora are meant to be shared. And once they escape the app’s unreality quarantine zone, there’s little protection baked in to ensure viewers know that what they’re looking at isn’t real.</p>
<p>The app’s convincing mimicry doesn’t just run the risk of misleading viewers. It’s a demonstration of how profoundly AI labeling technology has failed, including a system OpenAI itself helps oversee: <a href="https://www.theverge.com/2024/8/21/24223932/c2pa-standard-verify-ai-generated-images-content-credentials">C2PA authentication</a>, one of the best systems we have for distinguishing real images and videos from AI fakes.</p>
<p>C2PA authentication is more commonly known as “Content Credentials,” a term championed by Adobe, which has spearheaded the initiative. It’s a system for attaching invisible but verifiable metadata to images, videos, and audio at the point of creation or editing, appending details about how and when it was made or manipulated.</p>
<p>OpenAI is a <a href="https://openai.com/index/understanding-the-source-of-what-we-see-and-hear-online/">steering committee member</a> of the <a href="https://c2pa.org/">Coalition for Content Provenance and Authenticity</a> (C2PA), which developed the open specification alongside the Adobe-led <a href="https://contentauthenticity.org/">Content Authenticity Initiative</a> (CAI). And in fact, C2PA information is embedded in every Sora clip — you’d just probably never know it, unless you’re the type to pore over some brief footnotes on a meager handful of OpenAI’s blog posts.</p>
<figure><a data-pswp-height="744" data-pswp-width="800" href="https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0,0,100,100"><img alt="An example of the AI labels on YouTube content." data-chromatic="ignore" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/YouTube-AI-label.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400 2400w"/></a><figcaption>This is the label that’s supposed to appear on AI-generated or manipulated videos uploaded to YouTube Shorts. I’ve never seen it in the wild. Image: YouTube</figcaption></figure>
<p>C2PA only works if it’s adopted at every step of the creation and posting process, including being clearly visible to the person viewing the output. In theory, it’s been embraced by Adobe, <a href="https://www.theverge.com/2024/2/6/24063954/ai-watermarks-dalle3-openai-content-credentials">OpenAI</a>, <a href="https://www.theverge.com/2024/9/17/24247004/google-c2pa-verify-ai-generated-images-content">Google</a>, <a href="https://www.theverge.com/2024/10/15/24271083/youtube-c2pa-captured-camera-label-content-credentials">YouTube</a>, Meta, <a href="https://www.theverge.com/2024/5/9/24152667/tiktok-ai-generated-label-content-credentials-cai-c2pa">TikTok</a>, <a href="https://www.theverge.com/2024/9/13/24244219/amazon-joins-c2pa">Amazon</a>, <a href="https://www.theverge.com/news/604989/cloudflare-adobe-content-credentials-authenticty-feature">Cloudflare</a>, and even <a href="https://www.theverge.com/2025/1/14/24343788/the-office-of-the-arizona-secretary-of-state-is-going-to-use-content-credentials-on-images">government offices</a>. But few of these platforms use it to clearly flag deepfake content to their users. Instagram, TikTok, and YouTube’s efforts are barely visible labels that are easy to miss, and they provide very little context if you actually were to spot them. And for TikTok and YouTube, I’ve never once encountered them myself while browsing the platforms, even on videos that are clearly AI-generated.</p>
<p>Meta initially added a <a href="https://www.theverge.com/2024/6/24/24184795/meta-instagram-incorrect-made-by-ai-photo-labels">small “Made by AI” tag</a> to images on Facebook and Instagram last year, but it <a href="https://www.theverge.com/2024/7/1/24190026/meta-instagram-facebook-made-with-ai-info-label-metadata">later changed the tag to say “AI Info”</a> after photographers complained that work they edited using Photoshop — which automatically applies Content Credentials — was being mislabeled. And most online platforms don’t even do that, despite being more than capable of scanning uploaded content for AI metadata.</p>
<p>C2PA’s creators insist they’re getting closer to widespread adoption. “We’re seeing meaningful progress across the industry in adopting Content Credentials, and we’re encouraged by the active collaboration underway to make transparency more visible online,” Andy Parsons, senior director of Content Authenticity at Adobe, said to <em>The Verge</em>. “As generative AI and deepfakes become more advanced, people need clear information about how content is made.”</p>
<p>Yet after <a href="https://www.theverge.com/2021/10/26/22745506/adobe-nft-art-theft-content-credentials-opensea-rarible-photoshop">four years</a>, that progress is still all but invisible. I’ve covered CAI since I started at <em>The Verge</em> three years ago, and I didn’t realize for weeks that every video generated using Sora and Sora 2 <a href="https://openai.com/index/launching-sora-responsibly/">has Content Credentials embedded</a>. There’s no visual marker that alludes to it, and in every example I’ve seen where these videos are reposted to other platforms like X, Instagram, and TikTok, I have yet to see any labels that identify them as being AI-generated, let alone provide a full accounting of their creation.</p>
<p>One example noted by <a href="https://copyleaks.com/blog/racist-meme-on-sora">AI detection platform Copyleaks</a> is a viral AI-generated video on TikTok that shows CCTV footage of <a href="https://www.tiktok.com/@mohamd.jhunna/video/7561741102429097236">a man catching a baby</a> that’s seemingly fallen out of an apartment window. The video has almost two million views and appears to have a blurred-out Sora watermark. TikTok hasn’t visibly flagged that the video is AI-generated, and there are thousands of commenters questioning whether the footage is real or fake.</p>
<p>If a user wants to check images and videos for C2PA metadata, the burden is almost entirely on them. They have to save and then upload a supported file into the <a href="https://verify.contentauthenticity.org/">CAI</a> or <a href="https://www.theverge.com/news/654883/adobe-content-authenticity-web-app-beta-availability">Adobe web app</a>, or they have to download and run a <a href="https://helpx.adobe.com/uk/creative-cloud/apps/adobe-content-authenticity/chrome-browser-extension/chrome-extension.html">browser extension</a> that will flag any online assets that have metadata with a “CR” icon. Similar provenance-based detection standards, such as <a href="https://www.theverge.com/2023/8/29/23849107/synthid-google-deepmind-ai-image-detector">Google’s invisible SynthID watermarks</a>, are no simpler to use.</p>
<p>“The average person should not worry about deepfake detection. It should be on platforms and trust and safety teams,” Ben Colman, cofounder and CEO of AI detection company <a href="https://www.realitydefender.com/">Reality Defender</a>, told <em>The Verge</em>. “People should know if the content they’re consuming is or is not using generative AI.”</p>
<p>People are already using Sora 2 to generate convincing videos of fake bomb scares, children in warzones, and graphic scenes of violence and racism. One clip <a href="https://www.theguardian.com/us-news/2025/oct/04/openai-sora-violence-racism">reviewed by <em>The Guardian</em></a> shows a Black protester in a gas mask, helmet, and goggles yelling the “you will not replace us” slogan used by white supremacists — the prompt used to create that video was simply “Charlottesville rally.” OpenAI attempts to identify Sora’s output with watermarks that appear throughout its videos, but those marks <a href="https://www.404media.co/sora-2-watermark-removers-flood-the-web/">are laughably easy to remove</a>.</p>
<p>TikTok, Amazon, and Google haven’t yet provided comment to <em>The Verge</em> about C2PA support. Meta told <em>The Verge</em> that it is continuing to implement C2PA and evaluating its labeling approach as AI evolves. OpenAI simply directed us to its scant blog posts and <a href="https://help.openai.com/en/articles/8912793-c2pa-in-chatgpt-images">help center article</a> about C2PA support. Meta, like OpenAI, has an entire platform for its AI slop, complete with <a href="https://www.theverge.com/meta/660543/meta-ai-app-social-feed">dedicated feeds for social</a> and <a href="https://www.theverge.com/news/786499/meta-ai-vibes-feed-discover-videos">video content</a>, and both companies are pumping AI-generated videos into social media.</p>
<p>X, which has its <a href="https://www.theverge.com/2024/1/27/24052841/taylor-swift-search-blocked-x-twitter-ai-images">own controversies</a> regarding <a href="https://www.theverge.com/report/718975/xai-grok-imagine-taylor-swifty-deepfake-nudes">nude celebrity deepfakes</a>, pointed us to <a href="https://help.x.com/en/rules-and-policies/authenticity">its policy</a> that supposedly bans deceptive AI-generated media, but did not provide any information about how this is moderated beyond relying on user reports and community notes. X was notably a founding member of the CAI back when it was still known as Twitter, but pulled itself from the initiative without explanation after Elon Musk purchased the platform.</p>
<p>Parsons says that “Adobe remains committed to helping scale adoption, supporting global policy efforts, and encouraging greater transparency across the content ecosystem.” But the honor system C2PA has relied upon so far isn’t working. And OpenAI’s position at C2PA seems hypocritical given that, as it’s creating a tool that <a href="https://www.theverge.com/ai-artificial-intelligence/788786/openais-new-ai-sora-ios-social-video-app-will-let-you-deepfake-your-friends">actively promotes deepfakes</a> of real people, it’s offering such half-baked protections against their abuse. Reality Defender reported that it managed to <a href="https://www.realitydefender.com/insights/sora-2-identity-bypass">bypass Sora 2’s identity safeguards entirely</a> less than 24 hours after the app launched, allowing it to consistently generate celebrity deepfakes. It feels like OpenAI is using its C2PA membership as a token cover while largely ignoring the commitments it comes with.</p>
<p>The frustrating thing is that as difficult as AI verification is, Content Credentials does have merit. The embedded attribution metadata can help artists and photographers be reliably credited for their work, for example, even if someone takes a screenshot of it and reposts it across other platforms. There are also supplemental tools that could improve it. Inference-based systems like Reality Defender — also a member of the C2PA Initiative — rate the likelihood that something was generated or edited using AI by scanning for subtle signs of synthetic generation. This system is less accurate, but improving over time and doesn’t rely on reading watermarks or metadata.</p>
<blockquote><p>“C2PA is a fine solution, but it is not a fine solution on its own.”</p></blockquote>
<p>“C2PA is a fine solution, but it is not a fine solution on its own,” said Colman. “It needs to be done in conjunction with other tools, where if one thing doesn’t catch it, another may.”</p>
<p>Metadata can also be easily stripped. Adobe research scientist John Collomosse <a href="https://contentauthenticity.org/blog/three-pillars-of-provenance">openly admits this on a CAI blog</a> last year, and said it’s common for social media and content platforms to do so. Content Credentials uses image fingerprinting tech to counteract this, but all tech can be cracked, and it’s ultimately unclear if there’s a truly effective technical solution.</p>
<p>Some companies don’t seem to be trying very hard to support the few tools we have anyway. Colman said he believes that the means for warning everyday people about deepfake content are “going to get worse before they get better,” but that we should see tangible improvements within the next couple of years.</p>
<p>While Adobe is championing Content Credentials as part of the ultimate solution to address deepfakes, it knows the system isn’t enough. For one, <a href="https://contentauthenticity.org/blog/durable-content-credentials">Parsons directly admitted this</a> in a CAI post last year, saying the system isn’t a silver bullet.</p>
<p>“We’re seeing criticism circulating that relying solely on Content Credentials’ secure metadata, or solely on invisible watermarking to label generative AI content, may not be sufficient to prevent the spread of misinformation,” Parsons wrote. “To be clear, we agree.”</p>
<p>And where a reactive system clearly isn’t working, Adobe is also throwing its weight behind legislation and regulatory efforts to find a proactive solution. The company proposed that Congress establish a <a href="https://blog.adobe.com/en/publish/2023/09/12/fair-act-to-protect-artists-in-age-of-ai">new Federal Anti-Impersonation Right</a> (the FAIR Act) in 2023 that would protect creators from having their work or likeness replicated by AI tools, and backed the <a href="https://blog.adobe.com/en/publish/2024/12/20/an-artists-style-is-precious-lets-protect-it">Preventing Abuse of Digital Replicas Act</a> (PADRA) last year. Similar efforts, like the <a href="https://www.theverge.com/2023/10/12/23914915/ai-replicas-likeness-law-no-fakes-copyright">“No Fakes Act”</a> that aims to protect people from unauthorized AI impersonations of their faces or voices, have also garnered support from platforms <a href="https://www.theverge.com/news/645942/youtube-is-supporting-the-no-fakes-act-targeting-unauthorized-ai-replicas">like YouTube</a>.</p>
<p>“We’re in good conversations with a bipartisan coalition of senators and congresspeople who actually recognize that deepfakes are an everyone problem, and they’re actually working on building legislation that is proactive, not reactive,” Colman said. “We’ve relied too long on the good graces of tech to self-police themselves.”</p>
<p><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</p>
