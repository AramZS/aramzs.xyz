---
author: leaflet.pub
cover_image: >-
  https://leaflet.pub/lish/did%253Aplc%253A4qsyxmnsblo4luuycm3572bq/3majnsnvafs2b/3maqpvianlc2a/opengraph-image?f3a3a13b3930d7d9
date: '2026-01-05T13:10:05.131Z'
dateFolder: 2026/01/05
description: Why AI makes the hidden economics of software unavoidable
isBasedOn: 'https://aicoding.leaflet.pub/3maqpvianlc2a'
link: 'https://aicoding.leaflet.pub/3maqpvianlc2a'
slug: 2026-01-05-httpsaicodingleafletpub3maqpvianlc2a
tags:
  - ai
  - code
  - tech
title: Code Was Never the Asset
---
<p>When we talk about software economics in the age of generative AI, it can feel like we’re inventing something entirely new. But the truth is that many of the economic pressures now simply reveal dynamics that have always existed beneath the surface of software development.</p>
<h3 data-index="2">The Myth of Code as Capital</h3>
<p>For decades, most engineering cultures treated code like a durable asset. The prevailing mindset was:</p>
<p>This made sense when the dominant cost of software was writing it: hiring, training, manual coding, test cycles, and team coordination absorbed most of the budget. But that interpretation created a myth: that code itself is valuable capital.</p>
<p>It isn’t.</p>
<p>Legacy systems — codebases decades old still running mission-critical functions — are often costly only because they are expensive to understand and maintain, not because their lines of code are inherently valuable. According to industry definitions, a legacy system is code that continues to serve a purpose but has become burdensome to evolve because of outdated technology or missing automated tests and documentation.</p>
<p>Even the term <a href="https://www.ibm.com/think/topics/legacy-code">“legacy code”</a> in software engineering — code without tests — implies maintenance risk, not long-term capital value.</p>
<h3 data-index="10">Systems Already Reveal the Hidden Economics</h3>
<p>Look at how successful evolutionary modernization of legacy systems happens in practice. Thoughtworks and other practitioners <a href="https://martinfowler.com/articles/legacy-modernization-gen-ai.html">favor </a><a href="https://martinfowler.com/articles/legacy-modernization-gen-ai.html">incremental, evolutionary strategies</a> over “big bang” rewrites because they reduce risk and cost.</p>
<p>What do these approaches have in common?</p>
<p>These are not new economic insights. They’re how humans have long coped with complexity when preserving old code becomes more expensive than replacing it.</p>
<p>Even before AI, many approaches (<a href="https://www.youtube.com/watch?v=qH_y45he4-o">including my own</a>) to evolutionary architecture were solidified as responses to the high cost of maintaining monolithic legacy codebases. They accept that restating, reshaping, and replacing parts of systems can be cheaper than preserving and patching old ones.</p>
<h3 data-index="17">Pace Layers: How Software Already Had Multiple Cost Regimes</h3>
<p>As discussed in a previous post, <a href="https://aicoding.leaflet.pub/3maob46kbz22v">a useful lens for understanding why not all code should be treated the same is </a><a href="https://aicoding.leaflet.pub/3maob46kbz22v">pace layering</a> — a model first articulated by Stewart Brand to explain how complex systems adapt and endure.</p>
<p>In Brand’s original framing, different layers of a system evolve at different speeds:</p>
<p>Applied to software, this predicts that some parts of a system ought to change rapidly, and others slowly — because the economic cost and impact of change differ by layer.</p>
<p>This insight was true long before AI. In practice:</p>
<p>Traditional engineering already valued some code as more durable because its replacement was expensive. This matches Brand’s argument that “fast learns, slow remembers”. Fast layers respond quickly to shocks while slow layers retain memory and continuity.</p>
<h3 data-index="26">AI Reveals What We Already Ignored</h3>
<p>Generative AI collapses the cost of producing code, but not the cost of understanding it. Writing is cheap; comprehension is expensive. This exposes a core truth that legacy practitioners already knew instinctively:</p>
<blockquote data-index="28">Software isn’t valuable because it exists and serves a purpose. its value also lies in the requirement that we can reason about it, evolve it safely, and trust its behavior.</blockquote>
<p>That’s why legacy systems become expensive: the burden of understanding and maintaining code outweighs its utility. Traditional approaches like software archaeology — reverse-engineering undocumented code — are symptomatic of organizations trying to carry cost forward because retiring code was harder than preserving it.</p>
<p>AI accelerates this pressure.</p>
<h3 data-index="32">Why This Matters Now</h3>
<p>Today, as tools can generate massive amounts of code with little human effort, the economic question is no longer "How do we write code efficiently?" It's "How do we reduce the long-term cost of what we write?"</p>
<p>That question demands we rethink what we treat as persistent. Underneath the shiny façade of AI productivity, the real economic driver will be systems that minimize the cost of comprehension, evaluation, and replacement.</p>
<p>More on this in the following posts.</p>
