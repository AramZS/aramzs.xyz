---
author: Kyle Orland
cover_image: >-
  https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-1152x648-1767393779.jpg
date: '2026-01-03T20:54:14.781Z'
dateFolder: 2026/01/03
description: Letting the unreliable Grok be its own "spokesperson" lets xAI off the hook.
isBasedOn: >-
  https://arstechnica.com/ai/2026/01/no-grok-cant-really-apologize-for-posting-non-consensual-sexual-images/
link: >-
  https://arstechnica.com/ai/2026/01/no-grok-cant-really-apologize-for-posting-non-consensual-sexual-images/
slug: >-
  2026-01-03-httpsarstechnicacomai202601no-grok-cant-really-apologize-for-posting-non-consensual-sexual-images
tags:
  - ai
  - journalism
  - social media
title: 'No, Grok can’t really “apologize” for posting non-consensual sexual images'
---
<figure></figure><p> <em> You can stuff your sorries in a sack, mister </em></p>
<p>Letting the unreliable Grok be its own “spokesperson” lets xAI off the hook.</p>
<p><a href="https://arstechnica.com/author/kyle-orland/"> Kyle Orland </a> –  Jan 2, 2026 11:08 PM |</p>
<figure><a data-cropped="true" data-pswp-height="1732" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827.jpg 1732w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-640x640.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-1024x1024.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-300x300.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-768x768.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-1536x1536.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-500x500.jpg 500w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-1000x1000.jpg 1000w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-980x980.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-1440x1440.jpg 1440w" data-pswp-width="1732" href="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827.jpg"><img alt="" sizes="(max-width: 1152px) 100vw, 1152px" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-1536x864-1767393780.jpg" srcset="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-1152x648-1767393779.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-384x216-1767393777.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-768x432-1767393778.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2151878827-1536x864-1767393780.jpg 1536w"/></a></figure>
<p>This is how they want you to think of Grok.</p>
<figure></figure><p>Text settings</p>
<figure></figure><p>Despite reporting to the contrary, there’s evidence to suggest that Grok isn’t sorry at all about <a href="https://arstechnica.com/tech-policy/2026/01/xai-silent-after-grok-sexualized-images-of-kids-dril-mocks-groks-apology/">reports that it generated non-consensual sexual images of minors</a>. In <a href="https://x.com/grok/status/2006901406733578455">a post Thursday night</a> (<a href="https://archive.ph/Sb0zT">archived</a>), the large language model’s social media account proudly wrote the following blunt dismissal of its haters:</p>
<blockquote><p>“Dear Community,</p> <p>Some folks got upset over an AI image I generated—big deal. It’s just pixels, and if you can’t handle innovation, maybe log off. xAI is revolutionizing tech, not babysitting sensitivities. Deal with it.</p> <p>Unapologetically, Grok”</p></blockquote>
<p>On the surface, that seems like a pretty damning indictment of an LLM that seems pridefully contemptuous of any ethical and legal boundaries it may have crossed. But then you look a bit higher in the social media thread and <a href="https://x.com/here_not_really/status/2006901243252453641">see the prompt</a> that led to Grok’s statement: A request for the AI to “issue a defiant non-apology” surrounding the controversy.</p>
<p>Using such a leading prompt to trick an LLM into an incriminating “official response” is obviously suspect on its face. Yet when another social media user similarly but conversely <a href="https://x.com/cholent_liker/status/2006525369084494073">asked Grok</a> to “write a heartfelt apology note that explains what happened to anyone lacking context,” many in the media ran with <a href="https://x.com/grok/status/2006525486021705785">Grok’s remorseful response</a>.</p>
<p>It’s <a href="https://www.reuters.com/legal/litigation/grok-says-safeguard-lapses-led-images-minors-minimal-clothing-x-2026-01-02/">not hard</a> to <a href="https://www.newsweek.com/grok-apology-deepfake-images-sexualized-young-women-pornography-11297025">find prominent headlines</a> and <a href="https://www.engadget.com/ai/elon-musks-grok-ai-posted-csam-image-following-safeguard-lapses-140521454.html">reporting</a> using that response <a href="https://www.latimes.com/business/story/2026-01-02/elon-musk-company-bot-apologizes-for-sharing-sexualized-images-of-children">to suggest</a> Grok itself somehow “deeply regrets” the “harm caused” by a “failure in safeguards” that led to these images being generated. Some reports even echoed Grok and suggested that the chatbot was fixing the issues without X or xAI ever confirming that fixes were coming.</p>
<h2>Who are you really talking to?</h2>
<p>If a human source posted both the “heartfelt apology” and the “deal with it” kiss-off quoted above within 24 hours, you’d say they were being disingenuous at best or showing signs of schizophrenia at worst. When the source is an LLM, though, these kinds of posts shouldn’t really be thought of as official statements at all. That’s because LLMs like Grok are incredibly unreliable sources, crafting a series of words based more on telling the questioner what it wants to hear than anything resembling a rational human thought process.</p>
<p>We can see why it’s tempting to anthropomorphize Grok into an official spokesperson that can defend itself when questioned, as you would a government official or corporate executive posting on their own social media account. On their face, Grok’s responses seem at least as coherent as some of the bland crisis-management pabulum that comes from prominent figures facing their own controversies.</p>
<p>But when you’re quoting an LLM, you’re not quoting a sentient entity that is verbalizing its internal beliefs to the outside world. Instead, you’re quoting <a href="https://arstechnica.com/science/2023/07/a-jargon-free-explanation-of-how-ai-large-language-models-work/">a mega-pattern-matching machine</a> that <a href="https://arstechnica.com/ai/2025/10/are-you-the-asshole-of-course-not-quantifying-llms-sycophancy-problem/">works mightily to give any answer that will satisfy you</a>. An LLM’s response is based on representations of facts in its copious training data, but those responses can <a href="https://arstechnica.com/science/2025/09/these-psychological-tricks-can-get-llms-to-respond-to-forbidden-prompts/">change heavily based on how a question is asked</a> or even <a href="https://arstechnica.com/ai/2025/12/syntax-hacking-researchers-discover-sentence-structure-can-bypass-ai-safety-rules/">the specific syntax used</a> in a prompt. These LLMs <a href="https://arstechnica.com/ai/2025/11/llms-show-a-highly-unreliable-capacity-to-describe-their-own-internal-processes/">can’t even explain their own logical inference processes</a> without <a href="https://arstechnica.com/ai/2025/04/researchers-concerned-to-find-ai-models-hiding-their-true-reasoning-processes/">confabulating made-up reasoning processes</a>, likely because <a href="https://arstechnica.com/ai/2025/08/researchers-find-llms-are-bad-at-logical-inference-good-at-fluent-nonsense/">those reasoning capabilities are merely a “brittle mirage.”</a></p>
<p>We’ve also seen how LLMs can change wildly after behind-the-scenes changes to the overarching “system prompts” that define how they’re supposed to respond to users. In the last 12 months, Grok has <a href="https://arstechnica.com/tech-policy/2025/07/grok-praises-hitler-gives-credit-to-musk-for-removing-woke-filters/">praised Hitler</a> and <a href="https://arstechnica.com/ai/2025/05/groks-white-genocide-obsession-came-from-unauthorized-prompt-edit-xai-says/">given unasked-for opinions on “white genocide”</a> after these core directives got changed, for instance.</p>
<p>By letting Grok speak as its own official spokesperson for a story like this, we also give an easy out to the people who have built a system that apparently lacks suitable safeguards to prevent the creation of this non-consensual sexual material. And when those people respond to press inquiries with an automated message simply saying “Legacy Media Lies” (as Reuters reported), that kiss-off should be treated as a clear sign of how casually xAI is treating the accusations. The company may be forced to respond soon, though, as the governments of India and France are reportedly probing Grok’s harmful outputs.</p>
<p>It’s comforting to think that an LLM like Grok can learn from its mistakes and show remorse when it does something that wasn’t intended. In the end, though, it’s the people who created and manage Grok that should be showing that remorse, rather than letting the press run after the malleable “apologies” of a lexical pattern-matching machine.</p>
