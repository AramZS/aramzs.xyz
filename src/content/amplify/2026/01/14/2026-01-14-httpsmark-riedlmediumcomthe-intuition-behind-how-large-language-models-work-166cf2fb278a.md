---
author: Mark Riedl
cover_image: 'https://miro.medium.com/v2/resize:fit:1035/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg'
date: '2026-01-14T13:32:52.021Z'
dateFolder: 2026/01/14
description: >-
  Large Language Models (LLMs) are fancy artificial neural networks. But you
  don‚Äôt have time to learn the math or engineering. Unfortunately‚Ä¶
isBasedOn: >-
  https://mark-riedl.medium.com/the-intuition-behind-how-large-language-models-work-166cf2fb278a
link: >-
  https://mark-riedl.medium.com/the-intuition-behind-how-large-language-models-work-166cf2fb278a
slug: >-
  2026-01-14-httpsmark-riedlmediumcomthe-intuition-behind-how-large-language-models-work-166cf2fb278a
tags:
  - tech
title: 'The Intuition Behind How Large Language Models Work, Part I'
---
<p>Large Language Models (LLMs) are fancy artificial neural networks. But you don‚Äôt have time to learn the math or engineering. Unfortunately, a lot of primers will throw up diagrams like this:</p>
<figure><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 320px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*YTQXNzOWR_h72qobG1XfMg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*YTQXNzOWR_h72qobG1XfMg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*YTQXNzOWR_h72qobG1XfMg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*YTQXNzOWR_h72qobG1XfMg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*YTQXNzOWR_h72qobG1XfMg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*YTQXNzOWR_h72qobG1XfMg.png 1100w, https://miro.medium.com/v2/resize:fit:640/format:webp/1*YTQXNzOWR_h72qobG1XfMg.png 640w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 320px" srcset="https://miro.medium.com/v2/resize:fit:640/1*YTQXNzOWR_h72qobG1XfMg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*YTQXNzOWR_h72qobG1XfMg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*YTQXNzOWR_h72qobG1XfMg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*YTQXNzOWR_h72qobG1XfMg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*YTQXNzOWR_h72qobG1XfMg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*YTQXNzOWR_h72qobG1XfMg.png 1100w, https://miro.medium.com/v2/resize:fit:640/1*YTQXNzOWR_h72qobG1XfMg.png 640w"/><img alt="" src="https://miro.medium.com/v2/resize:fit:320/1*YTQXNzOWR_h72qobG1XfMg.png"/></picture><figcaption>The Large Language Model as it is commonly presented.</figcaption></figure>
<p>What the hell is that üëÜ? Nah‚Ä¶</p>
<p>What you really need is the <strong>intuition</strong> behind why LLMs work. And what those intuitions might tell us about what LLMs can and cannot do.</p>
<p>So what happens when you type a prompt into a chatbot like ChatGPT, Claude, Gemini, Copilot, or Grok?</p>
<h1 data-selectable-paragraph="">Let‚Äôs play a game</h1>
<p>Before we get started, let‚Äôs play a game (I promise it‚Äôs relevant). Try to guess the word I am thinking:</p>
<blockquote><p data-selectable-paragraph="">__________</p></blockquote>
<p>What did you guess? There are a lot of words in the English language alone. You are probably wrong. So that isn‚Äôt very fair of me. Let me give you the word that came right before the one I am thinking of:</p>
<blockquote><p data-selectable-paragraph="">litter ___________</p></blockquote>
<p>What word am I thinking of? You might be able to narrow all the English words down to ‚Äúbin‚Äù, or ‚Äúbox‚Äù, or ‚Äúmates‚Äù. I‚Äôm probably not thinking of ‚Äúavocado‚Äù. How do you know that? Because some words occur together more often than not in your experience. Great, let me give you the three words before the word I‚Äôm thinking of:</p>
<blockquote><p data-selectable-paragraph="">used the litter __________</p></blockquote>
<p>Now you are probably thinking ‚Äúbox‚Äù but you might not be able to rule out ‚Äúbin‚Äù. One more preceding word:</p>
<blockquote><p data-selectable-paragraph="">cat used the litter __________</p></blockquote>
<p>At this point it seems very likely I am thinking of the word ‚Äúbox‚Äù. Congratulations.</p>
<p>What was that all about? The preceding words are <strong>clues</strong><em>. </em>The preceding words made certain next words more likely, and you made an educated guess based on the most probable word combinations that linked back to the preceding words.</p>
<p>If you pick the most probably word and add it to the end of the sentence, you‚Äôve just generated a word to add to the end of the sentence. And if you keep looking back at the growing list of words and picking really probably words, you can generate sentences then paragraphs, then entire documents.</p>
<p>That is what a Large Language Model does. That‚Äôs it.</p>
<h1 data-selectable-paragraph="">LLMs Are playing deduction games</h1>
<p>Let‚Äôs look at a somewhat more complicated example:</p>
<blockquote><p data-selectable-paragraph="">The cat left its litter mates and used the litter __________</p></blockquote>
<p>You probably zeroed in on ‚Äúbox‚Äù and the best next word again. This time I‚Äôve used the word ‚Äúlitter‚Äù twice in the same sentence, but in different ways. Why doesn‚Äôt an LLM guess ‚Äúmates‚Äù because the word that came after the first ‚Äúlitter‚Äù was ‚Äúmates‚Äù. Not all words in this sentence are equally influential on the missing last word. When you read the sentence you probably intuit some unstated relationships between words.</p>
<figure><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*g11x_VVM0JSld1YPRK6Syw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*g11x_VVM0JSld1YPRK6Syw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*g11x_VVM0JSld1YPRK6Syw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*g11x_VVM0JSld1YPRK6Syw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*g11x_VVM0JSld1YPRK6Syw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*g11x_VVM0JSld1YPRK6Syw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g11x_VVM0JSld1YPRK6Syw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*g11x_VVM0JSld1YPRK6Syw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*g11x_VVM0JSld1YPRK6Syw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*g11x_VVM0JSld1YPRK6Syw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*g11x_VVM0JSld1YPRK6Syw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*g11x_VVM0JSld1YPRK6Syw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*g11x_VVM0JSld1YPRK6Syw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*g11x_VVM0JSld1YPRK6Syw.png 1400w"/><img alt="" src="https://miro.medium.com/v2/resize:fit:700/1*g11x_VVM0JSld1YPRK6Syw.png"/></picture></figure>
<p>The first ‚Äúlitter‚Äù isn‚Äôt particularly related to the second use of the word ‚Äúlitter‚Äù or the word ‚Äúmates‚Äù. But ‚Äúcat‚Äù answers the question ‚Äúwho is using the litter‚Äù and is thus highly relevant even though the word is farther away. We work our way through the implications of the words on each other, we get a sense for what is important and what isn‚Äôt important and factor that into our guess.</p>
<p>I‚Äôm not saying that a Large Language Model does sentence diagramming. But that intuition will sort of work. The LLM is looking at all pairs of words and assessing if they are relevant to each other or not. And if they are relevant to the missing next word, then they contribute more heavily to the next word guess.</p>
<p><strong>So, LLMs are playing a fancy deduction game to try to narrow down the possible words and guess what you want it to say.</strong></p>
<p>There are some implications to this. Most notably: the more words you put in the prompt the easier it is to guess the word that should come next. So if you are asking a question, you get a better answer when you given it more information about what you are asking for. If you are writing a story, then the more information you give about the beginning of the story, the characters, and so on, the more the next generated part will probably match what you were hoping for. When it comes to prompts, the more the better. The more clues you give the LLM, the better it performs.</p>
<p>Another implication: LLMs do not think the way we think. They are not perfect oracles. They are not giant repositories of truth. They are not wise. The word ‚Äúwisdom‚Äù doesn‚Äôt even apply. They are fancy word guessers. More than that, they are trying to guess what you want to hear. If you want to hear answers to questions, the LLM might be able to guess the words that make up the answer. But it‚Äôs just a guess. Facts are more probable than non-facts, so you might actually get a right answer. But you also might not. You might want to hear about yourself, and it will guess words that make you feel good about yourself, even if you are self-deceptive or having a mental breakdown or getting swept up in conspiracy theories in the news.</p>
<p>If you interact with a chatbot you should always have two things in mind:</p>
<ol><li data-selectable-paragraph="">The chatbot is trying to guess the words you want to hear.</li><li data-selectable-paragraph="">The chatbot isn‚Äôt thinking about the implications of its words, what you are trying to achieve, why you want the generated response, or what happens to you it makes the wrong guess.</li></ol>
<p>Okay, so if you want to stop reading now, you have the intuition you need to protect yourself from over-estimating LLMs.</p>
<h1 data-selectable-paragraph="">How does it work?</h1>
<p>Are you still reading? You probably want to know a little bit more about <strong>how</strong> an LLM plays the fancy deduction game to guess words. Okay, we can do that without getting into math or neural network architecture diagrams.</p>
<p>What happens inside a LLM when I enter a prompt like:</p>
<blockquote><p data-selectable-paragraph="">The cat left its litter mates and uses the litter __________</p></blockquote>
<p>Large Language Models are a type of artificial neural network called a <em>Transformer</em>. It is going to go through the process of transforming your words into‚Ä¶ something. Before we get to what that something is, let‚Äôs walk through the transformation process. There are basically two transformations that occur over and over again:</p>
<ol><li data-selectable-paragraph=""><em>Embedding</em></li><li data-selectable-paragraph=""><em>Attention</em></li></ol>
<h2 data-selectable-paragraph="">Embedding</h2>
<p>Everything starts off embedding. This process converts words (technically tokens, but you can roughly think of tokens as words or parts of words) into arrays of numbers.</p>
<figure><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*aUKKMUHSQRTEJrCfnCSujA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*aUKKMUHSQRTEJrCfnCSujA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*aUKKMUHSQRTEJrCfnCSujA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*aUKKMUHSQRTEJrCfnCSujA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*aUKKMUHSQRTEJrCfnCSujA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*aUKKMUHSQRTEJrCfnCSujA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUKKMUHSQRTEJrCfnCSujA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*aUKKMUHSQRTEJrCfnCSujA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*aUKKMUHSQRTEJrCfnCSujA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*aUKKMUHSQRTEJrCfnCSujA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*aUKKMUHSQRTEJrCfnCSujA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*aUKKMUHSQRTEJrCfnCSujA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*aUKKMUHSQRTEJrCfnCSujA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*aUKKMUHSQRTEJrCfnCSujA.png 1400w"/><img alt="" src="https://miro.medium.com/v2/resize:fit:700/1*aUKKMUHSQRTEJrCfnCSujA.png"/></picture></figure>
<p>The arrays are hundreds or thousands of numbers long and not just 1s and 0s, but I think this is easier to visualize. But arrays of numbers are hard to visualize too, so let‚Äôs just stick with words. Just remember, each word from this point forward is now just a convenient human-readable place-holder for an array of numbers that means the same thing.</p>
<figure><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*_UgIERzuRAIMVg6_GjmxqA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*_UgIERzuRAIMVg6_GjmxqA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*_UgIERzuRAIMVg6_GjmxqA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*_UgIERzuRAIMVg6_GjmxqA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*_UgIERzuRAIMVg6_GjmxqA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_UgIERzuRAIMVg6_GjmxqA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_UgIERzuRAIMVg6_GjmxqA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*_UgIERzuRAIMVg6_GjmxqA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*_UgIERzuRAIMVg6_GjmxqA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*_UgIERzuRAIMVg6_GjmxqA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*_UgIERzuRAIMVg6_GjmxqA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*_UgIERzuRAIMVg6_GjmxqA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*_UgIERzuRAIMVg6_GjmxqA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*_UgIERzuRAIMVg6_GjmxqA.png 1400w"/><img alt="" src="https://miro.medium.com/v2/resize:fit:700/1*_UgIERzuRAIMVg6_GjmxqA.png"/></picture></figure>
<h2 data-selectable-paragraph="">Attention</h2>
<p>The next process is <strong>attention</strong>. You might have heard people reference the phrase ‚Äúattention is all you need‚Äù. That is the rather bold title of the paper from Google that introduced the Transformer language model. So what is attention? Attention is a processes whereby each word ‚Äúattends‚Äù to each other word and decides if they are related or not.</p>
<figure><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*_QwSBkupZNj7wjJnvDjA0Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*_QwSBkupZNj7wjJnvDjA0Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*_QwSBkupZNj7wjJnvDjA0Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*_QwSBkupZNj7wjJnvDjA0Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*_QwSBkupZNj7wjJnvDjA0Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_QwSBkupZNj7wjJnvDjA0Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_QwSBkupZNj7wjJnvDjA0Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*_QwSBkupZNj7wjJnvDjA0Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*_QwSBkupZNj7wjJnvDjA0Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*_QwSBkupZNj7wjJnvDjA0Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*_QwSBkupZNj7wjJnvDjA0Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*_QwSBkupZNj7wjJnvDjA0Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*_QwSBkupZNj7wjJnvDjA0Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*_QwSBkupZNj7wjJnvDjA0Q.png 1400w"/><img alt="" src="https://miro.medium.com/v2/resize:fit:700/1*_QwSBkupZNj7wjJnvDjA0Q.png"/></picture></figure>
<p>Some words are related and others are less related. I‚Äôve drawn an arc between each pair of words. I‚Äôve bolded the ones that are highly related. The rest are less related. ‚ÄúCat‚Äù and ‚Äúits‚Äù are related because they both seem to refer to the same entity. ‚ÄúCat‚Äù and ‚Äúleft‚Äù are related because one indicates the entity that is performing the verb action. And so on.</p>
<h2>Get Mark Riedl‚Äôs stories in your inbox</h2>
<p>Join Medium for free to get updates from this writer.</p>
<p>The fact that this looks like the earlier sentence-diagramming diagram is not a coincidence. The arcs don‚Äôt have nice neat labels that describe what the relationships are.</p>
<p>Wait a minute‚Ä¶ how do words know if they should be related? How does the first ‚Äúlitter‚Äù know it is related to the ‚Äúmates‚Äù and the second ‚Äúlitter‚Äù know it isn‚Äôt related to ‚Äúmates‚Äù? It is the job of the embedding to make sure words that should be highly related to each are converted into number arrays that are very similar to each other. If you look back, you will notice that the different ‚Äúlitters‚Äù have different number arrays, and that the array for the first litter is much more similar to the array for ‚Äúmates‚Äù than it is for ‚Äúuse‚Äù and vice versa. But how did the embedding know to do that? If the embedding makes arrays similar for the right set of words, the whole neural network makes a better guess and gets rewarded. But if the embedding makes arrays similar for unrelated words, like ‚Äúto‚Äù and ‚Äúcat‚Äù, it is going to have trouble making good guesses, and it gets punished. The embedding is where learning happens. Ignoring the details, the embedding tries different ways of embedding words until it gets good at guessing. That‚Äôs neural network learning in a nutshell. The embedding is going to have to sort through a lot of possible combinations, and that is going to take a long time and require a lot of computing power. But for now, let‚Äôs assume that the embedding has figured how to make really useful arrays.</p>
<p>When words are determined to be related (because their number arrays are similar), the number arrays are merged together to create new words. In the diagram above, I am not showing the number arrays, and instead showing things like ‚Äúcat+left‚Äù. This does not literally mean ‚Äúcat‚Äù and ‚Äúleft‚Äù are added together. What is means is that a new number array is created that means something like ‚Äúa cat that left‚Äù. You can think of this as a completely made up word that just means ‚Äúa cat that left‚Äù. So now with my new completely made up word, I can distinguish between a cat and a cat in the process of leaving something. Likewise, I have a new completely made up word that is used specifically to talk about mates that have been left (as opposed to just mates).</p>
<p>Close your eyes, and imagine a cat just sitting there. Close your eyes again and imagine a cat in the process of leaving. You have concepts for these two things in your mind, you just tend to use the same words, or combine words to express them as language.</p>
<p>That is what the <strong>Transformer</strong> is doing: it is <em>transforming</em> collections of words into more abstract concepts.</p>
<p>It is very powerful to be able to have unique words to distinguish between the type of litter that consist of mates and the type of litter that is used. You can probably already start to see how the words that would follow left+mates (the concept of the type of litter in the form of mates) would be different from the words that would follow use+litter (the concept of the type of litter that can be used).</p>
<p>Okay that was a lot going on in the attend phase. But we aren‚Äôt done. We now repeat these steps.</p>
<h2 data-selectable-paragraph="">More embedding</h2>
<p>Attention is followed by another embedding phase.</p>
<figure><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg 1400w"/><img alt="" src="https://miro.medium.com/v2/resize:fit:700/1*M76lBVcuS8QKz6jmmSrnIQ.jpeg"/></picture></figure>
<p>The made up words are converted into arrays of numbers. For clarity of visualization I will use the ‚Äúcat+left‚Äù notation and will have to remember that the only thing actually there is an array of numbers ‚Äî in this case a unique combination of numbers to express the made up word.</p>
<h2 data-selectable-paragraph="">More attention</h2>
<p>And then we do another attention phase. This time we have made up words attending to other made up words.</p>
<figure><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*_ylQGI7nmSdXOn7aRVRHGw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*_ylQGI7nmSdXOn7aRVRHGw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*_ylQGI7nmSdXOn7aRVRHGw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*_ylQGI7nmSdXOn7aRVRHGw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*_ylQGI7nmSdXOn7aRVRHGw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_ylQGI7nmSdXOn7aRVRHGw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ylQGI7nmSdXOn7aRVRHGw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*_ylQGI7nmSdXOn7aRVRHGw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*_ylQGI7nmSdXOn7aRVRHGw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*_ylQGI7nmSdXOn7aRVRHGw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*_ylQGI7nmSdXOn7aRVRHGw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*_ylQGI7nmSdXOn7aRVRHGw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*_ylQGI7nmSdXOn7aRVRHGw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*_ylQGI7nmSdXOn7aRVRHGw.png 1400w"/><img alt="" src="https://miro.medium.com/v2/resize:fit:700/1*_ylQGI7nmSdXOn7aRVRHGw.png"/></picture></figure>
<p>By having made up words attend to other made up words, we take abstract concepts and make new words to express even more abstract concepts. Now we have a special word just to mean a cat that left its mates. We are not just talking about any old cat, or any old cat that left something, we now have a special word just to mean a cat that left its mates.</p>
<p>The original prompt has been transformed into the sequence: cat+left+mates, left+litter+mates, cat+use+litter. Or in terms easier to read as humans: a cat that left it‚Äôs mates, the mates made up a litter that and was left, some litter that was used by a cat.</p>
<p>When you put it that way, how can there be any ambiguity about the word that comes next? And that is the point.</p>
<p>We run the embedding and attention phases many more times ‚Äî possibly 120 times or more in the case of the largest industrial grade LLMs like ChatGPT‚Äîto squeeze as many clues out as possible.</p>
<h2 data-selectable-paragraph="">Decoding</h2>
<p>The next stage is called <strong>decoding</strong>. I‚Äôm going to hand-wave the decoding quite a bit. The goal of the decoder is to covert the final set of made up words/concepts into a score for each possible next word. It actually involves more embedding and attention.</p>
<figure><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*nrU7LGgcSXRP80k0UOszfQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*nrU7LGgcSXRP80k0UOszfQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*nrU7LGgcSXRP80k0UOszfQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*nrU7LGgcSXRP80k0UOszfQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*nrU7LGgcSXRP80k0UOszfQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*nrU7LGgcSXRP80k0UOszfQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nrU7LGgcSXRP80k0UOszfQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*nrU7LGgcSXRP80k0UOszfQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*nrU7LGgcSXRP80k0UOszfQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*nrU7LGgcSXRP80k0UOszfQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*nrU7LGgcSXRP80k0UOszfQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*nrU7LGgcSXRP80k0UOszfQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*nrU7LGgcSXRP80k0UOszfQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*nrU7LGgcSXRP80k0UOszfQ.png 1400w"/><img alt="" src="https://miro.medium.com/v2/resize:fit:700/1*nrU7LGgcSXRP80k0UOszfQ.png"/></picture></figure>
<p>Each possible word gets a score based on how likely it is to follow the made up word/concepts. The word ‚Äúbox‚Äù is much more likely to come after cat+use+litter. Because if you were to transform the entire corpus of human writing like this, ‚Äúbox‚Äù will show up after cat+use+litter frequently and very few other words will ever appear after cat+use+litter.</p>
<h2 data-selectable-paragraph="">Sampling</h2>
<p>After decoding comes <strong>sampling</strong>. The easiest way to do sampling is to pick the word with the highest score. This will create a lot of repetitive text, so most chatbots instead pick randomly but proportionally to the score. This gives a bit of variability so the chatbot will never quite answer the same question the same way twice.</p>
<figure><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Hh-oRJI1tRkebPAU8-XQhA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Hh-oRJI1tRkebPAU8-XQhA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Hh-oRJI1tRkebPAU8-XQhA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Hh-oRJI1tRkebPAU8-XQhA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Hh-oRJI1tRkebPAU8-XQhA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Hh-oRJI1tRkebPAU8-XQhA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hh-oRJI1tRkebPAU8-XQhA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Hh-oRJI1tRkebPAU8-XQhA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Hh-oRJI1tRkebPAU8-XQhA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Hh-oRJI1tRkebPAU8-XQhA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Hh-oRJI1tRkebPAU8-XQhA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Hh-oRJI1tRkebPAU8-XQhA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Hh-oRJI1tRkebPAU8-XQhA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Hh-oRJI1tRkebPAU8-XQhA.png 1400w"/><img alt="" src="https://miro.medium.com/v2/resize:fit:700/1*Hh-oRJI1tRkebPAU8-XQhA.png"/></picture></figure>
<p>Okay, that‚Äôs it. A prompt is transformed into abstract words that uniquely provide clues about the best word to come next. The clues get converted into scores. The best scoring word gets selected.</p>
<p>The guessed word is added to the prompt and the whole process repeats, producing word after word after word. Congratulations, you know how a Large Language Model works. Everything else is just fancy math that makes these intuitions work.</p>
<h1 data-selectable-paragraph="">Some implications</h1>
<p>Now that we understand the process in a bit more detail, we can discuss some additional implications.</p>
<p>To reiterate some earlier implications more words means more clues. The more specific your prompt the better the response.</p>
<p>LLMs work best when you know the answer and can thus give it really good clues. Of course this often defeats the purpose of LLMs if one is using it to answer a question one doesn‚Äôt know the answer to. Being knowledgeable enough to know when an answer is right is really helpful. If you don‚Äôt you might want to verify the answer against other sources. Again, this feels like it defeats the purpose of using LLMs. Consider them one piece of an arsenal of resources for finding the answer to something.</p>
<p>Now, point to the part of the process where the LLM is required to tell the truth. I‚Äôll wait‚Ä¶</p>
<p>You can‚Äôt. Nowhere in embedding, attention, decoding, or sampling is there any mechanism that says something must be true to proceed. It‚Äôs all just turning combinations of words into clues. Sometimes the wrong words will be related to each other. This results in misleading clues, which in turn may result in poor guesses. A poorly chosen word gets added to the prompt and can in turn lead to more misleading clues and more poor guesses. Poor words compound on each other and the LLM doesn‚Äôt have any way of going back and making a different word choice. It just goes with it, and doubles down on everything.</p>
<p>For example, when responding to the prompt ‚ÄúWhat awards have Mark Riedl won?‚Äù, the LLM will start quite sensibly with ‚ÄúMark Riedl is a Professor of Computer Science at Georgia Tech and has won‚Ä¶‚Äù The problem is that I have not won that many awards, and many that I have won are not in publicly available data to train on. But the LLM cannot backtrack once it has gone this far. It must go on, each of its own words are clues on how to proceed. What awards might a computer scientist professor have one? A lot of people talk about the ‚ÄúTuring Award‚Äù in computer science so that might be a good guess based on the words so far. Except in reality I have not won any such award.</p>
<p>When this happens we call it a <strong>hallucination</strong>. The term <strong>confabulation</strong> is probably more accurate and less anthropomorphizing, but not widely adopted. A hallucination is just the situation where the generated words do not match our understanding of the real world. It‚Äôs annoying (or sometimes funny) when we realize that the words are not matching our understanding of the world. When we do not realize the words don‚Äôt match what the real world is like, we are in dangerous territory without knowing it. We might make important decisions based in information that doesn‚Äôt conform to the real world.</p>
<p>How do you know when an LLM is hallucinating? You don‚Äôt. If you could, we could design a mechanism to stop it from happening. We can‚Äôt.</p>
<p>The term hallucination just means output we don‚Äôt like, or we shouldn‚Äôt have liked in retrospect. That makes it really hard to deal with because it means an LLM needs to have a thorough understanding of the real world beyond just word use. People mostly talk and write about their experiences of the real world, so LLMs get things right a lot of the time. It‚Äôs easy to slip into complacency.</p>
<p><strong>The bottom line: </strong>Large Language Models are just guessing what you want to hear. They often come up with something that we want to hear based on the clues we leave. The better the clues we leave, the more likely we are to get what we want, such as a right answer to a question. But it is never guaranteed. The LLM can merge words into the wrong clues to follow or just make random guesses along the way. They can be very convincing even when hallucinating ‚Äî they don‚Äôt have <em>tells</em> like humans who are confabulating.</p>
<h1 data-selectable-paragraph="">What‚Äôs next?</h1>
<p>In <a data-discover="true" href="https://mark-riedl.medium.com/the-intuition-behind-how-large-language-models-work-part-ii-8c6a127a4a99">Part II</a>, I‚Äôll build off the intuitions behind LLMs presented here to discuss more advanced concepts that are used in production chatbots. One enhancement is called <strong>Retrieval-Augmented Generation</strong> (RAG), which combines web search with LLM generation, and is one way to reduce ‚Äî but not completely eliminate ‚Äî hallucination. Another enhancement is <strong>Chain of Thought </strong>(COT), which gives LLMS reasoning-like capabilities to solve more complex problems. We will look a bit into what it means for chatbots to be <strong>Agents</strong>.</p><p><a data-discover="true" href="https://mark-riedl.medium.com/the-intuition-behind-how-large-language-models-work-part-ii-8c6a127a4a99"><strong>The Intuition Behind How Large Language Models Work, Part II</strong></a></p><figure><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg 1400w"/><img alt="" src="https://miro.medium.com/v2/resize:fit:700/1*gcmhNNxF9Jvvv-JrMvLaHQ.jpeg"/></picture></figure>
