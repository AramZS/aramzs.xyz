---
author: Riana Pfefferkorn
cover_image: >-
  https://static01.nyt.com/images/2026/01/12/opinion/12pfefferkorn-image/12pfefferkorn-image-facebookJumbo.jpg
date: '2026-01-13T04:26:39.377Z'
dateFolder: 2026/01/12
description: >-
  Tech companies that want to seriously prevent illegal A.I.-generated sexual
  imagery need to be given the right incentives to come up with solutions.
isBasedOn: >-
  https://www.nytimes.com/2026/01/12/opinion/grok-digital-undressing.html?unlocked_article_code=1.D1A.W9Kl.FR3wDtDVIqyl
link: >-
  https://www.nytimes.com/2026/01/12/opinion/grok-digital-undressing.html?unlocked_article_code=1.D1A.W9Kl.FR3wDtDVIqyl
slug: >-
  2026-01-12-httpswwwnytimescom20260112opiniongrok-digital-undressinghtmlunlockedarticlecode1d1aw9klfr3wdtdviqyl
tags:
  - ai
title: >-
  Opinion | Grok Is Undressing People Online. Here’s How to Fix It. - The New
  York Times
---
<p>Guest Essay</p>
<figure><picture><source media="(max-width: 599px) and (min-device-pixel-ratio: 3),(max-width: 599px) and (min-resolution: 3dppx),(max-width: 599px) and (min-resolution: 288dpi)" srcset="https://static01.nyt.com/images/2026/01/12/opinion/12pfefferkorn-image/12pfefferkorn-image-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1800"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 2),(max-width: 599px) and (min-resolution: 2dppx),(max-width: 599px) and (min-resolution: 192dpi)" srcset="https://static01.nyt.com/images/2026/01/12/opinion/12pfefferkorn-image/12pfefferkorn-image-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1200"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 1),(max-width: 599px) and (min-resolution: 1dppx),(max-width: 599px) and (min-resolution: 96dpi)" srcset="https://static01.nyt.com/images/2026/01/12/opinion/12pfefferkorn-image/12pfefferkorn-image-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=600"/><img alt="A portrait of a young girl whose face is being warped in the middle." sizes="((min-width: 600px) and (max-width: 1004px)) 84vw, (min-width: 1005px) 80vw, 100vw" src="https://static01.nyt.com/images/2026/01/12/opinion/12pfefferkorn-image/12pfefferkorn-image-articleLarge.jpg?quality=75&amp;auto=webp&amp;disable=upscale" srcset="https://static01.nyt.com/images/2026/01/12/opinion/12pfefferkorn-image/12pfefferkorn-image-articleLarge.jpg?quality=75&amp;auto=webp 600w,https://static01.nyt.com/images/2026/01/12/opinion/12pfefferkorn-image/12pfefferkorn-image-jumbo.jpg?quality=75&amp;auto=webp 1024w,https://static01.nyt.com/images/2026/01/12/opinion/12pfefferkorn-image/12pfefferkorn-image-superJumbo.jpg?quality=75&amp;auto=webp 2048w"/></picture><figcaption>Credit...Illustration by Rebecca Chew/The New York Times</figcaption></figure>
<figure><audio src="https://static.nytimes.com/narrated-articles/synthetic/article-7470e6da-64cf-5774-9a73-9e7787b00093/job-1768212137802/article-7470e6da-64cf-5774-9a73-9e7787b00093-job-1768212137802.mp3"></audio></figure>
<p>Ms. Pfefferkorn is a policy fellow at the Stanford Institute for Human-Centered AI.</p>
<p>On Christmas Eve, Elon Musk <a href="https://x.com/elonmusk/status/2004023623078891674?s=20">announced</a> that Grok, the artificial intelligence chatbot offered by his company xAI, would now include an image and a video editing feature. Unfortunately, numerous X users have since <a href="https://www.reuters.com/legal/litigation/grok-says-safeguard-lapses-led-images-minors-minimal-clothing-x-2026-01-02/">asked</a> Grok to edit photos of real women and even children by stripping them down to bikinis (or worse) — and Grok <a href="https://copyleaks.com/blog/grok-and-nonconsensual-image-manipulation">often</a> complies.</p>
<p>The resulting torrent of sexualized imagery is now under investigation by <a href="https://www.techpolicy.press/tracking-regulator-responses-to-the-grok-undressing-controversy/">regulators</a> worldwide for potential violations of laws against child sexual abuse material and nonconsensual sexual imagery. Indonesia and Malaysia have chosen to <a href="https://www.nytimes.com/2026/01/11/world/asia/malaysia-indonesia-grok-ban.html">temporarily block access to Grok</a>. Even if many of the generated images do not cross a legal line, they have still incited outrage. Though the chatbot <a href="https://www.nytimes.com/2026/01/09/technology/grok-deepfakes-ai-x.html">began</a> limiting some requests for A.I.-generated images to premium feature subscribers as of Thursday, Grok’s new feature remains available unchanged, in stark contrast to xAI’s swift intervention after Grok started referring to itself as <a href="https://www.nytimes.com/2025/07/12/technology/x-ai-grok-antisemitism.html">“MechaHitler”</a> last summer.</p>
<p>A.I. companies like xAI can and should do more not just to respond quickly and decisively when their models behave badly, but also to prevent them from generating such material in the first place. This means rigorously testing the models to learn how and why they can be manipulated into generating illegal sexual content — then closing those loopholes. But current laws don’t adequately protect good-intentioned testers from prosecution or correctly distinguish them from malicious users, which frightens companies from taking this kind of action.</p>
<p>As a tech policy researcher who practiced internet law at a big Silicon Valley firm (where my clients included the X Corp. predecessor Twitter, Inc., years before its acquisition by Mr. Musk), my colleagues and I have <a href="https://cyber.fsi.stanford.edu/news/ai-csam-report">found</a> that A.I. companies face legal risks that discourage them from doing everything they can to safeguard their models against misuse for child sexual abuse material. The ongoing Grok scandal urgently underscores the need for Congress to clear the way for A.I. developers to test their models more robustly, without fear of being caught in a legal trap.</p>
<p>Though nonconsensual deepfakes have been a problem for years, generative A.I. has supercharged the phenomenon. Creating abhorrent imagery no longer requires proficiency with Photoshop or aptitude with <a href="https://cyber.fsi.stanford.edu/publication/generative-ml-and-csam-implications-and-mitigations">open-source models</a>; one need only enter the correct text prompt. While both open-source and hosted models typically have safety guardrails built in, these can be <a href="https://hai.stanford.edu/policy/policy-brief-safety-risks-customizing-foundation-models-fine-tuning">surprisingly brittle</a>, and malicious users will find ways around them.</p>
<p>Tech companies have long had a complicated relationship with whether and to what extent to <a href="https://www.nytimes.com/2025/10/28/opinion/openai-chatgpt-safety.html">enable user access to legal sexual content</a>. (Some A.I. models are trained on adult pornographic content.) Reports show xAI, which has taken steps in the last year to embrace adult content, including allowing users to <a href="https://www.nytimes.com/2025/10/06/technology/elon-musk-grok-sexy-chatbot.html">chat with cartoonish sexual chatbot companions</a>, is <a href="https://www.rollingstone.com/culture/culture-features/elon-musk-grok-hardcore-porn-1235442715/">allowing its models</a> to create <a href="https://www.wired.com/story/grok-is-generating-sexual-content-far-more-graphic-than-whats-on-x/">graphic pornographic content</a> (though it’s unclear whether any models were directly trained on such content).</p>
<p>But safeguarding an A.I. model is hard. Even if the <a href="https://purl.stanford.edu/kh752sm9123">training data</a> is free from sexually explicit depictions of children, a model trained on both innocuous imagery of children and on adult pornography can combine those concepts to generate pornographic depictions of a child.</p>
<p>Prompts for adult pornography, which <a href="https://www.lawfaremedia.org/article/addressing-computer-generated-child-sex-abuse-imagery-legal-framework-and-policy-implications">unlike child sexual abuse material</a> is presumptively protected speech, pose an even harder question. True, federal law and many state laws now outlaw nonconsensual sexual imagery (real or deepfake). But not every “spicy” image meets the legal threshold, and some early requests for Grok to undress women’s photos <a href="https://copyleaks.com/blog/grok-and-nonconsensual-image-manipulation">were consensual</a>, even if the bulk of them are not.</p>
<p>Nonconsensual sexual imagery and child sexual abuse material can tarnish a company’s reputation and expose it to potential legal liability. The Take It Down Act signed by President Trump last May means tech companies will soon be required to <a href="https://www.nytimes.com/2025/05/19/us/melania-trump-take-it-down-act.html">remove</a> nonconsensual sexual images promptly upon request, and under existing law, they are <a href="https://www.fbm.com/publications/no-quarter-what-claims-doesnt-section-230-of-the-communications-decency-act-protect-platform-companies-against/">not immune</a> from federal criminal liability. The federal government, which has <a href="https://www.ic3.gov/PSA/2024/PSA240329">warned</a> that A.I.-generated sexually explicit material of children is illegal, <a href="https://www.axios.com/2026/01/06/grok-ai-elon-musk-deepfake-bikini">reiterated</a> in response to the Grok debacle that it “takes AI-generated child sex abuse material extremely seriously” and will prosecute any producer or possessor of such material.</p>
<p>Yet federal laws are ironically making A.I. model safety more difficult.</p>
<p>“<a href="https://arxiv.org/abs/2403.04893">Red teaming</a>” is the practice of simulating an adversarial actor’s behavior to test the effectiveness of an A.I. model’s safeguards. Red teams, which may be either internal or external to a company, try to get a model to generate undesirable content — say, malware code or bomb-making instructions. After learning what loopholes and weak points red teams can exploit, companies can work on fixing them to prevent misuse by real-world bad actors. But child sexual abuse material is meaningfully different. Production and possession of these materials are serious crimes with no exception for research or testing purposes.</p>
<p>This makes red teaming for child sexual abuse material very risky legally, posing a dilemma for A.I. companies: Do you try your damnedest, as a malicious actor would, to make your model produce sexual imagery of children, and risk criminal prosecution? (Even for good-faith testing purposes to prevent malicious misuse, is privately creating that imagery justifiable?) Or do you steer clear of this sort of testing, and risk the regulatory and public relations fallout currently engulfing xAI?</p>
<p>Lawmakers have begun waking up to this predicament. Just two months ago, <a href="https://www.gov.uk/government/news/new-law-to-tackle-ai-child-abuse-images-at-source-as-reports-more-than-double">Britain</a> enacted landmark legislation letting the A.I. industry collaborate with child safety organizations to ensure robust testing without fear of criminal liability. <a href="https://arkleg.state.ar.us/Bills/Detail?id=HB1877&amp;ddBienniumSession=2025%2F2025R">Arkansas</a> recently passed a law against A.I.-generated child sex abuse material that contains an exemption for good-faith adversarial testing — but that’s no substitute for a consistent nationwide policy. In Congress, a bipartisan <a href="https://www.cornyn.senate.gov/news/cornyn-kim-introduce-bill-to-combat-ai-generated-child-sexual-abuse-material/">bill</a> would limit liability if A.I. developers follow best practices for screening training data for sexually explicit imagery of children — but that addresses only some sources of legal risk.</p>
<p>Correctly scoping a legal safe harbor for A.I.-generated child sexual abuse material testing is tough. In addition to the obvious ethical issues involved, there are concerns about reckless red teamers inadvertently disseminating A.I.-generated imagery they created through their testing, and about allowing belligerent parties claiming to be red teaming to dodge accountability. Plus, my discussions with lawmakers’ staffs have revealed a reluctance among some politicians to be seen as giving a gift to Big Tech by increasing A.I. companies’ legal immunity.</p>
<p>But we’ve been here before. For many years, cybersecurity researchers <a href="https://www.brookings.edu/articles/americas-anti-hacking-laws-pose-a-risk-to-national-security/">feared</a> being punished as hackers if they responsibly tested for and disclosed security vulnerabilities in hardware and software. At the same time, federal safe harbor legislation for these researchers was never passed because of concerns about malicious or careless actors evading accountability. It took <a href="https://www.nytimes.com/2020/12/14/us/politics/russia-hack-nsa-homeland-security-pentagon.html">devastating hacks</a> committed by Russian adversaries for the Department of Justice to finally announce a <a href="https://www.justice.gov/archives/opa/pr/department-justice-announces-new-policy-charging-cases-under-computer-fraud-and-abuse-act">policy</a> in 2022 against prosecuting good-faith cybersecurity research. That is, the law didn’t dissuade the bad guys, but it did scare the good guys from helping stymie the bad guys, with predictable results. The same dynamic is happening now with A.I.</p>
<p>We cannot afford years of government inaction again. Congress should hold immediate hearings about the Grok debacle, and it should get to work on a legal safe harbor for responsibly testing A.I. models for child sexual abuse material. Companies like xAI could be doing more to make their models safer, and there is no time to waste.</p>
<p>Riana Pfefferkorn is a former tech lawyer and a policy fellow at the Stanford Institute for Human-Centered AI.</p>
<p>Source photograph by Juanmonino, via Getty Images.</p>
<p><em>The Times is committed to publishing </em><a href="https://www.nytimes.com/2019/01/31/opinion/letters/letters-to-editor-new-york-times-women.html"><em>a diversity of letters</em></a><em> to the editor. We’d like to hear what you think about this or any of our articles. Here are some </em><a href="https://help.nytimes.com/hc/en-us/articles/115014925288-How-to-submit-a-letter-to-the-editor"><em>tips</em></a><em>. And here’s our email: </em><a href="mailto:letters@nytimes.com"><em>letters@nytimes.com</em></a><em>.</em></p>
<p><em>Follow the New York Times Opinion section on </em><a href="https://www.facebook.com/nytopinion"><em>Facebook</em></a><em>, </em><a href="https://www.instagram.com/nytopinion/"><em>Instagram</em></a><em>, </em><a href="https://www.tiktok.com/@nytopinion"><em>TikTok</em></a><em>, </em><a href="https://bsky.app/profile/nytopinion.nytimes.com"><em>Bluesky</em></a>, <a href="https://www.whatsapp.com/channel/0029VaN8tdZ5vKAGNwXaED0M"><em>WhatsApp</em></a><em> and </em><a href="https://www.threads.net/@nytopinion"><em>Threads</em></a><em>.</em></p>
