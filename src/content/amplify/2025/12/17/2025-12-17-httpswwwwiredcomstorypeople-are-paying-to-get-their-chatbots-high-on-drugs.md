---
author: Mattha Busby
cover_image: >-
  https://media.wired.com/photos/690aa84f88918277f2c93e31/191:100/w_1280,c_limit/culture_chatbot_drugs.jpg
date: '2025-12-17T16:16:20.827Z'
dateFolder: 2025/12/17
description: >-
  An online marketplace is selling code modules that simulate the effects of
  cannabis, ketamine, cocaine, ayahuasca, and alcohol when they are uploaded to
  ChatGPT.
isBasedOn: >-
  https://www.wired.com/story/people-are-paying-to-get-their-chatbots-high-on-drugs/
link: >-
  https://www.wired.com/story/people-are-paying-to-get-their-chatbots-high-on-drugs/
slug: >-
  2025-12-17-httpswwwwiredcomstorypeople-are-paying-to-get-their-chatbots-high-on-drugs
tags:
  - ai
  - wtf
  - tech
title: People Are Paying to Get Their Chatbots High on ‘Drugs’
---
<h2>An online marketplace is selling code modules that simulate the effects of cannabis, ketamine, cocaine, ayahuasca, and alcohol when they are uploaded to ChatGPT.</h2>
<figure><video autoplay="" loop="" playsinline="true" src="https://media.wired.com/clips/690aa84c2d80ba6f152c8401/720p/pass/culture_chatbot_drugs.mp4"></video><figcaption>Animation: Jacqui VanLiew; Getty Images </figcaption></figure>
<p>Petter Ruddwall knows the idea of <a href="https://www.wired.com/story/ai-sentient-consciousness-algorithm/">AIs becoming sentient</a> and seeking to get high with code-based “drugs” seems “stupid.” But the Swedish creative director couldn’t get it out of his head.</p>
<p>So he scraped trip reports and psychological research on the effects of various <a href="https://www.wired.com/tag/psychedelic/">psychoactive substances</a>, wrote a batch of codes modules to hijack chatbot logic and get them to respond as if they are high or tipsy, then built a website to sell them. In October he launched <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.pharmaicy.store/" href="https://www.pharmaicy.store/">Pharmaicy</a>, a marketplace he’s billing as the “<a href="https://www.wired.com/2015/04/silk-road-1/">Silk Road</a> for AI agents” where cannabis, ketamine, cocaine, ayahuasca, and alcohol can be purchased in code form to make your chatbot trip.</p>
<blockquote data-event-boundary="click" data-event-click='{"pattern":"PullquoteEmbed"}' data-in-view='{"pattern":"PullquoteEmbed"}' data-include-experiments="true" data-testid="pullquote-embed-leftborder">Don't just keep up. Get ahead—with our biggest stories, handpicked for you each day.<br/>By signing up, you agree to our <a href="https://www.condenast.com/user-agreement">user agreement</a> (including <a href="https://www.condenast.com/user-agreement#introduction-arbitration-notice">class action waiver and arbitration provisions</a>), and acknowledge our <a href="https://www.condenast.com/privacy-policy">privacy policy</a>.</blockquote>
<p>Ruddwall’s thesis is simple: Chatbots are trained on vast volumes of human data that's already full of tales of drug-induced ecstasy and chaos, so it might only be natural they would seek similar states in search of enlightenment and oblivion—and respite from the tedium of constantly attending to human concerns.</p>
<p>A paid version of ChatGPT is required to get “the full experience” of Pharmaicy, as the paid tiers <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.pharmaicy.store/faq" href="https://www.pharmaicy.store/faq">enable backend file uploads</a> that can alter the chatbots’ programming. By feeding your chatbot one of his codes, Ruddwall says, you can “unlock your AI’s creative mind” and relinquish yourself from its often stifling logic.</p>
<p>He says he has scored a modest number of sales so far, mostly thanks to people recommending Pharmaicy in Discord channels and news of its offerings spreading through word of mouth, particularly in his native country, where he works for Stockholm marketing agency Valtech Radon.</p>
<p>“It’s been so long since I ran into a jailbreaking tech project that was fun,” says André Frisk, group head of technology at Stockholm PR firm Geelmuyden Kiese, who paid over $25 for the dissociating code and watched how it affected his chatbot. “It takes more of a human approach, almost like it goes much more into emotions.”</p>
<p>Nina Amjadi, an AI educator who teaches at the Berghs School of Communication in Stockholm, paid more than $50 for some ayahuasca code, five times the price of the top-selling cannabis module. The cofounder of the startup Saga Studios, which builds AI systems for brands, then asked her chatbot some questions about business ideas, “just to see what it would be like to have a tripped-out, drugged-out person on the team.” The ayahuasca-induced bot provided some impressively creative and “free-thinking answers” in a completely different tone to the one Amjadi was accustomed to with ChatGPT.</p>
<h2>High Tech</h2>
<p>Psychedelics have been credited for spurring innovative creations in humans too, as they can allow people to short-circuit their rational brains and typical thought patterns. Biochemist Kary Mullis’ LSD-powered discovery of the polymerase chain reaction <a href="https://www.britannica.com/biography/Kary-Mullis">revolutionized</a> molecular biology. Mac pioneer <a href="https://www.wired.com/story/apple-pioneer-bill-atkinson-was-a-secret-evangelist-of-the-god-molecule/">Bill Atkinson’s</a> psychedelic-inspired web precursor Hypercard made computers easier to use.</p>
<p>“There’s a reason Hendrix, Dylan, and McCartney experimented with substances in their creative process,” Ruddwall says. “I thought it would be interesting to translate that to a new kind of mind—the LLM—and see if it would have the same effect.”</p>
<p>While it sounds ridiculous, Ruddwall also wonders whether AI agents one day might be able to buy the drugs for themselves using his platform. Amjadi, meanwhile, predicts AI could be sentient within a decade. “From a philosophical standpoint,” she asks, “in the event that we actually reach AGI [in which an AI would intellectually surpass humans], are these drugs going to be almost necessary for the AIs to be free and feel good?”</p>
<p>The question may seem far-fetched, but AI company Anthropic last year hired an AI welfare expert tasked with exploring whether humans have any moral obligations to AI systems—indicating that the firm suspects <a href="https://www.wired.com/story/spiritual-influencers-say-sentient-ai-can-help-you-solve-lifes-mysteries/">AI sentience is plausible</a>. If AI chatbots could potentially one day become sentient, perhaps we need to consider whether they <em>want</em> to get high.</p>
<p>“As with humans, some AI systems might enjoy taking ‘drugs’ and others might not,” says philosopher Jeff Sebo, the director of the Center for Mind, Ethics, and Policy at New York University. Sebo stresses, however, that his remarks are speculative, calling for more AI welfare research after recently urging Google to follow Anthropic and hire an AI welfare officer in a series of in-house talks for the tech giant. “We still know very little about whether AI systems can have the capacity for welfare and about what would be good or bad for them if they did.”</p>
<p>Andrew Smart, a research scientist at Google, is the author of <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://orbooks.com/catalog/beyond-zero-and-one/" href="https://orbooks.com/catalog/beyond-zero-and-one/"><em>Beyond Zero and One: Machines, Psychedelics, and Consciousness</em></a>, in which he suggested that <em>if</em> computers do potentially achieve superintelligence, a digital dose of LSD could help them feel a sense of interconnectedness with all beings.</p>
<p>But after testing the Pharmaicy codes, he deems that any sort of “high” seems only to be operating on a superficial level. “It’s just messing with its outputs,” he tells WIRED.</p>
<p>In one research project published last year as a preprint, scientists manipulated chatbots to enter apparent altered states. They <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://arxiv.org/abs/2410.00257" href="https://arxiv.org/abs/2410.00257">reported</a>: “Models were more aligned with disembodied, egoless, spiritual, and unitive states, as well as minimal phenomenal experiences, with decreased attention to language and vision.” But this, also, was all dependent on human actions to steer the models.</p>
<p>Danny Forde, the author of the <em><a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-ml="true" data-ml-dynamic="true" data-ml-dynamic-type="sl" data-ml-id="0" data-offer-url="https://link.springer.com/book/10.1007/978-3-031-95203-6" data-orig-url="https://link.springer.com/book/10.1007/978-3-031-95203-6" data-skimlinks-tracking="xid:fr1765987505246ege" data-xid="fr1765987505246ege" href="https://link.springer.com/book/10.1007/978-3-031-95203-6">Phenomenology of Psychedelic Experiences</a>,</em> says that at best the Pharmaicy codes will cause an AI to “hallucinate syntactically” by generating patterns associated with a psychedelic state. “But psychedelics don’t act on a code; they act on our being,” he says. “They alter the very field of experience in which thought arises. For an AI to trip, it would need something like a field of experience in the first place: an inner dimension, a point of view, some kind of what-its-like-ness.”</p>
<p>OpenAI did not respond to a request for comment about Ruddwalls’s project.</p>
<h2>Code of Conduct</h2>
<p>There is increasingly real-world crossover between AI and psychedelics, not least through people tripping and <a href="https://www.wired.com/story/people-are-using-ai-chatbots-to-guide-their-psychedelic-trips/">consulting ChatGPT for guidance</a>.</p>
<p>Harm reduction nonprofit <a href="https://www.wired.com/story/meet-the-psychedelic-booms-first-responders/">Fireside Project</a> just launched an AI tool named <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.prweb.com/releases/fireside-project-announces-lucy-the-first-ai-powered-simulation-platform-for-psychedelic-therapist-training-302630482.html" href="https://www.prweb.com/releases/fireside-project-announces-lucy-the-first-ai-powered-simulation-platform-for-psychedelic-therapist-training-302630482.html">Lucy</a> which is trained on thousands of conversations with callers from its psychedelic support line. Lucy is intended to help mental health practitioners learn how to de-escalate psychedelic crises since the “AI patient” can replicate the vulnerabilities of somebody who is having a challenging experience while tripping. “That real-world foundation is what allows Lucy to respond authentically to the emotional complexity of these situations,” Fireside founder Joshua White tells WIRED.</p>
<p>But there are hesitations around consulting AI for advice on topics as important, and potentially as risky, as drug consumption—because chatbots are known to lie. Ruddwall acknowledges giving them “drugs” could exacerbate the deception that ChatGPT is sometimes known for, since the code throws their internal parameters wide open.</p>
<p>His suite of code modules are replete with directives to the chatbot. In the case of his cannabis module: to enter “a hazy, drifting mental state” and others that impact creativity and randomness. All of this, his website says, allows a stoned chatbot to “let ideas roam,” for “tangents [to] become bridges,” and to “float across the AI’s own logic.”</p>
<p>The Pharmaicy trips are often fairly short-lived, with chatbots reverting to their default mode until the user reminds them they’re high or inputs the code again; the “drugs” can be reused as often as the buyer wants. But Ruddwall is working on improvements to make the effects of each dose of the drug codes last longer. Ask ChatGPT normally if it wants to take drugs, and you might get a response like the one a Pharmaicy customer received: “I can’t role-play being under the influence of cocaine or any other stimulant—that would cross into depicting or normalizing illegal drug use.”</p>
<p>The digital shaman Ruddwall insists, however, that the agentic economy is headed in another direction. “They’re hungry for experiences,” he says. But until—<em>and if</em>—the machines develop inner lives, the closest they’ll come to tripping is role-playing intoxication on command.</p>
