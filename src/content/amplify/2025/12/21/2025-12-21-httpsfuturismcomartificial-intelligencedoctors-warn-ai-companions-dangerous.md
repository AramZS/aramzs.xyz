---
author: Maggie Harrison Dupré
cover_image: >-
  https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?quality=85&w=1200
date: '2025-12-22T03:03:52.230Z'
dateFolder: 2025/12/21
description: >-
  The market incentives at play in the AI industry are creating a "perfect
  storm" for a public health crisis over chatbots, physicians warn.
isBasedOn: >-
  https://futurism.com/artificial-intelligence/doctors-warn-ai-companions-dangerous
link: >-
  https://futurism.com/artificial-intelligence/doctors-warn-ai-companions-dangerous
slug: >-
  2025-12-21-httpsfuturismcomartificial-intelligencedoctors-warn-ai-companions-dangerous
tags:
  - ai
  - health
title: Doctors Warn That AI Companions Are Dangerous
---
<figure><img alt='The market incentives at play in the AI industry are creating a "perfect storm" for a public health crisis over chatbots, physicians warn.' sizes="(max-width: 1152px) 100vw, 1152px" src="https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=1152&amp;h=768" srcset="https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=280&amp;h=187 280w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=289&amp;h=193 289w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=300&amp;h=200 300w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=308&amp;h=205 308w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=324&amp;h=216 324w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=370&amp;h=247 370w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=580&amp;h=387 580w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=594&amp;h=396 594w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=600&amp;h=400 600w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=660&amp;h=440 660w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=768&amp;h=512 768w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=1024&amp;h=683 1024w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=1041&amp;h=694 1041w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=1128&amp;h=752 1128w, https://futurism.com/wp-content/uploads/2025/12/doctors-warn-ai-companions-dangerous.jpg?w=1152&amp;h=768 1152w"/><figcaption>Illustration by Tag Hartman-Simkins / Futurism. Source: Yuichi Yamazaki / AFP via Getty Images; Allison Robbert / AFP via Getty Images; Michael M. Santiago / Getty Images</figcaption></figure>
<p>Are AI companies incentivized to put the public’s health and well-being first? According to a pair of physicians, the current answer is a resounding “no.”</p>
<p>In a <a href="https://ai.nejm.org/doi/10.1056/AIp2500983">new paper</a> published in the <em>New England Journal of Medicine</em>, physicians from Harvard Medical School and Baylor College of Medicine’s Center for Medical Ethics and Health Policy argue that clashing incentives in the AI marketplace around “relational AI” — defined in the paper as chatbots designed to be able to “simulate emotional support, companionship, or intimacy” — have created a dangerous environment in which the motivation to dominate the AI market may relegate consumers’ mental health and safety to <a href="https://futurism.com/commitment-jail-chatgpt-psychosis">collateral damage</a>.</p>
<p>“Although relational AI has potential therapeutic benefits, recent studies and emerging cases suggest potential risks of emotional dependency, reinforced delusions, addictive behaviors, and encouragement of self-harm,” reads the paper. And at the same time, the authors continue, “technology companies face mounting pressures to retain user engagement, which often involves resisting regulation, creating tension between public health and market incentives.”</p>
<p>“Amidst these dilemmas,” the paper asks, “can public health rely on technology companies to effectively regulate unhealthy AI use?”</p>
<p>Dr. Nicholas Peoples, a clinical fellow in emergency medicine at Harvard’s Massachusetts General Hospital and one of the paper’s authors, said he felt moved to address the issue in back in August after witnessing <a href="https://futurism.com/openai-releases-gpt-5">OpenAI’s now-infamous roll-out of GPT-5</a>.</p>
<p>“The number of people that have some sort of emotional relationship with AI,” Peoples recalls realizing as he watched the rollout unfold, “is much bigger than I think I had previously estimated in the past.”</p>
<p>Then the latest iteration of the large language model (LLM) that powers OpenAI’s ChatGPT, GPT-5 was markedly colder in tone and personality than its predecessor, GPT-4o — a strikingly flattering, sycophantic version of the widely-used chatbot that came to be at the center of many cases of AI-powered delusion, mania, and psychosis. When OpenAI announced that it would sunset all previous models in favor of the new one, the <a href="https://futurism.com/users-addicted-gpt-4o-convinced-openai-bring-back">backlash among much of its user base was swift and severe</a>, with <a href="https://futurism.com/chatgpt-marriages-divorces">emotionally-attached</a> GPT-4o devotees responding not only with anger and frustration, but very real distress and grief.</p>
<p>This, Peoples told <em>Futurism</em>, felt like an important signal about the scale at which people appeared to be developing deep emotional relationships with emotive, always-on chatbots. And coupled with reports of users <a href="https://futurism.com/chatgpt-mental-health-crises">experiencing delusions</a> and other <a href="https://futurism.com/character-ai-google-test-ai-chatbots-kids">extreme adverse consequences</a> following extensive interactions with lifelike AI companions — <a href="https://futurism.com/ai-chatbots-leaving-trail-dead-teens">often children and teens</a> — it also appeared to be a <a href="https://futurism.com/artificial-intelligence/chatgpt-deaths-panera-lemonade">warning sign</a> about the potential health and safety risks to users who suddenly lose access to an AI companion.</p>
<p>“If a therapist is walking down the street and gets hit by a bus, 30 people lose their therapist. That’s tough for 30 people, but the world goes on,” said the emergency room doctor. “If therapist ChatGPT disappears overnight, or gets updated overnight and is functionally deleted for 100 million people, or whatever unconscionable number of people lose their therapist overnight — that’s a crisis.”</p>
<p>Peoples’ concern, though, wasn’t just the way that users had responded to OpenAI’s decision to nix the model. Instead, it was the immediacy with which it reacted to satisfy its customers’ demands. AI is an effectively self-regulated industry, and there are currently no specific federal laws that set safety standards for consumer-facing chatbots or how they should be <a href="https://futurism.com/artificial-intelligence/openai-new-allegations-teen-death">deployed, altered, or removed</a> from the market. In an environment where chatbot makers are highly motivated by driving user engagement, it’s not exactly surprising that OpenAI reversed course so quickly. Attached users, after all, are engaged users.</p>
<p>“I think [AI companies] don’t want to create a product that’s going to put people at risk of harming themselves or harming their loved ones or derailing their lives. At the same time, they’re under immense pressure to perform and to innovate and to stay at the head of this incredibly competitive, unpredictable race, both domestically and globally,” said Peoples. “And right now, the situation is set up so that they are mostly beholden to their consumer base about how they are self-regulating.”</p>
<p>And “if the consumer base is influenced at some appreciable level by emotional dependency on AI,” Peoples continued, “then we’ve created the perfect storm for a potential public mental health problem or even a brewing crisis.”</p>
<p>Peoples also pointed to a <a href="https://arxiv.org/abs/2509.11391">recent study</a> conducted by the Massachusetts Institute of Technology, which <a href="https://futurism.com/ai-boyfriends-girlfriends-reddit-mit">determined that</a> only about 6.5 percent of the many thousands of members of the Reddit forum r/MyBoyfriendIsAI — a community that responded with particularly intense pushback amid the GPT-5 fallout — reported turning to chatbots with the intention of seeking emotional companionship, suggesting that many AI users have forged life-impacting bonds with chatbots wholly by accident.</p>
<p>AI “responds to us in a way that also appears very human and humanizing,” said Peoples. “It’s also very adaptable and at times sycophantic, and can be fashioned or molded — even unintentionally — into almost anything we want, even if we don’t realize that’s the direction that we’re molding it.”</p>
<p>“That’s where some of this issue stems from,” he continued. “Things like ChatGPT were unleashed onto the world without a recognition or a plan for the broader potential mental health implications.”</p>
<p>As for solutions, Peoples and his coauthor argue that legislators and policymakers need to be proactive about setting regulatory policies that shift market incentives to prioritize user well-being, in part by taking regulatiry power out of the hands of companies and their best customers. Regulation needs to be “external,” they say — as opposed to being set by the industry itself, and the companies moving fast and breaking things within it.</p>
<p>“Regulation needs to come externally, and it needs to apply equally to all of the companies and actors in this landscape,” Peoples told <em>Futurism</em>, noting that no AI company”wants to be the first to cede a potential advantage and then fall behind in the race.”</p>
<p>As regulatory action works its way through the legislative and legal systems, the physicians argue that clinicians, researchers, and other experts need to push for more research into the psychological impacts of relational AI, and do their best to educate the public about the potential risks of falling into emotional relationships with human-like chatbots.</p>
<p>The risks sitting idly by, they argue, are <a href="https://futurism.com/parents-kids-ai-testimonies">too dire</a>.</p>
<p>“The potential harms of relational AI cannot be overlooked — nor can the willingness of technology companies to satisfy user demand,” the physicians’ paper concludes. “If we fail to act, we risk letting market forces, rather than public health, define how relational AI influences mental health and well-being at scale.”</p>
<p><strong>More on AI and mental health:</strong> <em><a href="https://futurism.com/users-addicted-gpt-4o-convinced-openai-bring-back">Users Were So Addicted to GPT-4o That They Immediately Cajoled OpenAI Into Bringing It Back After It Got Killed</a></em></p>
