---
author: Sally Weale
cover_image: >-
  https://i.guim.co.uk/img/media/aecfd04253be0eb423a32054044321ad35fd1153/0_0_2500_2000/master/2500.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&precrop=40:21,offset-x50,offset-y0&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=8af2b4dff35405d0075daf0da5221463
date: '2025-12-06T05:36:40.221Z'
dateFolder: 2025/12/06
description: >-
  The use of ‘nudify’ apps is becoming more and more prevalent, with hundreds of
  teachers having seen images created by pupils, often of their peers. The
  fallout is huge – and growing fast
isBasedOn: >-
  https://www.theguardian.com/society/ng-interactive/2025/dec/02/the-rise-of-deepfake-pornography-in-schools?CMP=share_btn_url
link: >-
  https://www.theguardian.com/society/ng-interactive/2025/dec/02/the-rise-of-deepfake-pornography-in-schools?CMP=share_btn_url
slug: >-
  2025-12-06-httpswwwtheguardiancomsocietyng-interactive2025dec02the-rise-of-deepfake-pornography-in-schoolscmpsharebtnurl
tags:
  - ai
  - youth
title: >-
  The rise of deepfake pornography in schools: ‘One girl was so horrified she
  vomited’
---
<figure><picture><source media="(min-width: 980px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 980px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 980px)" srcset="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 740px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 740px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=700&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 740px)" srcset="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=700&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 660px)" srcset="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=645&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 480px)" srcset="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=645&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=465&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 320px)" srcset="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=465&amp;dpr=1&amp;s=none&amp;crop=none"/><img alt="schoolchildren (faces not seen) sit on a staircase, with tech-y background" src="https://i.guim.co.uk/img/media/3cac73afc88c5980b02fb9bacf70da7a027233dc/0_0_4000_5000/master/4000.jpg?width=465&amp;dpr=1&amp;s=none&amp;crop=none"/></picture><figcaption>‘It reflects and reinforces a culture where consent and respect for personal boundaries are undermined.’ Composite: Guardian Design; Abstract Aerial Art/Getty Images</figcaption></figure>
<h2 data-gu-name="standfirst">The use of ‘nudify’ apps is becoming more and more prevalent, with hundreds of teachers having seen images created by pupils, often of their peers. The fallout is huge – and growing fast</h2>
<p>‘It worries me that it’s so normalised. He obviously wasn’t hiding it. He didn’t feel this was something he shouldn’t be doing. It was in the open and people saw it. That’s what was quite shocking.”</p>
<p>A headteacher is describing how a teenage boy, sitting on a bus on his way home from school, casually pulled out his phone, selected a picture from social media of a girl at a neighbouring school and used a “nudifying” app to doctor her image.</p>
<p>Ten years ago it was <a data-link-name="in body link" href="https://www.theguardian.com/society/2015/nov/10/sexting-becoming-the-norm-for-teens-warn-child-protection-experts">sexting and nudes</a> causing havoc in classrooms. Today, advances in artificial intelligence (AI) have made it child’s play to generate deepfake nude images or videos, featuring what appear to be your friends, your classmates, even your teachers. This may involve removing clothes, getting an image to move suggestively or pasting someone’s head on to a pornographic image.</p>
<p>The headteacher does not know why this particular girl – a student at her school – was selected, whether the boy knew her, or whether it was completely random. It only came to her attention because he was spotted by another of her pupils who realised what was happening and reported it to the school.</p>
<p>The parents were contacted, the boy was traced and the police were called in. But such is the stigma and shame associated with image-based sexual abuse and the sharing of deepfakes that a decision was made that the girl who was the target should not be told.</p>
<p>“The girl doesn’t actually even know,” the head said. “I talked to the parents and the parents didn’t want her to know.”</p>
<figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><picture><source media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/fa62178509cbcb8fb48e61370a8aa83040a3e550/0_0_2500_2000/master/2500.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 660px)" srcset="https://i.guim.co.uk/img/media/fa62178509cbcb8fb48e61370a8aa83040a3e550/0_0_2500_2000/master/2500.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/fa62178509cbcb8fb48e61370a8aa83040a3e550/0_0_2500_2000/master/2500.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 480px)" srcset="https://i.guim.co.uk/img/media/fa62178509cbcb8fb48e61370a8aa83040a3e550/0_0_2500_2000/master/2500.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/fa62178509cbcb8fb48e61370a8aa83040a3e550/0_0_2500_2000/master/2500.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 320px)" srcset="https://i.guim.co.uk/img/media/fa62178509cbcb8fb48e61370a8aa83040a3e550/0_0_2500_2000/master/2500.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none"/><img alt="Students (faces unseen) at laptops, with a tech-y outline" src="https://i.guim.co.uk/img/media/fa62178509cbcb8fb48e61370a8aa83040a3e550/0_0_2500_2000/master/2500.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none"/></picture><figcaption data-spacefinder-role="inline"> Composite: Guardian Design; Alistair Berg/Getty Images</figcaption></figure>
<p>The boy on the bus is just one example of how deepfakes and easily accessed nudifying technology are playing out among schoolchildren – often to devastating effect. In Spain last year, 15 boys in the south-western region of Extremadura were <a data-link-name="in body link" href="https://www.theguardian.com/world/article/2024/jul/09/spain-sentences-15-school-children-over-ai-generated-naked-images">sentenced to a year’s probation</a> after being convicted of using AI to produce fake naked images of their female schoolmates, which they shared on WhatsApp groups. About 20 girls were affected, most of them aged 14, while the youngest was 11.</p>
<p>In Australia, about 50 high school students at Bacchus Marsh grammar in Victoria reported that <a data-link-name="in body link" href="https://www.theguardian.com/australia-news/article/2024/jun/12/schoolboy-arrested-after-allegedly-posting-fake-explicit-images-of-female-students-ntwnfb">their images had been faked and distributed</a> – the mother of one student said her daughter was so horrified by the sexually explicit images that she vomited. In the US, more than 30 female students at Westfield high school in New Jersey discovered that deepfake pornographic images of them had been shared among their male classmates on Snapchat.</p>
<p>It’s happening in the UK, too. A new poll of 4,300 secondary school teachers in England, carried out by Teacher Tapp on behalf of the Guardian, found that about one in 10 were aware of students at their school creating “deepfake, sexually explicit videos” in the last academic year.</p>
<p>Three-quarters of these incidents involved children aged 14 or younger, while one in 10 incidents involved 11-year-olds, and 3% were younger still, illustrating just how easy the technology is to access and use. Among participating teachers, 7% said they were aware of a single incident, and 1% said it had happened twice, while a similar proportion said it had happened three times or more in the last academic year.</p>
<p>Earlier this year, a Girlguiding survey found that one in four respondents aged 13 to 18 had seen a sexually explicit deepfake image of a celebrity, a friend, a teacher or themselves.</p>
<p>“A year ago I was using examples from the US and Spain to talk about these issues,” says Margaret Mulholland, a special needs and inclusion specialist at the Association of School and College Leaders. “Now it’s happening on our doorstep and it’s really worrying.”</p>
<p>Last year <a data-link-name="in body link" href="https://www.thetimes.com/uk/education/article/private-schools-in-police-inquiry-over-deepfake-porn-images-of-girls-d9qc2wgvk?t=1758907799450">the Times reported</a> that two private schools in the UK were at the centre of a police investigation into the alleged making and sharing of deepfake pornographic images. The newspaper said police were investigating claims that the deepfakes were created at a boys’ school by someone manipulating images taken from the social media accounts of pupils at a girls’ school.</p>
<p>The children’s commissioner for England, Dame Rachel de Souza, has called for nudification apps such as ClothOff, which was investigated as part of the Guardian’s <a data-link-name="in body link" href="https://www.theguardian.com/news/audio/2024/mar/11/black-box-hunt-clothoff-deepfake-porn-app-podcast">Black Box podcast</a> series about AI, to be banned. “Children have told me they are frightened by the very idea of this technology even being available, let alone used,” she says.</p>
<p>It’s not easy to find teachers willing to speak about deepfake incidents. Those who agreed to be interviewed by the Guardian insisted on strict anonymity. Other accounts were provided by academics researching deepfakes in schools, and providers of sex education.</p>
<p>Tanya Horeck, a professor of film and feminist media studies at Anglia Ruskin University, has been talking to headteachers as part of a fact-finding mission to uncover the scale of the problem in schools. “All of them had incidents of deepfakes in their schools and they saw this as an emerging problem,” she says. In one case, a 15-year-old girl who was new to a school was targeted by male students who created a pornographic deepfake video of her. She was so distressed she initially refused to go to school. “Almost all the examples they told me about were boys making deepfakes of girls,” says Horeck.</p>
<p>“The other thing that I noticed is that there’s this real tension around how they should handle these issues. So some teachers were saying, ‘Yeah, we just get the police in right away and students are expelled’ – that kind of approach,” Horeck says. “Then other teachers were saying, ‘Well, that’s not the way to handle it. We’ve got to have more of a restorative justice approach, where we’re talking to these young people and finding out why they’re doing these things.’</p>
<p>“So there seems to be some kind of inconsistency and uncertainty on how to deal with these cases – but I think it’s really hard for teachers because they’re not getting clear guidance.”</p>
<p>Laura Bates, the founder of the Everyday Sexism Project, says there is something particularly shocking about deepfake images. In her book <a data-link-name="in body link" href="https://guardianbookshop.com/the-new-age-of-sexism-9781471190513/?utm_source=editoriallink&amp;utm_medium=merch&amp;utm_campaign=article">The New Age of Sexism: How the AI Revolution Is Reinventing Misogyny</a>, she writes: “Of all the forms of abuse I receive they are the ones that hurt most deeply – the ones that stay with me. It’s hard to describe why, except to say that it feels like <em>you. </em>It feels like someone has taken you and done something to you and there is nothing you can do about it. Watching a video of yourself being violated without your consent is an almost out-of-body experience.”</p>
<figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><picture><source media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/f919671e864334a3ea0034712b2a8249d2d51aa6/0_0_8192_5464/master/8192.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 660px)" srcset="https://i.guim.co.uk/img/media/f919671e864334a3ea0034712b2a8249d2d51aa6/0_0_8192_5464/master/8192.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/f919671e864334a3ea0034712b2a8249d2d51aa6/0_0_8192_5464/master/8192.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 480px)" srcset="https://i.guim.co.uk/img/media/f919671e864334a3ea0034712b2a8249d2d51aa6/0_0_8192_5464/master/8192.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none"/><source media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)" srcset="https://i.guim.co.uk/img/media/f919671e864334a3ea0034712b2a8249d2d51aa6/0_0_8192_5464/master/8192.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none"/><source media="(min-width: 320px)" srcset="https://i.guim.co.uk/img/media/f919671e864334a3ea0034712b2a8249d2d51aa6/0_0_8192_5464/master/8192.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none"/><img alt="An empty staircase in a school" src="https://i.guim.co.uk/img/media/f919671e864334a3ea0034712b2a8249d2d51aa6/0_0_8192_5464/master/8192.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none"/></picture><figcaption data-spacefinder-role="inline">‘The fallout was significant.’ Photograph: Nick David/Getty Images</figcaption></figure>
<p>Among school-age children, the impact can be huge. Girls and young women are left feeling violated and humiliated. School friendship groups are shattered and there can be a deep sense of betrayal when one student discovers another has created a deepfake sexualised image of them, and shared it around the school. Girls can’t face lessons, while teachers with little training do their best to support and educate. Meanwhile, boys and young men are being drawn into criminal behaviour, often because they don’t understand the consequences of their actions.</p>
<p>“We do see students who are very upset and feel betrayed and horrified by this kind of abuse,” says Dolly Padalia, the CEO of the School of Sexuality Education, a charity providing sex education in schools and universities. “One example is where a school got in touch with us. A student had been found to have taken images of lots of students within the year group and was making deepfakes.</p>
<p>“These had then been leaked, and the fallout was quite significant. Students were really upset. They felt very betrayed and violated. It’s a form of abuse. The police were involved. The student was removed from school and we were asked to come in and support. The school responded very, very quickly, but I would say that’s not enough. In order for us to really be preventing sexual violence, we need to be more proactive.”</p>
<p>It is estimated that 99% of sexually explicit deepfakes accessible online are of women and girls, but there are cases of boys being targeted. The charity Everyone’s Invited (EI), which collects testimonies from survivors of sexual abuse, has run into at least one such case: “One student shared with the EI education team that a boy in their year group, who was well liked and friends with many of the girls, was targeted when another boy created an AI-generated sexual image of him. That image was then circulated around the school, causing significant distress and trauma.”</p>
<p>EI also flags how these tools are being trivialised and used in disturbing ways, such as the “changing your friend into your boyfriend” filter. “On social media platforms like TikTok and Snapchat, they are increasingly accessible and normalised. While this may seem playful or harmless to some, it reflects and reinforces a culture where consent and respect for personal boundaries are undermined.”</p>
<p>Against a backdrop of widespread misogyny in schools, a growing number of teachers are also being targeted, EI and others report: “It is something that, as a society, we urgently need to confront. Education has to stay in front of technology, and adults must feel equipped to lead these conversations rather than shy away from them.”</p>
<p>Seth James is a designated safeguarding lead – a senior member of staff with overall responsibility for child protection and safeguarding within a school – and the author of <a data-link-name="in body link" href="https://thedslblog.substack.com/">the DSL Blog</a>. “For everyone working in schools, it feels like new sets of challenges and risks are constantly being thrown up by technological developments,” he says. “AI generally – and particularly deepfakes and nudify apps – feel like the next train coming down the track.</p>
<p>“‘More education’ is appealing as a solution to these sorts of challenges – because it’s intuitive and relatively easy – but on its own it’s like trying to hold back a forest fire with a water pistol. And likewise, the police seem completely overwhelmed by the scale of these issues. As a society we need broader solutions, and better strategy.”</p>
<p>He continues: “We should all try to imagine how we would have felt 20 years ago if someone had suggested inventing a handheld device which could be used to create realistic pornographic material that featured actual people that you know in real life. And then they’d suggested giving one of these devices to all of our children. Because that’s basically where we are now. We’re letting these things become ‘normal’ on our watch.”</p>
<p>Jessica Ringrose, a professor of sociology of gender and education at University College London’s Institute of Education, has worked in schools on issues including masculinity, gender inequality and sexual violence. She is also co-author of a book called Teens, Social Media, and Image Based Abuse, and is now researching tech-facilitated gender-based violence.</p>
<p>“The way that young people are using these technologies is not necessarily all bad,” she says, “but what they need is better media literacy.” She welcomes the government’s <a data-link-name="in body link" href="https://www.theguardian.com/education/2025/jul/15/secondary-schools-england-to-tackle-incel-culture-relationships-sex-education">updated relationships, sex and health education guidance</a>, which “recognised that misogyny is a problem that needs to be tackled in the school system”. However, she says: “They need to put the dots together. They need to join up a concern with gender and sexual-based violence with technology. You can’t rely on Ofcom or the regulators to protect young people. We need proactive, preventive education.”</p>
<p>Where is the government in all this? “Our new relationships, sex and health education guidance will make sure that all young people understand healthy relationships, sexual ethics and the dangers of online content such as pornography and deepfake,” a Department for Education spokesperson said. “As part of our Plan for Change mission to halve violence against women and girls, we are also providing schools with new funded resources to help teachers explain the law and harms relating to online content as part of their age-appropriate lessons.”</p>
<p>Ringrose stresses the urgency. “These issues are happening – non-consensual creation and distribution of images is happening. These technologies are at people’s fingertips. I mean, it’s super-easy for any kid to access these things.”</p>
<p>She is sceptical about efforts to ban smartphones in schools and worries they will make it harder for young people who may be targeted with abusive imagery to seek help. “Abstinence around things like technology doesn’t work,” she says. “You actually have to teach people to use it properly. We need to engage with this as a really important element of the curriculum.”</p>
<p>Which takes us back to the boy on the bus, where this story began. He was stopped because a girl on the same bus had recently had a lesson in school about online safety as part of her PSHE (personal, social, health and economic) curriculum. She recognised what he was doing and told her teachers.</p>
<p>Education works.</p>
<p><em> In the UK, the <a data-link-name="in body link" href="https://www.nspcc.org.uk/">NSPCC</a> offers support to children on 0800 1111, and adults concerned about a child on 0808 800 5000. The National Association for People Abused in Childhood (<a data-link-name="in body link" href="https://napac.org.uk/">Napac</a>) offers support for adult survivors on 0808 801 0331. In the US, call or text the <a data-link-name="in body link" href="https://www.childhelp.org/hotline/">Childhelp</a> abuse hotline on 800-422-4453. In Australia, children, young adults, parents and teachers can contact the <a data-link-name="in body link" href="https://kidshelpline.com.au/">Kids Helpline</a> on 1800 55 1800, or <a data-link-name="in body link" href="https://bravehearts.org.au/">Bravehearts</a> on 1800 272 831, and adult survivors can contact <a data-link-name="in body link" href="https://www.blueknot.org.au/">Blue Knot Foundation</a> on 1300 657 380. Other sources of help can be found at <a data-link-name="in body link" href="https://www.childhelplineinternational.org/child-helplines/child-helpline-network/">Child Helplines International</a></em></p>
<p><em><strong> Do you have an opinion on the issues raised in this article? If you would like to submit a response of up to 300 words by email to be considered for publication in our <a data-link-name="in body link" href="https://www.theguardian.com/tone/letters">letters</a> section, please <a data-link-name="in body link | mailto:guardian.letters@theguardian.com?body=Please%20include%20your%20name%E2%80%8B%E2%80%8B,%20full%20postal%20address%20and%20phone%20number%20with%20your%20letter%20below.%20Letters%20are%20usually%20published%20with%20the%20author%27s%20name%20and%20city/town/village.%20The%20rest%20of%20the%20information%20is%20for%20verification%20only%20and%20to%20contact%20you%20where%20necessary." href="mailto:guardian.letters@theguardian.com?body=Please%20include%20your%20name%E2%80%8B%E2%80%8B,%20full%20postal%20address%20and%20phone%20number%20with%20your%20letter%20below.%20Letters%20are%20usually%20published%20with%20the%20author%27s%20name%20and%20city/town/village.%20The%20rest%20of%20the%20information%20is%20for%20verification%20only%20and%20to%20contact%20you%20where%20necessary.">click here</a>.</strong></em></p>
