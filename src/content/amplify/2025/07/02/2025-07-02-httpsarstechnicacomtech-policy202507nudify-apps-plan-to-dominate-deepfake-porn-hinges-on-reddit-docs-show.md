---
author: Ashley Belanger
cover_image: >-
  https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-1152x648.jpg
date: '2025-07-02T22:06:43.000Z'
dateFolder: 2025/07/02
description: >-
  Clothoff is a popular app that makes fake nude images and plans to grow
  worldwide. It ignores lawsuits and buys other similar apps to gain more users.
  Many people, including young girls and celebrities, suffer harm from its fake
  images.
isBasedOn: >-
  https://arstechnica.com/tech-policy/2025/07/nudify-apps-plan-to-dominate-deepfake-porn-hinges-on-reddit-docs-show/
link: >-
  https://arstechnica.com/tech-policy/2025/07/nudify-apps-plan-to-dominate-deepfake-porn-hinges-on-reddit-docs-show/
slug: >-
  2025-07-02-httpsarstechnicacomtech-policy202507nudify-apps-plan-to-dominate-deepfake-porn-hinges-on-reddit-docs-show
tags:
  - ai
  - tech
  - social media
title: 'Nudify app’s plan to dominate deepfake porn hinges on Reddit, docs show'
---
<figure></figure><p> <em> "Obsessed with money" </em></p>
<p>Report: Clothoff ignored California’s lawsuit while buying up 10 rivals.</p>
<p><a href="https://arstechnica.com/author/ashleybelanger/"> Ashley Belanger </a> –  Jul 1, 2025 8:43 PM |</p>
<figure><a data-cropped="true" data-pswp-height="1414" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400.jpg 2121w, https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-640x427.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-1024x683.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-768x512.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-1536x1024.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-2048x1365.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-980x653.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-1440x960.jpg 1440w" data-pswp-width="2121" href="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400.jpg"><img alt="" sizes="(max-width: 1152px) 100vw, 1152px" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-1536x864.jpg" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2161789400-1536x864.jpg 1536w"/></a><figcaption>/ Credit:Larysa Vdovychenko | iStock / Getty Images Plus</figcaption></figure>
<figure></figure><figure></figure><p>Clothoff—one of the leading apps used to quickly and cheaply make fake nudes from images of real people—reportedly is planning a global expansion to continue dominating deepfake porn online.</p>
<p>Also known as a nudify app, Clothoff has resisted attempts to unmask and confront its operators. Last August, the app was among those that San Francisco's city attorney, David Chiu, sued in hopes of forcing a shutdown. But recently, a whistleblower—who had "access to internal company information" as a former Clothoff employee—<a href="https://www.spiegel.de/international/zeitgeist/using-ai-to-humiliate-women-the-men-behind-deepfake-pornography-a-0de338f9-9cec-4ae8-a5ef-9a356b0a5bd4">told the investigative outlet Der Spiegel</a> that the app's operators "seem unimpressed by the lawsuit" and instead of worrying about shutting down have "bought up an entire network of nudify apps."</p>
<p>Der Spiegel found evidence that Clothoff today owns at least 10 other nudify services, attracting "monthly views ranging between hundreds of thousands to several million." The outlet granted the whistleblower anonymity to discuss the expansion plans, which the whistleblower claimed was motivated by Clothoff employees growing "cynical" and "obsessed with money" over time as the app—which once felt like an "exciting startup"—gained momentum. Because generating convincing fake nudes can cost just a few bucks, chasing profits seemingly relies on attracting as many repeat users to as many destinations as possible.</p>
<p>Currently, Clothoff runs on an annual budget of around $3.5 million, the whistleblower told Der Spiegel. It has shifted its marketing methods since its launch, apparently now largely relying on Telegram bots and X channels to target ads at young men likely to use their apps.</p>
<p>Der Spiegel's report documents Clothoff's "large-scale marketing plan" to expand into the German market, as revealed by the whistleblower. The alleged campaign hinges on producing "naked images of well-known influencers, singers, and actresses," seeking to entice ad clicks with the tagline "you choose who you want to undress."</p>
<p>A few of the stars named in the plan confirmed to Der Spiegel that they never agreed to this use of their likenesses, with some of their representatives suggesting that they would pursue legal action if the campaign is ever launched.</p>
<p>However, <a href="https://arstechnica.com/tech-policy/2024/01/fake-ai-taylor-swift-images-flood-x-amid-calls-to-criminalize-deepfake-porn/">even celebrities like Taylor Swift</a> have struggled to combat deepfake nudes spreading online, while tools like Clothoff are increasingly used to torment young girls in <a href="https://arstechnica.com/tech-policy/2024/03/florida-middle-schoolers-charged-with-making-deepfake-nudes-of-classmates/">middle</a> and <a href="https://arstechnica.com/tech-policy/2023/11/deepfake-nudes-of-high-schoolers-spark-police-probe-in-nj/">high school</a>.</p>
<p>Similar celebrity campaigns are planned for other markets, Der Spiegel reported, including British, French, and Spanish markets. And Clothoff has notably already become a go-to tool in the US, not only targeted in the San Francisco city attorney's lawsuit, but also in a <a href="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Jane-Doe-v-KG-complaint-2-2-24.pdf">complaint</a> raised by a high schooler in New Jersey suing a boy who used Clothoff to nudify one of her Instagram photos taken when she was 14 years old, then shared it with other boys on Snapchat.</p>
<p>Clothoff is seemingly hoping to entice more young boys worldwide to use its apps for such purposes. The whistleblower told Der Spiegel that most of Clothoff's marketing budget goes toward "advertising posts in special Telegram channels, in sex subs on Reddit, and on 4chan."</p>
<p>In ads, the app planned to specifically target "men between 16 and 35" who like benign stuff like "memes" and "video games," as well as more toxic stuff like "right-wing extremist ideas," "misogyny," and "Andrew Tate," an influencer <a href="https://www.theguardian.com/australia-news/2025/jul/01/teenagers-as-young-as-13-could-view-misogynistic-andrew-tate-videos-on-youtube-report-finds-ntwnfb">criticized for promoting misogynistic views to teen boys</a>.</p>
<p>Chiu was hoping to defend young women increasingly targeted in fake nudes by shutting down Clothoff, along with several other nudify apps targeted in his lawsuit. But so far, while Chiu has reached a <a href="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Chiu-v-Sol-Ecom-Order-6-2-25.pdf">settlement </a>shutting down two websites, porngen.art and undresser.ai, <a href="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Chiu-v-Sol-Ecom-6-13-25-Declaration-of-Karun-Tilak.pdf">attempts to serve Clothoff</a> through available legal channels have not been successful, deputy press secretary for Chiu's office, Alex Barrett-Shorter, told Ars.</p>
<p>Meanwhile, Clothoff continues to evolve, recently marketing a feature that Clothoff claims attracted more than a million users eager to make explicit videos out of a single picture.</p>
<h2>Clothoff denies it plans to use influencers</h2>
<p>Der Spiegel's efforts to unmask the operators of Clothoff led the outlet to Eastern Europe, after reporters stumbled upon a "database accidentally left open on the Internet" that seemingly exposed "four central people behind the website."</p>
<p>This was "consistent," Der Spiegel said, with a whistleblower claim that all Clothoff employees "work in countries that used to belong to the Soviet Union." Additionally, Der Spiegel noted that all Clothoff internal communications it reviewed were written in Russian, and the site's email service is based in Russia.</p>
<p>A person claiming to be a Clothoff spokesperson named Elias denied knowing any of the four individuals flagged in their investigation, Der Spiegel reported, and disputed the $3 million budget figure. Elias claimed a nondisclosure agreement prevented him from discussing Clothoff's team any further. However, soon after reaching out, Der Spiegel noted that Clothoff took down the database, which had a name that translated to "my babe."</p>
<p>Regarding the shared marketing plan for global expansion, Elias denied that Clothoff intended to use celebrity influencers, saying that "Clothoff forbids the use of photos of people without their consent."</p>
<p>He also denied that Clothoff could be used to nudify images of minors; however, one Clothoff user who spoke to Der Spiegel on the condition of anonymity, confirmed that his attempt to generate a fake nude of a US singer failed initially because she "looked like she might be underage." But his second attempt a few days later successfully generated the fake nude with no problem. That suggests Clothoff's age detection may not work perfectly.</p>
<p>As Clothoff's growth appears unstoppable, the user explained to Der Spiegel why he doesn’t feel that conflicted about using the app to generate fake nudes of a famous singer.</p>
<p>"There are enough pictures of her on the Internet as it is," the user reasoned.</p>
<p>However, that user draws the line at generating fake nudes of private individuals, insisting, "If I ever learned of someone producing such photos of my daughter, I would be horrified."</p>
<p>For young boys who appear flippant about creating fake nude images of their classmates, the consequences have ranged from suspensions to juvenile criminal charges, and for some, there could be other costs. In the lawsuit where the high schooler is attempting to sue a boy who used Clothoff to bully her, there's currently resistance from boys who participated in group chats to share what evidence they have on their phones. If she wins her fight, she's asking for $150,000 in damages per image shared, so sharing chat logs could potentially increase the price tag.</p>
<p>Since she and the San Francisco city attorney each filed their lawsuits, the <a href="https://arstechnica.com/tech-policy/2025/04/congress-close-to-passing-deepfake-law-trump-said-he-wants-to-use-it-himself/">Take It Down Act has passed</a>. That law makes it easier to force platforms to remove AI-generated fake nudes. But <a href="https://arstechnica.com/tech-policy/2025/04/trumps-rush-to-stop-revenge-porn-and-ai-nudes-may-break-encryption/">experts expect the law will face legal challenges</a> over censorship fears, so the very limited legal tool might not withstand scrutiny.</p>
<p>Either way, the Take It Down Act is a safeguard that came too late for the earliest victims of nudify apps in the US, only some of whom are turning to courts seeking justice due to largely opaque laws that made it unclear if generating a fake nude was illegal.</p>
<p>"Jane Doe is one of many girls and women who have been and will continue to be exploited, abused, and victimized by non-consensual pornography generated through artificial intelligence," the high schooler's complaint noted. "Despite already being victimized by Defendant’s actions, Jane Doe has been forced to bring this action to protect herself and her rights because the governmental institutions that are supposed to protect women and children from being violated and exploited by the use of AI to generate child pornography and nonconsensual nude images failed to do so."</p>
