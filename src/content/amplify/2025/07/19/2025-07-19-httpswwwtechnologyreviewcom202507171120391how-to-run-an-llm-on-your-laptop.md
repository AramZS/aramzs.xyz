---
author: Grace Huckins
cover_image: >-
  https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?resize=1200,600
date: '2025-07-20T00:48:29.570Z'
dateFolder: 2025/07/19
description: >-
  It’s now possible to run useful models from the safety and comfort of your own
  computer. Here’s how.
isBasedOn: >-
  https://www.technologyreview.com/2025/07/17/1120391/how-to-run-an-llm-on-your-laptop/
link: >-
  https://www.technologyreview.com/2025/07/17/1120391/how-to-run-an-llm-on-your-laptop/
slug: >-
  2025-07-19-httpswwwtechnologyreviewcom202507171120391how-to-run-an-llm-on-your-laptop
tags:
  - ai
  - tech
title: How to run an LLM on your laptop
---
<p>It’s now possible to run useful models from the safety and comfort of your own computer. Here’s how.</p>
<figure><img alt="a hand holding a usb stick in front of a laptop with an llm diagram" data-smartcrop-focus="50,50" data-wpsmartcrop-natural-dims="1063,598" sizes="(max-width: 32rem) 360px,(max-width: 48rem) 728px,(max-width: 64rem) 808px,(max-width: 80rem) 1064px,(max-width: 90rem) 1126px,1080px" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=2252,1266" srcset="https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=720,480 720w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=360,240 360w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=1456,818 1456w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=728,409 728w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=1616,908 1616w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=808,454 808w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=2128,1196 2128w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=1064,598 1064w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=2252,1266 2252w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=1126,633 1126w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=2160,1214 2160w,https://wp.technologyreview.com/wp-content/uploads/2025/07/llm-install2.jpg?fit=1080,607 1080w"/><figcaption>Stephanie Arnett/MIT Technology Review | Adobe Stock, Envato</figcaption></figure>
<p><strong>MIT Technology Review<em>’s <a href="https://www.technologyreview.com/tag/how-to">How To</a> series helps you get things done. </em></strong></p>
<p>Simon Willison has a plan for the end of the world. It’s a USB stick, onto which he has loaded a couple of his favorite open-weight LLMs—models that have been shared publicly by their creators and that can, in principle, be downloaded and run with local hardware. If human civilization should ever collapse, Willison plans to use all the knowledge encoded in their billions of parameters for help. “It’s like having a weird, condensed, faulty version of Wikipedia, so I can help reboot society with the help of my little USB stick,” he says.</p>
<p>But you don’t need to be planning for the end of the world to want to run an LLM on your own device. Willison, who writes a popular blog about local LLMs and software development, has plenty of compatriots: <a href="https://www.reddit.com/r/LocalLLaMA/">r/LocalLLaMA</a>, a subreddit devoted to running LLMs on your own hardware, has half a million members.</p>
<p>For people who are concerned about privacy, want to break free from the control of the big LLM companies, or just enjoy tinkering, local models offer a compelling alternative to ChatGPT and its web-based peers.</p>
<p>The local LLM world used to have a high barrier to entry: In the early days, it was impossible to run anything useful without investing in pricey GPUs. But researchers have had so much success in shrinking down and speeding up models that anyone with a laptop, or even a smartphone, can now get in on the action. “A couple of years ago, I’d have said personal computers are not powerful enough to run the good models. You need a $50,000 server rack to run them,” Willison says. “And I kept on being proved wrong time and time again.”</p>
<h3><strong>Why you might want to download your own LLM</strong></h3>
<p>Getting into local models takes a bit more effort than, say, navigating to ChatGPT’s online interface. But the very accessibility of a tool like ChatGPT comes with a cost. “It’s the classic adage: If something’s free, you’re the product,” says Elizabeth Seger, the director of digital policy at Demos, a London-based think tank.</p>
<p>OpenAI, which offers both paid and free tiers, trains its models on users’ chats by default. It’s not too difficult to opt out of this training, and it also used to be possible to remove your chat data from OpenAI’s systems entirely, until a recent legal decision in the <em>New York Times</em>’ ongoing lawsuit against OpenAI required the company to maintain all user conversations with ChatGPT.</p>
<p>Google, which has access to a wealth of data about its users, also trains its models on both free and paid users’ interactions with Gemini, and the only way to opt out of that training is to set your chat history to delete automatically—which means that you also lose access to your previous conversations. In general, Anthropic does not train its models using user conversations, but it will train on conversations that have been “flagged for Trust &amp; Safety review.”</p>
<p>Training may present particular privacy risks because of the ways that models internalize, and often recapitulate, their training data. Many people trust LLMs with deeply personal conversations—but if models are trained on that data, those conversations might not be nearly as private as users think, according to some experts.</p>
<p>“Some of your personal stories may be cooked into some of the models, and eventually be spit out in bits and bytes somewhere to other people,” says Giada Pistilli, principal ethicist at the company Hugging Face, which runs a huge library of freely downloadable LLMs and other AI resources.</p>
<p>For Pistilli, opting for local models as opposed to online chatbots has implications beyond privacy. “Technology means power,” she says. “And so who[ever] owns the technology also owns the power.” States, organizations, and even individuals might be motivated to disrupt the concentration of AI power in the hands of just a few companies by running their own local models.</p>
<p>Breaking away from the big AI companies also means having more control over your LLM experience. Online LLMs are constantly shifting under users’ feet: Back in April, ChatGPT <a href="https://openai.com/index/sycophancy-in-gpt-4o/">suddenly started sucking up</a> to users far more than it had previously, and just last week Grok started calling itself MechaHitler on X.</p>
<p>Providers tweak their models with little warning, and while those tweaks might sometimes improve model performance, they can also cause undesirable behaviors. Local LLMs may have their quirks, but at least they are consistent. The only person who can change your local model is you.</p>
<p>Of course, any model that can fit on a personal computer is going to be less powerful than the premier online offerings from the major AI companies. But there’s a benefit to working with weaker models—they can inoculate you against the more pernicious limitations of their larger peers. Small models may, for example, hallucinate more frequently and more obviously than Claude, GPT, and Gemini, and seeing those hallucinations can help you build up an awareness of how and when the larger models might also lie.</p>
<p>“Running local models is actually a really good exercise for developing that broader intuition for what these things can do,” Willison says.</p>
<h3><strong>How to get started</strong></h3>
<p>Local LLMs aren’t just for proficient coders. If you’re comfortable using your computer’s command-line interface, which allows you to browse files and run apps using text prompts, <a href="https://ollama.com/">Ollama</a> is a great option. Once you’ve installed the software, you can download and run any of the hundreds of models they offer with a <a href="https://github.com/ollama/ollama/blob/main/README.md#quickstart">single command</a>.</p>
<p>If you don’t want to touch anything that even looks like code, you might opt for <a href="https://lmstudio.ai/">LM Studio</a>, a user-friendly app that takes a lot of the guesswork out of running local LLMs. You can browse models from Hugging Face from right within the app, which provides plenty of information to help you make the right choice. Some popular and widely used models are tagged as “Staff Picks,” and every model is labeled according to whether it can be run entirely on your machine’s speedy GPU, needs to be shared between your GPU and slower CPU, or is too big to fit onto your device at all. Once you’ve chosen a model, you can download it, load it up, and start interacting with it using the app’s chat interface.</p>
<p>As you experiment with different models, you’ll start to get a feel for what your machine can handle. According to Willison, every billion model parameters require about one GB of RAM to run, and I found that approximation to be accurate: My own 16 GB laptop managed to run Alibaba’s <a href="https://huggingface.co/Qwen/Qwen3-14B">Qwen3 14B</a> as long as I quit almost every other app. If you run into issues with speed or usability, you can always go smaller—I got reasonable responses from Qwen3 8B as well.</p>
<p>And if you go really small, you can even run models on your cell phone. My beat-up iPhone 12 was able to run Meta’s Llama 3.2 1B using an app called LLM Farm. It’s not a particularly good model—it very quickly goes off into bizarre tangents and hallucinates constantly—but trying to coax something so chaotic toward usability can be entertaining. If I’m ever on a plane sans Wi-Fi and desperate for a probably false answer to a trivia question, I now know where to look.</p>
<p>Some of the models that I was able to run on my laptop were effective enough that I can imagine using them in my journalistic work. And while I don’t think I’ll depend on phone-based models for anything anytime soon, I really did enjoy playing around with them. “I think most people probably don’t need to do this, and that’s fine,” Willison says. “But for the people who want to do this, it’s so much fun.”</p>
<figure></figure>
