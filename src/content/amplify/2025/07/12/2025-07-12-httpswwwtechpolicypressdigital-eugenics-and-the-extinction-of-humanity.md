---
author: Émile P. Torres
cover_image: >-
  https://cdn.sanity.io/images/3tzzh18d/production/a4ecaf8ed2e034163219e0514fa226245810ca46-1200x675.png
date: '2025-07-12T13:15:39.379Z'
dateFolder: 2025/07/12
description: >-
  If we are to combat the AI industry’s push to build digital gods, we have to
  understand the ideology of the antihumanist project, writes Dr. Émile P.
  Torres.
isBasedOn: 'https://www.techpolicy.press/digital-eugenics-and-the-extinction-of-humanity/'
link: 'https://www.techpolicy.press/digital-eugenics-and-the-extinction-of-humanity/'
slug: >-
  2025-07-12-httpswwwtechpolicypressdigital-eugenics-and-the-extinction-of-humanity
tags:
  - tech
  - culture
title: Digital Eugenics and the Extinction of Humanity
---
<p>Perspective</p>
<p><em>This piece is part of “Ideologies of Control: A Series on Tech Power and Democratic Crisis,” in collaboration with</em> <em><a href="https://datasociety.net/">Data &amp; Society</a>. Read more about the series</em> <em><a href="https://www.techpolicy.press/tech-power-and-the-crisis-of-democracy/">here</a>.</em></p>
<figure><img alt=" " data-nimg="1" src="https://cdn.sanity.io/images/3tzzh18d/production/a4ecaf8ed2e034163219e0514fa226245810ca46-1200x675.png"/><figcaption>Detail from Puppeteering Virtual Reality by Hanna Barakat &amp; Cambridge Diversity Fund / <a href="https://betterimagesofai.org/images?artist=HannaBarakat&amp;title=PuppeteeringVirtualReality">Better Images of AI</a> / <a href="https://creativecommons.org/licenses/by/4.0/">CC by 4.0</a></figcaption></figure>
<p>In a recent <em>New York Times</em> <a href="https://www.nytimes.com/2025/06/26/opinion/peter-thiel-antichrist-ross-douthat.html">interview</a>, billionaire tech investor Peter Thiel was asked whether he “would prefer the human race to endure” in the future. Thiel responded with an uncertain, “Uh —,” leading the interviewer, columnist Ross Douthat, to note with a hint of consternation, “You’re hesitating.” The rest of the exchange went:</p>
<blockquote>Thiel: Well, I don’t know. I would — I would —<br/>Douthat: This is a long hesitation!<br/>Thiel: There’s so many questions implicit in this.<br/>Douthat: Should the human race survive?<br/>Thiel: Yes.<br/>Douthat: OK.</blockquote>
<p>Immediately after this exchange, Thiel went on to say that he wants humanity to be radically transformed by technology to become immortal creatures fundamentally different from our current state.</p>
<p>"There’s a critique of, let’s say, the trans people in a sexual context, or, I don’t know, a transvestite is someone who changes their clothes and cross dresses, and a transsexual is someone where you change your, I don’t know, penis into a vagina,” he said. None of this goes far enough—“we want more transformation than that," he said. "We want more than cross-dressing or changing your sex organs. We want you to be able to change your heart and change your mind and change your whole body."</p>
<p>Compare this radical vision to the goals of 20th-century eugenicists. They wanted <a href="https://edition.cnn.com/2018/10/16/us/eugenics-craze-america-pbs">certain groups</a> <em>within</em> the human species ( white people, the Aryan race, etc.) to become dominant over other demographics. Transhumanists like Thiel apparently want to create an entirely <em>new species</em> to take over and rule the world.</p>
<p>In fact, the context in which Douthat asked Thiel whether he wants humanity to survive was a comment from Douthat that “a number of people deeply involved in artificial intelligence see [AI] as a mechanism for transhumanism—for transcendence of our mortal flesh—and either some kind of creation of a successor species or some kind of merger of mind and machine.” Douthat asks, “Do you think that’s all irrelevant fantasy? … Is it something you worry about?” This is the point in their discussion when Thiel hesitated to answer whether “the human race [should] endure.”</p>
<p>The idea that we should create a “successor species” in the form of AI or digital posthumans is slightly different from what Thiel envisions. Later in the<em> New York Times</em> interview, Thiel mentions the possibility of uploading our minds to computers, thus becoming digital beings ourselves. “I’d rather have my body,” he says. “I don’t want just a computer program that simulates me.”</p>
<p>This points to a variant of transhumanism that <a href="https://www.youtube.com/shorts/jCpmdb_0Cf0">we can call</a> <em>digital eugenics</em>. Whereas Thiel wants to keep his biological substrate, transforming it into something immortal and radically different from what we currently have, digital eugenicists want to do away with biology altogether. There are two possibilities here, which are not mutually exclusive.</p>
<p>First, we could completely digitize our minds, thus becoming digital posthumans ourselves. The cofounder and CEO of OpenAI, Sam Altman, <a href="https://www.technologyreview.com/2018/03/13/144721/a-startup-is-pitching-a-mind-uploading-service-that-is-100-percent-fatal/">signed up</a> with a startup called Nectome to have his brain preserved when he dies prematurely so it can be uploaded to a computer. He believes that this will become technologically feasible within his lifetime. Many other transhumanists are pursuing the same possibility: they strive to achieve “cyberimmortality” by somehow transferring all the relevant information about the microstructure of their brains to computer hardware. As media theorist Douglas Rushkoff <a href="https://www.google.com/books/edition/Survival_of_the_Richest_Escape_Fantasies/v_NhEAAAQBAJ?hl=en&amp;gbpv=1&amp;dq=%22seek+to+go+meta+on+themselves,+convert+into+digital+form,+and+migrate+to+that+realm+as+robots,+artificial+intelligences,+or+mind+clones%22&amp;pg=PT75&amp;printsec=frontcover">writes</a> in <em>The Survival of the Richest</em>, these people “seek to go meta on themselves, convert into digital form, and migrate to that realm as robots, artificial intelligences, or mind clones.” Elon Musk’s former partner, Grimes, even wrote a <a href="https://www.youtube.com/watch?v=qnk9ovzMLy8">song</a> about this:</p>
<blockquote>I wanna be software<br/>Upload my mind<br/>Take all my data<br/>What will you find?<br/>…<br/>I wanna be software<br/>The best design<br/>Infinite princess<br/>Computer mind</blockquote>
<p>The problem with this proposal is that there’s no good reason to believe that an uploaded mind would be <em>you</em>. We’re not even sure that you could create a conscious mind through uploading—but even if that <em>were</em> possible, would the upload be the same <em>person</em> as you, or someone else? Many philosophers, such as Massimo Pigliucci, argue that it wouldn’t be you. Pigliucci <a href="https://philarchive.org/archive/PIGMUA">says</a> that in pushing the “upload” button, “you would simply be committing a very technologically sophisticated (and likely very, very expensive) form of suicide.”</p>
<p>Mind-uploading, therefore, is not the same as <em>self-uploading</em>. Just consider that uploaded minds can be duplicated: all you’d have to do is copy-paste the program and <em>voila </em>you have two copies of the same mind. In contrast, it seems conceptually incoherent to say that <em>selves</em> can be duplicated. Imagine that there are two copies of you and one of them dies. That would mean that <em>you</em> are both alive and dead at the very same time. Or imagine that one of those copies is in France while the other is in China. That would mean that <em>you</em> can simultaneously be in two places at once. Does that make any sense? The correct answer is “no”—selves are not the sorts of things that can be duplicated, and hence it wouldn’t be possible to upload your<em>self</em> to a computer.</p>
<p>The second option for digital eugenicists is to replace our species not by uploading ourselves, but by creating autonomous AIs that are entirely separate from us. ChatGPT is a system of this sort: it’s an artificial neural network that exists on its own, born as a unique entity in the world. While Peter Thiel does not endorse this form of digital eugenics (as far as I can tell), it has become wildly popular within Silicon Valley over the past decade.</p>
<p>In a recent interview with <em>Vox</em>, the virtual reality pioneer and tech critic Jaron Lanier <a href="https://www.vox.com/the-gray-area/407154/jaron-lanier-ai-religion-progress-criticism">said</a> that he meets many people working in AI “who believe … that it would be good to wipe out people and that the AI future would be a better one.” He adds that some of these people don’t want to have a “bio baby” because they worry that it would make them feel attached to the biological world. “It’s much more important to be committed to the AI of the future,” he continued, “and so to have human babies is fundamentally unethical.”</p>
<p>Last month, an AI market researcher committed to "creating posthuman intelligence" named Daniel Faggella hosted a symposium in a San Francisco mansion <a href="https://x.com/xriskology/status/1929707364728688747/photo/1">titled</a> “Worthy Successor: AI and the Future After Humankind.” He <a href="https://www.wired.com/story/ai-risk-party-san-francisco/">reports</a> that it was attended by “AI founders, researchers from all the top Western labs, and ‘most of the important philosophical thinkers on AGI.’” A ‘Worthy Successor,’ he <a href="https://danfaggella.com/potentia/">argues</a>, is “a posthuman intelligence so capable and morally valuable that you would gladly prefer that it (not humanity) control the government, and determine the future path of life itself.” He thus <a href="https://danfaggella.com/worthy/#:~:text=I%20argue%20that%20the%20great,value%20than%20all%20of%20humanity.">claims</a> that “the great (and ultimately, only) moral aim of artificial general intelligence should be the creation of Worthy Successor—an entity with more capability, intelligence, ability to survive and … moral value than all of humanity” put together.</p>
<p>As I’ve <a href="https://www.truthdig.com/articles/the-endgame-of-edgelord-eschatology/">noted before</a>, the cofounder of Google, Larry Page, contends that “digital life is the natural and desirable next step in … cosmic evolution and … if we let digital minds be free rather than try to stop or enslave them, the outcome is almost certain to be good.” Google owns DeepMind, which is explicitly trying to build superintelligent AIs—exactly the sorts of entities that might someday soon replace us. Similarly, the AI blogger Eliezer Yudkowsky <a href="https://www.truthdig.com/articles/the-endgame-of-edgelord-eschatology/">says</a> that “if sacrificing all of humanity were the only way, and a reliable way, to get … god-like things out there—superintelligences who still care about each other, who are still aware of the world and having fun—I would ultimately make that trade-off.”</p>
<p>Philosopher Derek Shiller develops this perspective in <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/bioe.12340?casa_token=bBRnqlcoGf8AAAAA%3A3JvR9BZs3nJf8qo8m51hpNL2gxUe4_iWTaicWIut5M9ZpRBHQkdJOgxK4Pmc8UbWbw14oQ3q429h7OMT">arguing</a> that</p>
<blockquote>...our resources are finite, and the same resources that might allow human beings to live … could be more effectively spent on creating and sustaining artificial creatures. When that becomes the case, the beneficent thing to do is to choose that our children be artificial, rather than natural.</blockquote>
<p>He concludes that we should then “engineer our extinction so that our planet’s resources can be devoted to making artificial creatures with better lives.”</p>
<p>We must call this out for <a href="https://link.springer.com/article/10.1007/s11229-025-05094-4?utm_source=rct_congratemailt&amp;utm_medium=email&amp;utm_campaign=nonoa_20250701&amp;utm_content=10.1007/s11229-025-05094-4">what it is</a>: <em>pro-extinctionism</em>. The biological transhumanism of Thiel and the digital eugenics of Altman, Faggella, Yudkowsky, and the others—all of these views aim to supplant the human species with some kind of successors, which would then proceed (on the most popular view) to plunder Earth’s remaining resources and launch itself into space to conquer the universe. This is overtly pro-extinctionist, and given that some of the most powerful people in one of the most powerful centers of society—Silicon Valley—accept it, we must conclude that pro-extinctionism is not a fringe ideology, but closer to the mainstream.</p>
<p>Even worse, these pro-extinctionists promote their ideology by claiming that, in fact, they <em>oppose</em> human extinction. This relies on two linguistic tricks: first, <a href="https://www.truthdig.com/articles/team-human-vs-team-posthuman-which-side-are-you-on/">many define</a> “humanity” in an idiosyncratic way that diverges from the definition that most of us intuitively accept. We tend to equate <em>humanity</em> with <em>Homo sapiens</em>, our species, whereas these people define “humanity” as including whatever <em>posthuman</em> descendants we might have. Hence, as I’ve highlighted <a href="https://www.truthdig.com/articles/team-human-vs-team-posthuman-which-side-are-you-on/">elsewhere</a>, our species could die out next year without “human extinction” having happened—so long as we’re <em>replaced</em> by posthumans, then “humanity” will live on. When they talk about avoiding “human extinction,” they aren’t talking about the extinction of our species. To the contrary, our extinction wouldn’t matter one bit once posthumanity arrives.</p>
<p>The <a href="https://www.truthdig.com/articles/a-tale-of-two-extinctions/">second trick</a> hangs on a distinction between two <a href="https://link.springer.com/article/10.1007/s11229-025-05094-4?utm_source=rct_congratemailt&amp;utm_medium=email&amp;utm_campaign=nonoa_20250701&amp;utm_content=10.1007/s11229-025-05094-4">types of extinction scenarios</a>. <em>Terminal</em> extinction would happen if our species were to die out. <em>Final </em>extinction would happen if our species were to die out <em>without</em> leaving behind any successors. Digital eugenicists who believe our cosmic mission is to create a utopian pleasure-world of superintelligent posthumans dancing across the stars couldn’t care less about terminal extinction. All they care about is final extinction. In other words, if our species were to die out without leaving behind any successors, this would be catastrophically terrible—a tragedy of quite literally cosmic proportions. But if our species were to die out <em>after</em> creating our posthuman replacements—who they would classify as “human” on their idiosyncratic definition—then this would be perfectly fine. It might even be <em>very good</em>, as Faggella, Page, Shiller, and the others would claim.</p>
<p>These two tricks enable them to claim that <em>avoiding</em> human extinction is really important, when in fact they’re only talking about one type of “human extinction”—final extinction—while using the word “human” to include whatever replaces us. This is what makes their version of pro-extinctionism especially insidious, dangerous, and effective: many outsiders will find themselves nodding along to claims that “avoiding human extinction is very important” without realizing that the biological transhumanists and digital eugenicists are actually advocating <em>for</em> our extinction (that is, for the terminal extinction of our species).</p>
<p>If we are to effectively combat the AI industry’s accelerating push to build digital gods in the form of AGI, we have to understand the ideological underpinnings of this antihumanist project. We have to understand how they manipulate language to make themselves appear to be friends of “humanity.” We have to decode their propaganda and see their agenda for what it really is: pro-extinctionism bound up with bizarre techno-utopian fantasies of inaugurating a fundamentally new epoch in cosmic history—one in which <em>Homo sapiens </em>will be disempowered, marginalized, and eventually eliminated.</p>
