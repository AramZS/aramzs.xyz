---
author: Dave Karpf
cover_image: >-
  https://substackcdn.com/image/fetch/$s_!FYvE!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc034f448-3b1d-490d-9a6b-fa66c879f1fe_1098x1102.png
date: '2025-07-17T11:47:54.240Z'
dateFolder: 2025/07/17
description: >-
  Here are five things that I believe to be true about generative Artificial
  Intelligence as it exists today.
isBasedOn: >-
  https://davekarpf.substack.com/p/five-things-i-believe-about-actually?r=eeyg&triedRedirect=true
link: >-
  https://davekarpf.substack.com/p/five-things-i-believe-about-actually?r=eeyg&triedRedirect=true
slug: >-
  2025-07-17-httpsdavekarpfsubstackcompfive-things-i-believe-about-actuallyreeygandtriedredirecttrue
tags:
  - ai
  - business
title: Five things I believe about actually-existing AI today.
---
<div><div dir="auto"><p>Here are five things that I believe to be true about generative Artificial Intelligence as it exists today. </p><p>(I’ve written about a few of these points before, but it has been a couple of years. I’ll include links to those old pieces below.)</p><p><hr/></p><ol><li><p><strong><span>Generative AI is best understood as a </span><a href="https://davekarpf.substack.com/p/on-generative-ai-and-satisficing" rel="">satisficing technology</a><span>.</span></strong><span> </span></p></li></ol><p>Satisficing is a portmanteau of “satisfy” and “suffice.” It was coined by Herbert Simon in the 1970s. </p><p><span>In layman’s terms, satisficing is the process of (1) establishing the threshold where your work product is </span><em>good enough</em><span>, (2) working until you have reached that threshold and then (3) stopping. Think of satisficing as an alternative to maximizing — expending maximum effort to produce your very best work. </span></p><p><span>Actually-existing AI is best-suited to scenarios where </span><em>good enough</em><span> is all you need. The 50-page report that has to be submitted but, realistically, no one was ever going to read.  Recording the minutes of a routine zoom call. Preparing an itinerary for a family trip to Disneyland. </span></p><p>Making that distinction between situations where you need to do your best work and ones where you just need to go through the motions is a good way to make sense of when and if the technology can be useful.</p><p><span>(For researchers and organizations, it is also well-suited to a lot of tasks where machine learning was already being used. If you were </span><em>already</em><span> </span><em>using </em><span>natural language processing for sentiment analysis, then generative AI is often going to be an upgrade.)</span></p><div data-component-name="DigestPostEmbed"><div><a href="https://davekarpf.substack.com/p/on-generative-ai-and-satisficing" rel="noopener" target="_blank"><h2>On Generative AI and Satisficing</h2></a><div><p>I’ve been thinking recently about how generative AI tools might fit into our lives. The best framework I can come up with revolves around Herbert Simon’s concept of “satisficing.”</p></div></div></div><ol start="2"><li><p><strong>Many of the problems with generative AI are because Private Equity/Venture Capital/Unregulated Industrialists have a different satisficing threshold than the rest of us.</strong></p></li></ol><p><span>Ted Chiang wrote my favorite essay on this topic: “</span><a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/will-ai-become-the-new-mckinsey" rel="">Will A.I. Become the New McKinsey</a><span>.” I recommend it every chance I get.</span></p><p><span>I also just finished reading Megan Greenwell’s new book, </span><em><a href="https://www.harpercollins.com/products/bad-company-megan-greenwell?variant=43151012757538" rel="">Bad Company: Private Equity and the Death of the American Dream</a></em><span>. The book is a riveting, a searing portrait of the costs we all collectively pay by allowing Private Equity vultures to continue to play a rigged game.</span></p><p><span>It is no surprise that generative AI has mostly been deployed to offer worse products at lower costs across a range of industries. The satisficing threshold for </span><em>good enough</em><span> is not set by individual journalists or doctors, or even by managing editors or hospital administrators. It is set by owners who know nothing about the product and care about nothing other than immediate returns.</span></p><p>One could imagine a version of generative AI that is useful to journalists, to medical professionals, to political organizers, etc. But the future of media, medicine, politics, etc is not developed in a vacuum by well-intentioned professionals. </p><p>This all seems plainly obvious if we imagine what actually-existing capital will likely try to use these technologies to achieve. And it is utterly invisible if we focus solely on the technologies themselves.</p><ol start="3"><li><p><strong>We should be much more worried about technology’s second failure mode than its first.</strong></p></li></ol><p><span>When we imagine the future of any emerging technology, there are two distinct failure modes. One can either imagine what might happen when a technology works </span><strong>as advertised, but at larger scale</strong><span>, or one can imagine what would happen </span><strong>if a technology breaks down, or does not work as well as advertised.</strong></p><p><span>Much of the discourse surrounding Artificial General Intelligence (or Superintelligence, the new term du jour) operates in the first failure mode. Eliezer Yudkowsky and the </span><a href="https://ai-2027.com/" rel="">AI 2027</a><span> people assume that we will have superintelligent AI very soon, and insist that the results will be disastrous. Sam Altman and Dario Amodei agree that we’ll have superintelligent AI quite soon, but figure </span><em>aw shucks it’ll just be incredible once we all adapt</em><span>.</span></p><p><span>…Meanwhile, Grok is </span><a href="https://www.wired.com/story/grok-antisemitic-posts-x-xai/" rel="">declaring itself Mechhitler</a><span> and the Department of Defense has announced a </span><a href="https://www.theguardian.com/technology/2025/jul/14/us-military-xai-deal-elon-musk" rel="">$200 million contract with xAI</a><span>.</span></p><p>I am not so worried about Grok becoming a superintelligent skynet. I am quite worried about the DoD handing critical responsibilities to an unreliable large language model run by a company that constantly overpromises and underdelivers. </p><div data-component-name="DigestPostEmbed"><div><a href="https://davekarpf.substack.com/p/two-failure-modes-of-emerging-technologies" rel="noopener" target="_blank"><h2>Two Failure Modes of Emerging Technologies</h2></a><div><p>There is a pervasive sense right now that, in the field of artificial intelligence, we are living in early times. Depending who you ask, AI is some mix of exciting, inevitable, and scary. But all agree that it is real, it is here, and it is growing. The present is merely prelude.</p></div></div></div><ol start="4"><li><p><strong>This is going to amplify dangerous conspiracy theories. If you keep talking about these systems as “artificially intelligent,” don’t be surprised when people find signs from God in there.</strong></p></li></ol><p><span>QAnon and the January 6th Big Lie both predate ChatGPT. Both were flimsy fucking theories. The QAnon crowd </span><a href="https://mediaengagement.org/wp-content/uploads/2024/10/157-QAnon-in-the-News-Case-Study.pdf" rel="">thought JFK Jr was secretly alive</a><span> and ready to reveal himself. They clung to the belief that DC pizza joint Comet Ping Pong was holding child sex slaves in cages in the basement. Comet Ping Pong does not have a basement. The January 6 conspiracists thought that Democrats controlled Georgia’s voting machines, despite Republicans being, y’know, </span><em>in charge of the whole damn state government</em><span>. </span></p><p><a href="https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html" rel="">Kashmir Hill had a story last month</a><span> about people looking to Chatbots for answers and falling down conspiratorial rabbit holes. My friend </span><a href="https://buildcognitiveresonance.substack.com/p/fuel-of-delusions" rel="">Ben Riley has written on the topic as well</a><span>. All of this is terrible. None of it is surprising. All of it will get worse. I do not see how it gets better anytime soon. (It surely won’t get better in the absence of government regulation.) </span></p><p><span>There are a lot of things that I like about Arvind Narayanan and Sayash Kapoor’s recent paper on “</span><a href="https://knightcolumbia.org/content/ai-as-normal-technology" rel="">AI as Normal Technology</a><span>.” One reason why I would much prefer that we refer to LLMs as “Machine Learning” instead of “Artificial Intelligence” is that, if you tell people you’re building digital god, some of them will surely believe you.</span></p><ol start="5"><li><p><strong>There is a bubble. It isn’t going to break anytime soon.</strong></p></li></ol><p><span>I have pretty firmly planted my flag in the </span><em>AI skeptics</em><span> camp. I don’t use AI for writing or for teaching — not because of some grand moral opposition, but because I don’t find it the least bit useful to my workflow.</span></p><p><span>I don’t think generative AI is pure vaporware, but I also don’t think it will ultimately qualify as a general purpose technology. When the dust settles, I suspect it will be transformative in roughly the same </span><a href="https://davekarpf.substack.com/p/bullet-points-on-the-social-trajectory" rel="">ways that the word processor was transformative</a><span>. </span></p><p><span>But if I’m right about that, then AI is currently in a massive financial bubble. The multi-billion-dollar valuations, the spending spree on talent and on chips and on </span><a href="https://techcrunch.com/2025/07/14/mark-zuckerberg-says-meta-is-building-a-5gw-ai-data-center/" rel="">gigawatt-scale data centers</a><span>… It all feels a lot like the late dotcom-era glut of broadband investment. </span></p><p><span>The returns are simply </span><a href="https://davekarpf.substack.com/p/on-ai-agents-how-are-these-digital" rel="">never going to recoup the investment costs</a><span>.</span></p><p><a href="https://www.wheresyoured.at/" rel="">Ed Zitron</a><span> has been the loudest and clearest AI critic on this point. He has argued at great length, for a couple years now, that this is a financial bubble and it is about to burst.</span></p><p><span>The one point where I pretty strongly disagree with Zitron is that I don’t expect this bubble to burst anytime soon. “The market can stay irrational longer than you can stay solvent.” We are living in exceptionally irrational times. Just look at Tesla’s overvalued stock. </span><a href="https://davekarpf.substack.com/p/bitcoin-is-worth-100000-it-still" rel="">Just look at Bitcoin’s recovery</a><span>.</span></p><p>The entire stock market is being propped up by companies whose valuation is anchored to the AI futurity bubble. When that pops, it is going to be a cataclysmic event for the whole finance sector. And that means, in turn, that the whole finance sector will pull out every trick to keep the system running a little longer.</p><p><span>We should be clear-eyed and critical of these technologies. We should be skeptical of the promises made by </span><a href="https://bsky.app/profile/davekarpf.bsky.social/post/3lshsq3ma222a" rel="">carnival barkers like Sam Altman</a><span>. But let’s put the brakes on predictions of its imminent demise. </span></p></div></div>
