---
author: By Will Oremus
cover_image: >-
  https://www.washingtonpost.com/wp-apps/imrs.php?src=https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/JWZSYXNTGT32YU52IE6ORO5MZM.JPG&w=1440
date: '2025-07-24T17:47:02.257Z'
dateFolder: 2025/07/24
description: >-
  The president’s executive order could reshape chatbots’ politics. But experts
  say training AI models to be neutral is easier said than done.
isBasedOn: >-
  https://www.washingtonpost.com/technology/2025/07/24/trump-ai-woke-executive-order/
link: >-
  https://www.washingtonpost.com/technology/2025/07/24/trump-ai-woke-executive-order/
slug: >-
  2025-07-24-httpswwwwashingtonpostcomtechnology20250724trump-ai-woke-executive-order
tags:
  - ai
  - politics
title: Trump is targeting ‘woke AI.’ Here’s what that means. - The Washington Post
---
<p>The president’s executive order could reshape chatbots’ politics. But experts say training AI models to be neutral is easier said than done.</p>
<p>President Donald Trump signed an executive order Wednesday to steer federal contracts toward companies whose AI models are deemed free of ideological bias.</p>
<p>The order, issued as part of the administration’s rollout of a wide-ranging “<a href="https://www.washingtonpost.com/politics/2025/07/23/trump-ai-action-plan-big-tech/">AI Action Plan</a>,” takes aim at what Trump calls “<a href="https://www.washingtonpost.com/technology/2023/02/24/woke-ai-chatgpt-culture-war/">woke AI</a>” — chatbots, image generators and other tools whose outputs are perceived as exhibiting a liberal bias. It specifically bars federal agencies from procuring AI models that promote diversity, equity and inclusion, or DEI.</p>
<p>“From now on,” Trump said, “the U.S. government will deal only with AI that pursues truth, fairness and strict impartiality.”</p>
<p>But what is ‘woke AI,' exactly, and how can tech companies avoid it?</p>
<p>Experts on the technology say the answer to both questions is murky. Some lawyers say the prospect of the Trump administration shaping what AI chatbots can and can’t say raises First Amendment issues.</p>
<p>“These are words that seem great — ‘free of ideological bias,’” said Rumman Chowdhury, executive director of the nonprofit Humane Intelligence and former head of machine learning ethics at Twitter. “But it’s impossible to do in practice.”</p>
<p>The concern that popular AI tools exhibit a liberal skew took hold on the right in 2023, when examples circulated on social media of OpenAI’s ChatGPT endorsing affirmative action and transgender rights or refusing to compose a poem praising Trump. It gained steam last year when Google’s Gemini image generator was found to be <a href="https://www.washingtonpost.com/technology/2024/02/22/google-gemini-ai-image-generation-pause/">injecting ethnic diversity</a> into inappropriate contexts — such as portraying Black, Asian and Native American people in response to requests for images of Vikings, Nazis or America’s “Founding Fathers.”</p>
<p>Google apologized and <a href="https://www.washingtonpost.com/technology/2024/02/22/google-gemini-ai-image-generation-pause/">reprogrammed the tool</a>, saying the outputs were an <a href="https://blog.google/products/gemini/gemini-image-generation-issue/">inadvertent by-product</a> of its effort to ensure that the product appealed to a range of users around the world.</p>
<p>ChatGPT and other AI tools can indeed exhibit a liberal bias in certain situations, said Fabio Motoki, a lecturer at the University of East Anglia. In a <a href="https://www.sciencedirect.com/science/article/pii/S0167268125000241">study published last month</a>, he and his co-authors found that OpenAI’s GPT-4 responded to political questionnaires by evincing views that aligned closely with those of the average Democrat.</p>
<p>But assessing a chatbot’s political leanings “is not straightforward,” he added<b>.</b> On certain topics, such as the need for U.S. military supremacy, OpenAI’s tools tend to produce writing and images that align more closely with Republican views. And other research, including an analysis by The Post, has found that AI image generators often reinforce <a href="https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/">ethnic, religious and gender stereotypes</a>.</p>
<p>AI models exhibit all kinds of biases, experts say. It’s part of how they work.</p>
<p>Chatbots and image generators draw on vast quantities of data ingested from across the internet to predict the most likely or appropriate response to a user’s query. So they might respond to one prompt by spouting misogynist tropes gleaned from an unsavory anonymous forum — then respond to a different prompt by regurgitating DEI policies scraped from corporate hiring policies.</p>
<p>Training an AI model to avoid such biases is notoriously tricky, Motoki said. You could try to do it by limiting the training data, paying humans to rate its answers for neutrality, or writing explicit instructions into its code. But all three approaches come with limitations and have been known to backfire by making the model’s responses less useful or accurate.</p>
<p>“It’s very, very difficult to steer these models to do what we want,” he said.</p>
<p>Google’s Gemini blooper was one example. Another came this year, when Elon Musk’s xAI <a href="https://www.washingtonpost.com/technology/2025/05/24/grok-musk-ai/">instructed its Grok chatbot</a> to prioritize “truth-seeking” over political correctness — leading it to spout racist and antisemitic conspiracy theories and at one point even refer to itself as “mecha-Hitler.”</p>
<p>Political neutrality, for an AI model, is simply “not a thing,” Chowdhury said. “It’s not real.”</p>
<p>For example, she said, if you ask a chatbot for its views on gun control, it could equivocate by echoing both Republican and Democratic talking points, or it might try to find the middle ground between the two. But the average AI user in Texas might see that answer as exhibiting a liberal bias, while a New Yorker might find it overly conservative. And to a user in Malaysia or France, where strict gun control laws are taken for granted, the same answer would seem radical.</p>
<p>How the Trump administration will decide which AI tools qualify as neutral is a key question, said Samir Jain, vice president of policy at the nonprofit Center for Democracy and Technology.</p>
<p>The executive order itself is not neutral, he said, because it rules out certain left-leaning viewpoints but not right-leaning viewpoints. The order lists “critical race theory, transgenderism, unconscious bias, intersectionality, and systemic racism” as concepts that should not be incorporated into AI models.</p>
<p>“I suspect they would say anything providing information about transgender care would be ‘woke,’” Jain said. “But that’s inherently a point of view.”</p>
<p>Imposing that point of view on AI tools produced by private companies could run the risk of a First Amendment challenge, he said, depending on how it’s implemented.</p>
<p>“The government can’t force particular types of speech or try to censor particular viewpoints, as a general matter,” Jain said. However, the administration does have some latitude to set standards for the products it purchases, provided its speech restrictions are related to the purposes for which it’s using them.</p>
<p>Some analysts and advocates said they believe Trump’s executive order is less heavy-handed than they had feared.</p>
<p>Neil Chilson, head of AI policy at the right-leaning nonprofit Abundance Institute, said the prospect of an overly prescriptive order on ‘woke AI’ was the one element that had worried him in advance of Trump’s AI plan, which he generally supported. But after reading the order, he said on Thursday that those concerns were “overblown” and he believes the order “will be straightforward to comply with.”</p>
<p>Mackenzie Arnold, director of U.S. policy at the Institute for Law and AI, a nonpartisan think tank, said he was glad to see the order makes allowances for the technical difficulty of programming AI tools to be neutral and offers a path for companies to comply by disclosing their AI models’ instructions.</p>
<p>“While I don’t like the styling of the EO on ‘preventing woke AI’ in government, the actual text is pretty reasonable,” he said, adding that the big question is how the administration will enforce it.</p>
<p>“If it focuses its efforts on these sensible disclosures, it’ll turn out OK,” he said. “If it veers into ideological pressure, that would be a big misstep and bad precedent.”</p>
