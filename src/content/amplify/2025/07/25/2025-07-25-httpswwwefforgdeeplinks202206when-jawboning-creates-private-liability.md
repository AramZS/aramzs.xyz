---
author: David Greene
cover_image: 'https://www.eff.org/files/banner_library/intermediary-4.jpg'
date: '2025-07-25T17:04:45.097Z'
dateFolder: 2025/07/25
description: >-
  A (Very) Narrow Path to Holding Social Media Companies Legally Liable for
  Collaborating with Government in Content ModerationFor the last several years
  we have seen numerous arguments that social media platforms are "state actors"
  that “must carry” all user speech. According to this argument, they...
isBasedOn: 'https://www.eff.org/deeplinks/2022/06/when-jawboning-creates-private-liability'
link: 'https://www.eff.org/deeplinks/2022/06/when-jawboning-creates-private-liability'
slug: >-
  2025-07-25-httpswwwefforgdeeplinks202206when-jawboning-creates-private-liability
tags:
  - tech
  - politics
  - social media
title: When “Jawboning” Creates Private Liability
---
<h3>A (Very) Narrow Path to Holding Social Media Companies Legally Liable for Collaborating with Government in Content Moderation</h3>
<p>For the last several years we have seen numerous arguments that social media platforms are "state actors" that “must carry” all user speech. According to this argument, they are legally required to publish all user speech and treat it equally. Under U.S. law, this is almost always incorrect. The First Amendment generally requires only governments to honor free speech rights and protects the rights of private entities like social media sites to curate content on their sites and impose content rules on their users.</p>
<p>Among the state actor theories presented is one based on collaboration with the government on content moderation. “<a href="https://en.wikipedia.org/wiki/Jawboning">Jawboning</a>”—or when government authorities influence companies’ social media policies—is extremely common. At what point, if any, does a private company become a state actor when they act according to it?</p>
<p>Deleting posts or cancelling accounts because a government official or agency requested or required it—just like <a href="https://www.eff.org/cases/hepting">spying on people’s communications on behalf of the government</a>—raises serious human rights concerns. The newly revised <a href="https://santaclaraprinciples.org/">Santa Clara Principles</a>, which outline standards that tech platforms must consider to make sure they provide adequate transparency and accountability, specifically scrutinize “State Involvement in Content Moderation.” As set forth in the Principles: “Companies should recognise the particular risks to users’ rights that result from state involvement in content moderation processes. This includes a state’s involvement in the development and enforcement of the company’s rules and policies, either to comply with local law or serve other state interests. Special concerns are raised by demands and requests from state actors (including government bodies, regulatory authorities, law enforcement agencies and courts) for the removal of content or the suspension of accounts.”</p>
<p>So, it is important that there be a defined, though narrow, avenue for holding social media companies liable for certain censorial collaborations with the government. But the bar for holding platforms accountable for such conduct must be high to preserve their First Amendment rights to edit and curate their sites.</p>
<h3><strong>Testing Whether a Jawboned Platform is a State Actor</strong></h3>
<p>We propose the following test. At a minimum: (1) the government must replace the intermediary’s editorial policy with its own, (2) the intermediary must willingly cede the editorial implementation of that policy to the government regarding the specific user speech, and (3) the censored party lacks an adequate remedy against the government. These findings are necessary, but not per se sufficient to establish the social media service as a state actor; there may always be <a href="https://www.courtlistener.com/opinion/118403/brentwood-academy-v-tennessee-secondary-school-athletic-assn/?q=%20Brentwood%20Acad.%20v.%20Tenn.%20Secondary%20Sch.%20Athletic%20Ass%E2%80%99n%2C%20531%20U.S.%20288%2C%20295%E2%80%9396%20(2001).&amp;type=o&amp;order_by=score%20desc&amp;stat_Precedential=on">“some countervailing reason against attributing activity to the government.”</a></p>
<p>In creating the test, we had two guiding principles.</p>
<p>First, when the government coerces or otherwise pressures private publishers to censor, the censored party’s first and favored recourse is against the government. Governmental manipulation of the already fraught content moderation systems to control public dialogue and silence disfavored voices raises classic First Amendment concerns, and both platforms and users should be able to sue the government for this. In First Amendment cases, there is a low threshold for suits <em>against government</em> <em>agencies and officials</em> that coerce private censorship: the government may violate speakers’ First Amendment rights with <a href="https://www.courtlistener.com/opinion/106530/bantam-books-inc-v-sullivan/?q=bantam%20books%20v%20sullivan">“system[s] of informal censorship”</a> aimed at speech intermediaries. In 2015, for example, EFF supported <a href="https://www.eff.org/nb/deeplinks/2015/11/eff-challenges-informal-government-censorship">a lawsuit</a> by Backpage.com after the Cook County sheriff pressured credit card processors to stop processing payments to the website.</p>
<p>Second, social media companies should retain their First Amendment rights to edit and curate the user posts on their sites as long as they are the ones controlling the editorial process. So, we sought to distinguish those situations where the platforms clearly abandoned editorial power and ceded editorial control to the government from those in which the government‘s desires were influential but not determinative.</p>
<p>We proposed this test <a href="https://www.eff.org/document/doe-v-google-ninth-circuit-court-appeals-amicus">in an amicus brief</a> recently filed in the Ninth Circuit in a case in which YouTube has been accused of deleting QAnon videos at the request and compulsion of individual Members of Congress. We argued in that brief that the test was not met in that case and that YouTube could not be liable as a state actor under the facts alleged.</p>
<p>However, even though they are not legally liable, social media companies should voluntarily disclose to a user when a government has demanded or requested action on their post, or whether the platform’s action was required by law. Platforms should also report all government demands for content moderation, and any government involvement in formulating or enforcing editorial policies, or flagging posts. Each of these recommendations is set out in the revised <a href="https://santaclaraprinciples.org/">Santa Clara Principles</a>.</p>
<p>The Santa Clara Principles also calls on governments to limit their involvement in content moderation. The Principle for Governments and Other State Actors states that governments “must not exploit or manipulate companies’ content moderation systems to censor dissenters, political opponents, social movements, or any person.” The Santa Clara Principles go on to urge governments to disclose their involvement in content moderation and to remove any obstacles they have placed on the companies to do so, such as gag orders.</p>
<p>Our position with respect to state action being established by government collaboration stands in contrast to the more absolute positions we have taken against other state action theories.</p>
<p>Although we have been <a href="https://www.eff.org/deeplinks/2019/04/content-moderation-broken-let-us-count-ways">sharp</a> <a href="https://www.eff.org/deeplinks/2018/10/blunt-policies-and-secretive-enforcement-mechanisms-lgbtq-and-sexual-health">critics </a><a href="https://www.eff.org/deeplinks/2017/12/adult-content-policies-textbook-private-censorship-fail">of </a><a href="https://www.eff.org/deeplinks/2022/01/new-tracking-global-online-censorship-site-explains-content-moderation-practices">how</a> <a href="https://www.eff.org/deeplinks/2021/04/content-moderation-losing-battle-infrastructure-companies-should-refuse-join-fight">the</a> <a href="https://www.eff.org/deeplinks/2020/12/how-covid-changed-content-moderation-year-review-2019">large</a> <a href="https://www.eff.org/press/releases/eff-human-rights-watch-and-over-70-civil-society-groups-ask-mark-zuckerberg-provide">social</a> <a href="https://www.eff.org/fa/deeplinks/2022/06/eff-warns-another-court-about-dangers-broad-site-blocking-orders">media </a><a href="https://www.brookings.edu/techstream/how-to-put-covid-19-content-moderation-into-context/">companies </a>curate user speech and its <a href="https://onlinecensorship.org/content/infographics">differential impacts</a> on <a href="https://www.washingtonpost.com/opinions/beware-the-digital-censor/2018/08/12/997e28ea-9cd0-11e8-843b-36e177f3081c_story.html">those traditionally denied voice</a>, we are also concerned that holding social media companies to the legal standards of the First Amendment would hinder their ability to moderate content in ways that serves users well: by removing or downranking posts that although legally protected were harassing or abusive to other users, or which were just offensive to many users the company sought to reach; or by adopting policies or community standards that focus on certain subject matters or communities, and excluding off-topic posts. Many social media companies offer curation services that suggest or prioritize certain posts over others, whether through Facebook’s Top Stories feed, or Twitter’s Home feed, etc., that some users seem to like. Plus, there are numerous practical problems. First, clear distinctions between legal and illegal speech are often elusive. Law enforcement often gets these wrong and judges and juries struggle with these distinctions. Second, it just doesn’t reflect reality: every social media service has an editorial policy that excludes or at least disfavors certain legal speech, and always have had such policies.</p>
<p>We filed our <a href="https://www.eff.org/document/prager-university-v-google-eff-amicus-brief">first amicus brief</a> setting out this position in 2018 and wrote about it <a href="https://www.eff.org/deeplinks/2018/11/eff-court-remedy-bad-content-moderation-isnt-give-government-more-power-control">here</a>. And we’ve been asserting that position in <a href="https://www.eff.org/deeplinks/2022/05/eff-court-california-law-does-not-bar-content-moderation-social-media">various</a> <a href="https://www.eff.org/document/prager-u-v-google-amicus-california-sixth-district-court-appeals">US</a> legal matters ever since. That first case and others like it argued incorrectly that social media companies functioned like public forums, places open to the public to associate and speak to each other, and thus should be treated like government controlled public forums like parks and sidewalks.</p>
<p>Other cases and the social media laws passed by Florida and Texas also argued that social media services, at least the very large ones, were “common carriers” which are open to all users on equal terms. In those cases, <a href="https://www.eff.org/press/releases/eff-supreme-court-put-texas-social-media-law-back-hold">our</a> <a href="https://www.eff.org/document/eff-amicus-brief-netchoice-v-florida">reasoning</a> <a href="https://www.eff.org/deeplinks/2021/10/eff-federal-court-block-unconstitutional-texas-social-media-law">for</a> <a href="https://www.eff.org/ja/document/netchoice-v-paxton-5th-cir-amicus-brief-eff-national-coalition-against-censorship-and-0">challenging </a><a href="https://www.eff.org/document/netchoice-v-paxton-eff-amicus-brief-oct-8-2021">the</a> laws remained the same: users are best served when social media companies are shielded from governmental interference with their editorial policies and decisions.</p>
<p>This policy-based position was consistent with what we saw as the correct legal argument: that social media companies themselves have the First Amendment right to adopt editorial policies, and to curate and edit the user speech that get submitted to them. And it’s important to defend that First Amendment right so as to shield these services from becoming compelled mouthpieces or censors of the government: if they didn’t have their own First Amendment rights to edit and curate their sites as they saw fit, then governments could tell them how to edit and curate their sites <a href="https://www.eff.org/deeplinks/2018/09/whats-doj-really-seeking-accountability-content-moderation-or-censoring-speech-it">according to the government’s wishes and desires</a>.</p>
<p>We stand by our position that social media platforms have the right to moderate content, and believe that allowing the government to dictate what speech platforms can and can’t publish is anathema to our democracy. But when censorship is a collaboration between private companies and the government, there should be a narrow, limited path to hold them accountable.</p>
