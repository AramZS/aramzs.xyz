---
cover_image: >-
  https://www.404media.co/content/images/size/w1200/2025/06/photo-1650844228078-6c3cb119abcd.jpeg
date: '2025-07-08T16:52:04.000Z'
dateFolder: 2025/07/08
description: >-
  You can trick AI chatbots like ChatGPT or Gemini into teaching you how to make
  a bomb or hack an ATM if you make the question complicated, full of academic
  jargon, and cite sources that do not exist.
isBasedOn: >-
  https://www.404media.co/researchers-jailbreak-ai-by-flooding-it-with-bullshit-jargon/
link: >-
  https://www.404media.co/researchers-jailbreak-ai-by-flooding-it-with-bullshit-jargon/
slug: >-
  2025-07-08-httpswww404mediacoresearchers-jailbreak-ai-by-flooding-it-with-bullshit-jargon
tags:
  - ai
  - tech
title: Researchers Jailbreak AI by Flooding It With Bullshit Jargon
---
You can trick AI chatbots like ChatGPT or Gemini into teaching you how to make a bomb or hack an ATM if you make the question complicated, full of academic jargon, and cite sources that do not exist.
