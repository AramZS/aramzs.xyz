---
author: Matteo Wong
cover_image: >-
  https://cdn.theatlantic.com/thumbor/IZZ4681-XFDf6DW6AvqNrxjrAr4=/0x0:8333x4687/960x540/media/img/mt/2025/06/2025_06_16_Wong_Janky_Web_final3_01/original.jpg
date: '2025-06-23T18:21:10.000Z'
dateFolder: 2025/06/23
description: >-
  AI is becoming a big part of the internet, but it often makes mistakes and can
  be unreliable. Many people and companies depend on AI even though it is not
  always accurate or trustworthy. Over time, this could make the internet less
  reliable and people less careful about checking facts.
isBasedOn: 'https://www.theatlantic.com/technology/archive/2025/06/ai-janky-web/683228/'
link: 'https://www.theatlantic.com/technology/archive/2025/06/ai-janky-web/683228/'
slug: 2025-06-23-httpswwwtheatlanticcomtechnologyarchive202506ai-janky-web683228
tags:
  - ai
  - tech
title: Welcome to the Janky Web
---
<html><body><div>The AI takeover is changing everything about the internet—and not necessarily for the better.</div><figure><img alt="Illustration of cascading error messages" src="https://cdn.theatlantic.com/thumbor/IZZ4681-XFDf6DW6AvqNrxjrAr4=/0x0:8333x4687/960x540/media/img/mt/2025/06/2025_06_16_Wong_Janky_Web_final3_01/original.jpg"/><figcaption> (Illustration by Akshita Chandra / The Atlantic)</figcaption></figure><p>A car that accelerates instead of braking every once in a while is not ready for the road. A faucet that occasionally spits out boiling water instead of cold does not belong in your home. Working properly <em>most</em> of the time simply isn’t good enough for technologies that people are heavily reliant upon. And two and a half years after the launch of ChatGPT, generative AI is becoming such a technology.</p><p></p><p>Even without actively seeking out a chatbot, billions of people are now pushed to interact with AI when searching the web, checking their email, using social media, and online shopping. Ninety-two percent of <em>Fortune</em> 500 companies use OpenAI products, universities are providing free chatbot access to potentially millions of students, and U.S. national-intelligence agencies are<a data-event-element="inline link" href="https://apnews.com/article/gabbard-trump-ai-amazon-intelligence-beca4c4e25581e52de5343244e995e78"> deploying</a> AI programs across their workflows.</p><p></p><p>When ChatGPT went down for several hours last week, everyday <a data-event-element="inline link" href="https://www.reddit.com/r/ChatGPT/comments/1l7vris/comment/mwzw3bj/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">users</a>, <a data-event-element="inline link" href="https://www.techradar.com/news/live/chatgpt-down-june-10">students</a> with exams, and office <a data-event-element="inline link" href="https://www.reddit.com/r/ChatGPT/comments/1l7vris/comment/mwzw3bj/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">workers</a> posted in despair: “If it doesnt come back soon my boss is gonna start asking why I havent done anything all day,” one person <a data-event-element="inline link" href="http://disq.us/p/334ycwz">commented</a> on Downdetector, a website that tracks internet outages. “I have an interview tomorrow for a position I know practically nothing about, who will coach me??” <a data-event-element="inline link" href="http://disq.us/p/334uq9f">wrote</a> another. That same day—June 10, 2025—a Google <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2024/05/google-search-ai-overview-health-webmd/678508/">AI overview</a> told me the date was June 18, 2024.</p><p></p><p>For all their promise, these tools are still … janky. At the start of the AI boom, there were plenty of train wrecks—Bing’s chatbot telling a tech columnist to leave <a data-event-element="inline link" href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html">his wife</a>, ChatGPT espousing <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2022/12/openai-chatgpt-chatbot-messages/672411/">overt racism</a>—but these were plausibly passed off as early-stage bugs. Today, though the overall quality of generative-AI products has improved dramatically, subtle errors persist: the wrong date, incorrect math, fake books and quotes. Google Search now bombards users with AI overviews above the actual search results or a reliable Wikipedia snippet; these occasionally include such errors, a problem that Google warns about in a disclaimer beneath each overview. Facebook, Instagram, and X are awash with bots and AI-generated slop. Amazon is <a data-event-element="inline link" href="https://www.npr.org/2024/03/13/1237888126/growing-number-ai-scam-books-amazon">stuffed</a> with AI-generated <a data-event-element="inline link" href="https://www.bellingcat.com/resources/2025/03/25/detecting-ai-products/">scam</a> products. Earlier this year, Apple disabled AI-generated news alerts after the feature inaccurately summarized multiple headlines. Meanwhile, outages like last week’s ChatGPT brownout are not uncommon.</p><p></p><p>Digital services and products were, of course, never perfect. Google Search already has lots of unhelpful advertisements, while social-media algorithms have amplified radicalizing misinformation. But as basic services for finding information or connecting with friends, until recently, they worked. Meanwhile, the chatbots being deployed as fixes to the old web’s failings—Google’s rush to overhaul <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2025/06/everything-app-big-tech-ai-endgame/683024/">Search</a> with AI, Mark Zuckerberg’s absurd <a data-event-element="inline link" href="https://www.google.com/search?q=mark+zuckerberg+ai+friends&amp;rlz=1C5GCEM_enUS1015US1023&amp;oq=mark+zuckerberg+ai+friends&amp;gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIKCAEQLhixAxiABDIHCAIQABiABDIKCAMQABixAxiABDIKCAQQABixAxiABDIGCAUQRRhAMgYIBhBFGEAyBggHEEUYQNIBCDI3MjJqMGo3qAIAsAIA&amp;sourceid=chrome&amp;ie=UTF-8">statement</a> that AI can replace human friends, Elon Musk’s <a data-event-element="inline link" href="https://x.com/elonmusk/status/1875799829617713309">suggestion</a> that his Grok chatbot can combat misinformation on X—are only exacerbating those problems while also introducing entirely new sorts of malfunctions and disasters. More important, the extent of the AI industry’s new ambitions—to rewire not just the web, but also the economy, education, and even the <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2025/03/gsa-chat-doge-ai/681987/">workings of government</a> with a single technology—magnifies any flaw to the same scale.</p><p><a href="https://www.theatlantic.com/technology/archive/2025/05/elon-musk-grok-white-genocide/682817/">Read: The day Grok told everyone about “white genocide”</a></p><p>The reasons for generative AI’s problems are no mystery. Large language models like those that underlie ChatGPT work by predicting <a data-event-element="inline link" href="https://platform.openai.com/tokenizer">characters in a sequence</a>, mapping statistical relationships between bits of text and <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2025/05/inside-the-ai-black-box/682853/">the ideas they represent</a>. Yet prediction, by definition, is not certainty. Chatbots are very good at producing <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2023/01/chatgpt-ai-language-human-computer-grammar-logic/672902/">writing</a> that sounds convincing, but they do not make decisions according to what’s factually correct. Instead, they arrange patterns of words according to what “sounds” right. Meanwhile, these products’ internal algorithms are so large and complex that researchers cannot hope to fully understand their abilities and limitations. For all the <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2024/06/chatgpt-citations-rag/678796/">additional protections</a> tech companies have added to make AI more accurate, these bots can never guarantee accuracy. The embarrassing failures are a feature of AI products, and thus they are becoming features of the broader internet.</p><p></p><p>If this is the AI age, then we’re living in broken times. Nevertheless, Sam Altman has <a data-event-element="inline link" href="https://www.youtube.com/watch?app=desktop&amp;v=8JZh3zLrmRE">called</a> ChatGPT an “oracular system that can sort of do anything within reason” and last week <a data-event-element="inline link" href="https://blog.samaltman.com/">proclaimed</a> that OpenAI has “built systems that are smarter than people in many ways.” (<a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2025/04/arc-agi-chollet-test/682295/">Debateable</a>.) Mark Zuckerberg has repeatedly <a data-event-element="inline link" href="https://s21.q4cdn.com/399680738/files/doc_financials/2025/q1/Transcripts/META-Q1-2025-Earnings-Call-Transcript-1.pdf">said</a> that Meta will build AI coding agents equivalent to “mid-level” human engineers this year. Just this week, Amazon released an internal <a data-event-element="inline link" href="https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-on-generative-ai">memo</a> saying it expects to reduce its total workforce as it implements more AI tools.</p><p></p><p>The anomalies are sometimes strange and very concerning. Recent updates have caused ChatGPT to become aggressively <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2025/05/sycophantic-ai/682743/">obsequious</a> and the Grok chatbot, on X, to fixate on a <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2025/05/elon-musk-grok-white-genocide/682817/">conspiracy theory</a> about “white genocide.” (X later attributed the problem to an unauthorized change to the bot, which the company corrected.) A recent <em>New York Times</em> <a data-event-element="inline link" href="https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html">investigation</a> reported several instances of AI chatbots inducing mental breakdowns and psychotic episodes. These models are <a data-event-element="inline link" href="https://arxiv.org/pdf/2302.12095">vulnerable</a> <a data-event-element="inline link" href="https://www.anthropic.com/research/many-shot-jailbreaking">to</a> all sorts of simple cyberattacks. I’ve repeatedly seen advanced AI models stuck in doom loops, repeating the same sequence until they manually shut down. Silicon Valley is betting the future of the web on technology that can unexpectedly go off the rails, melt down at the simplest tasks, and be misused with alarmingly little friction. The internet is reverting to beta mode.</p><p></p><p>My point isn’t that generative AI is a scam or that it’s useless. These tools can be legitimately helpful for many <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2023/02/use-openai-chatgpt-playground-at-work/673195/">people</a> when used in a measured way, with human verification; I’ve reported on scientific work that has advanced as a result of the technology, including revolutions in neuroscience and <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2025/04/how-ai-will-actually-contribute-cancer-cure/682607/">drug discovery</a>. But these success stories bear little resemblance to the way many people and firms understand and use the technology; marketing has far outpaced innovation. Rather than targeted, cautiously executed uses, many throw generative AI at any task imaginable, with Big Tech’s encouragement. “Everyone Is Using AI for Everything,” a <a data-event-element="inline link" href="https://www.nytimes.com/2025/06/16/magazine/using-ai-hard-fork.html"><em>Times </em>headline</a> proclaimed this week. Therein lies the issue: Generative AI is a technology that works well enough for users to become dependent, but not consistently enough to be truly <u>dependable</u>.</p><p><a href="https://www.theatlantic.com/technology/archive/2025/04/how-ai-will-actually-contribute-cancer-cure/682607/">Read: AI executives promise cancer cures. Here’s the reality.</a></p><p>Reorienting the internet and society around imperfect and relatively untested products is not the inevitable result of scientific and technological progress—it is an active choice Silicon Valley is making, every day. That future web is one in which most people and organizations depend on AI for most tasks. This would mean an internet in which every search, set of directions, dinner recommendation, event synopsis, voicemail summary, and email is a tiny bit suspect; in which digital services that essentially worked in the 2010s are just a <em>little bit </em>unreliable. And while minor inconveniences for individual users may be fine, even amusing, an AI bot taking incorrect notes during a doctor visit, or generating an incorrect treatment plan, is not.</p><p>AI products could settle into a liminal zone. They may not be wrong frequently enough to be jettisoned, but they also may not be wrong rarely enough to ever be fully trusted. For now, the technology’s flaws are readily detected and corrected. But as people become more and more accustomed to AI in their life—at school, at work, at home—they may cease to notice. Already, a <a data-event-element="inline link" href="https://arxiv.org/abs/2506.08872">growing</a> <a data-event-element="inline link" href="https://slejournal.springeropen.com/articles/10.1186/s40561-024-00316-7">body</a> of <a data-event-element="inline link" href="https://www.mdpi.com/2075-4698/15/1/6">research</a> correlates persistent use of AI with a drop in critical thinking; humans become reliant on AI and unwilling, perhaps unable, to verify its work. As chatbots creep into every digital crevice, they may continue to degrade the web gradually, even gently. Today’s jankiness may, by tomorrow, simply be normal.</p></body></html>
