---
author: >-
  Aristeidis Sidiropoulos, Christos Chrysanthos Nikolaidis, Theodoros Tsiolakis,
  Nikolaos Pavlidis, Vasilis Perifanis, Pavlos S. Efraimidis
cover_image: 'https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png'
date: '2025-09-08T17:20:53.405Z'
dateFolder: 2025/09/08
description: >-
  Membership Inference Attacks (MIAs) pose a significant privacy risk, as they
  enable adversaries to determine whether a specific data point was included in
  the training dataset of a model. While Machine Unlearning is primarily
  designed as a privacy mechanism to efficiently remove private data from a
  machine learning model without the need for full retraining, its impact on the
  susceptibility of models to MIA remains an open question. In this study, we
  systematically assess the vulnerability of models to MIA after applying
  state-of-art Machine Unlearning algorithms. Our analysis spans four diverse
  datasets (two from the image domain and two in tabular format), exploring how
  different unlearning approaches influence the exposure of models to membership
  inference. The findings highlight that while Machine Unlearning is not
  inherently a countermeasure against MIA, the unlearning algorithm and data
  characteristics can significantly affect a model's vulnerability. This work
  provides essential insights into the interplay between Machine Unlearning and
  MIAs, offering guidance for the design of privacy-preserving machine learning
  systems.
isBasedOn: 'https://arxiv.org/abs/2508.16150'
link: 'https://arxiv.org/abs/2508.16150'
slug: 2025-09-08-httpsarxivorgabs250816150
tags:
  - ai
title: >-
  Evaluating the Defense Potential of Machine Unlearning against Membership
  Inference Attacks
---
<p>[Submitted on 22 Aug 2025]</p>
<p>Authors:<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sidiropoulos,+A">Aristeidis Sidiropoulos</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nikolaidis,+C+C">Christos Chrysanthos Nikolaidis</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tsiolakis,+T">Theodoros Tsiolakis</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pavlidis,+N">Nikolaos Pavlidis</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Perifanis,+V">Vasilis Perifanis</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Efraimidis,+P+S">Pavlos S. Efraimidis</a></p>
<p>View a PDF of the paper titled Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks, by Aristeidis Sidiropoulos and 5 other authors</p>
<p><a href="https://arxiv.org/pdf/2508.16150">View PDF</a> <a href="https://arxiv.org/html/2508.16150v1">HTML (experimental)</a></p>
<blockquote> Abstract:Membership Inference Attacks (MIAs) pose a significant privacy risk, as they enable adversaries to determine whether a specific data point was included in the training dataset of a model. While Machine Unlearning is primarily designed as a privacy mechanism to efficiently remove private data from a machine learning model without the need for full retraining, its impact on the susceptibility of models to MIA remains an open question. In this study, we systematically assess the vulnerability of models to MIA after applying state-of-art Machine Unlearning algorithms. Our analysis spans four diverse datasets (two from the image domain and two in tabular format), exploring how different unlearning approaches influence the exposure of models to membership inference. The findings highlight that while Machine Unlearning is not inherently a countermeasure against MIA, the unlearning algorithm and data characteristics can significantly affect a model's vulnerability. This work provides essential insights into the interplay between Machine Unlearning and MIAs, offering guidance for the design of privacy-preserving machine learning systems. </blockquote>
<table><tbody><tr> <td>Subjects:</td> <td> Cryptography and Security (cs.CR)</td> </tr><tr> <td>Cite as:</td> <td><a href="https://arxiv.org/abs/2508.16150">arXiv:2508.16150</a> [cs.CR]</td> </tr> <tr> <td></td> <td>(or  <a href="https://arxiv.org/abs/2508.16150v1">arXiv:2508.16150v1</a> [cs.CR] for this version) </td> </tr> <tr> <td></td> <td> <a href="https://doi.org/10.48550/arXiv.2508.16150">https://doi.org/10.48550/arXiv.2508.16150</a>    arXiv-issued DOI via DataCite (pending registration)<br/> </td> </tr></tbody></table>
