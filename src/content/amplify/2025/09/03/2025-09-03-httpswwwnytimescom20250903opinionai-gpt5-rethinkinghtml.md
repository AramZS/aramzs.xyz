---
author: Gary Marcus
cover_image: >-
  https://static01.nyt.com/images/2025/09/03/multimedia/03marcus-hlmk/03marcus-hlmk-facebookJumbo.jpg
date: '2025-09-03T12:33:27.820Z'
dateFolder: 2025/09/03
description: Building bigger A.I. isn’t leading to better A.I.
isBasedOn: 'https://www.nytimes.com/2025/09/03/opinion/ai-gpt5-rethinking.html'
link: 'https://www.nytimes.com/2025/09/03/opinion/ai-gpt5-rethinking.html'
slug: 2025-09-03-httpswwwnytimescom20250903opinionai-gpt5-rethinkinghtml
tags:
  - ai
title: How to Rethink A.I.
---
<p>Guest Essay</p>
<figure><picture><source media="(max-width: 599px) and (min-device-pixel-ratio: 3),(max-width: 599px) and (min-resolution: 3dppx),(max-width: 599px) and (min-resolution: 288dpi)" srcset="https://static01.nyt.com/images/2025/09/03/multimedia/03marcus-hlmk/03marcus-hlmk-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1800"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 2),(max-width: 599px) and (min-resolution: 2dppx),(max-width: 599px) and (min-resolution: 192dpi)" srcset="https://static01.nyt.com/images/2025/09/03/multimedia/03marcus-hlmk/03marcus-hlmk-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=1200"/><source media="(max-width: 599px) and (min-device-pixel-ratio: 1),(max-width: 599px) and (min-resolution: 1dppx),(max-width: 599px) and (min-resolution: 96dpi)" srcset="https://static01.nyt.com/images/2025/09/03/multimedia/03marcus-hlmk/03marcus-hlmk-mobileMasterAt3x.jpg?quality=75&amp;auto=webp&amp;disable=upscale&amp;width=600"/><img alt="A photo illustration of different images layered on top of one another to create a vortex-like collage." sizes="((min-width: 600px) and (max-width: 1004px)) 84vw, (min-width: 1005px) 80vw, 100vw" src="https://static01.nyt.com/images/2025/09/03/multimedia/03marcus-hlmk/03marcus-hlmk-articleLarge.jpg?quality=75&amp;auto=webp&amp;disable=upscale" srcset="https://static01.nyt.com/images/2025/09/03/multimedia/03marcus-hlmk/03marcus-hlmk-articleLarge.jpg?quality=75&amp;auto=webp 600w,https://static01.nyt.com/images/2025/09/03/multimedia/03marcus-hlmk/03marcus-hlmk-jumbo.jpg?quality=75&amp;auto=webp 683w,https://static01.nyt.com/images/2025/09/03/multimedia/03marcus-hlmk/03marcus-hlmk-superJumbo.jpg?quality=75&amp;auto=webp 1366w"/></picture><figcaption>A photo illustration of different images layered on top of one another to create a vortex-like collage.</figcaption></figure>
<figure><audio src="https://static.nytimes.com/narrated-articles/synthetic/article-97e9b2fc-0611-568c-9861-637e97268eb9/job-1756890160606/article-97e9b2fc-0611-568c-9861-637e97268eb9-job-1756890160606.mp3"></audio></figure>
<p><i><p><i>By </i>Gary Marcus</p><blockquote><p>Mr. Marcus is a founder of two A.I. companies and the author of six books on natural and artificial intelligence.</p></blockquote></i></p>
<p>GPT-5, OpenAI’s latest artificial intelligence system, was supposed to be a game-changer, the culmination of billions of dollars of investment and nearly three years of work. Sam Altman, the company’s chief executive, implied that GPT-5 could be tantamount to artificial general intelligence, or A.G.I. — A.I. that is as smart and as flexible as any human expert.</p>
<p>Instead, as I have written, the model fell short. Within hours of its release, critics found all kinds of baffling errors: It failed some simple math questions, <a href="https://x.com/salmannaseer/status/1953840879137071429?s=61">couldn’t count</a> reliably and sometimes provided absurd answers <a href="https://x.com/wasgo/status/1954375973044101175?s=61">to old riddles</a>. Like its predecessors, the A.I. model still <a href="https://garymarcus.substack.com/p/why-do-large-language-models-hallucinate?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web&amp;triedRedirect=true">hallucinates</a> (though at a lower rate) and is <a href="https://x.com/maithra_raghu/status/1954614752426270867?s=61">plagued by questions around its reliability</a>. Although <a href="https://www.nytimes.com/2025/08/24/opinion/chat-gpt5-open-ai-future.html">some people have been impressed</a>, few saw it as a quantum leap, and nobody believed it was A.G.I. Many users asked for the old model back.</p>
<p>GPT-5 is a step forward, but nowhere near the A.I. revolution many had expected. That is bad news for the companies and investors who placed substantial bets on the technology. And it demands a rethink of government policies and investments that were built on wildly overinflated expectations. The current strategy of merely making A.I. bigger is deeply flawed — scientifically, economically and politically. Many things from regulation to research strategy must be rethought. One of the keys to this may be training and developing A.I. in ways inspired by the cognitive sciences.</p>
<p>Fundamentally, people like <a href="https://www.reddit.com/r/singularity/comments/1h7hmvf/sam_altman_says_there_is_no_scaling_wall_in_ai/">Mr. Altman</a>, the <a href="https://pod.wave.co/podcast/big-technology-podcast/anthropic-ceo-dario-amodei-ais-potential-openai-rivalry-genai-business-doomerism-451c32cc#:~:text=The%20idea%20that%20every%20few%20months%20we%20get%20an%20AI%20model%20that%20is%20better%20than%20the%20AI%20model%20we%20got%20before%2C%20and%20that%20we%20get%20that%20by%20investing%20more%20compute%20in%20AI%20models%2C%20more%20data%2C%20more%20new%20types%20of%20training%20models.">Anthropic chief executive Dario Amodei</a> and countless other tech leaders and investors had put far too much faith into a speculative and unproven hypothesis called scaling: the idea that training A.I. models on ever more data using ever more hardware would eventually lead to A.G.I., or even a “<a href="https://blog.samaltman.com">superintelligence</a>” that surpasses humans.</p>
<p>However, as I warned in a <a href="https://nautil.us/deep-learning-is-hitting-a-wall-238440/">2022 essay</a> titled “Deep Learning Is Hitting a Wall,” so-called scaling laws aren’t physical laws of the universe like gravity, but hypotheses based on historical trends. Large language models, which power systems like GPT-5, are nothing more than souped-up statistical regurgitation machines, so they will continue to stumble into problems around truth, hallucinations and reasoning. Scaling would not bring us to the holy grail of A.G.I.</p>
<p>Many in the tech industry were hostile to my predictions. Mr. Altman ridiculed me as a “<a href="https://x.com/sama/status/1512471289545383940?s=61">mediocre deep learning skeptic</a>” and last year claimed “<a href="https://www.msn.com/en-us/money/markets/sam-altman-says-there-is-no-wall-in-an-apparent-response-to-fears-of-an-ai-slowdown/ar-AA1u5AMY#:~:text=FinanceBuzz-,His%20comment%2C%20posted%20on%20X%2C%20follows%20a%20report%20in%20The,members%20of%20the%20technical%20staff.%22&amp;text=SuperSavingsOnline-,Ilya%20Sutskever%2C%20a%20cofounder%20of%20OpenAI%20and%20Safe%20Superintelligence%2C%20recently,Business%20Insider%20on%20Microsoft%20Start.">there is no wall</a>.” Elon Musk <a href="https://x.com/elonmusk/status/1796795162028335377?s=61">shared a meme</a> lampooning my essay.</p>
<p>It now seems I was right. Adding more data to large language models, which are trained to produce text by learning from vast databases of human text, helps them improve only to a degree. Even significantly scaled, they still don’t fully understand the concepts they are exposed to — which is why they sometimes botch answers or generate ridiculously incorrect drawings.</p>
<p>Scaling worked for a while — previous generations of GPT models made impressive advancements to their predecessors. But luck started to run out over the last year. Mr. Musk’s A.I. system, Grok 4, released in July, had 100 times as much training as Grok 2 had but it was only moderately better. Meta’s jumbo-size Llama 4 model, much larger than its predecessor, was mostly also viewed as a failure. As many now see, GPT-5 shows decisively that scaling has lost steam.</p>
<p>The chances of A.G.I.’s arrival by 2027 now seem remote. The government has let A.I. companies lead a charmed life with almost zero regulation. It now ought to enact legislation that addresses costs and harms unfairly offloaded onto the public — from <a href="https://www.nytimes.com/interactive/2024/05/19/technology/biased-ai-chatbots.html">misinformation</a> to <a href="https://www.nytimes.com/2025/08/20/opinion/amy-klobuchar-deepfakes.html">deepfakes</a>, “<a href="https://www.nytimes.com/2024/06/11/style/ai-search-slop.html">A.I. slop</a>” content, <a href="https://www.nytimes.com/2025/07/21/briefing/ai-vs-ai.html">cybercrime</a>, <a href="https://www.nytimes.com/2023/12/30/business/media/copyright-law-ai-media.html">copyright infringement</a>, <a href="https://www.nytimes.com/2025/08/26/technology/chatgpt-openai-suicide.html">mental health</a> and <a href="https://www.nytimes.com/2024/07/11/climate/artificial-intelligence-energy-usage.html">energy usage</a>.</p>
<p>Moreover, governments and investors should strongly support research investments outside of scaling. The cognitive sciences (including psychology, child development, philosophy of mind and linguistics) teach us that intelligence is about more than mere statistical mimicry and suggest three promising ideas for developing A.I. that is reliable enough to be trustworthy, with a much richer intelligence.</p>
<p>First, humans are constantly building and maintaining internal models of the world — or world models — of the people and objects around them, and how things work. For example, when you read a novel, you develop a kind of mental database for who each individual character is and what he or she represents. This might include characters’ occupations, their relationships to one another, what their motivations and goals are and so on. In a fantasy or science fiction novel, a world model might even include new physical laws.</p>
<p>Many of generative A.I.’s shortcomings <a href="https://garymarcus.substack.com/p/generative-ais-crippling-and-widespread?r=8tdk6">can be traced back to failures to extract proper world models</a> from their training data. This explains why the latest large language models, for example, are unable to fully grasp how chess works. As a result, they have a tendency to make illegal moves, no matter how many games they’ve been trained on. We don’t just need systems that mimic human language; we also need systems that understand the world so that they can reason about it in a deeper way. Focusing on how to build a new generation of A.I. systems centered around world models should be a central focus of future research. <a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/">Google DeepMind</a> and Fei-Fei Li’s <a href="https://www.worldlabs.ai">World Labs</a> are taking steps in this direction.</p>
<p>Second, the field of machine learning (which has powered large language models) likes to task A.I. systems to learn absolutely everything from scratch by scraping data from the internet, with nothing built in. But as cognitive scientists like Steven Pinker, Elizabeth Spelke and me have emphasized, the human mind is <a href="https://pubmed.ncbi.nlm.nih.gov/37248696/">born with some core knowledge</a> of the world that sets us up to grasp more complex concepts. Building in basic concepts like time, space and causality might allow systems to better organize the data they encounter into richer starting points — potentially leading to richer outcomes. (Verses AI’s work <a href="https://www.verses.ai/research-blog/axiom-mastering-arcade-games-in-minutes-with-active-inference-and-structure-learning">on physical and perceptual understanding in video games</a> is one step in this direction.)</p>
<p>Finally, the current paradigm takes a kind of one-size-fits-all approach by relying on a single cognitive mechanism — the large language model — to solve everything. But we know the human mind uses many different tools for many different kinds of problems. For example, the renowned psychologist Daniel Kahneman suggested humans utilize one system of thought that is quick, reflexive and automatic, driven largely by the statistics of experience but also superficial and prone to blunders; along with a second system that is driven more by abstract reasoning and deliberative thinking that’s slow and laborious. Large language models, which are a bit like the first system, try to do everything with a single statistical approach, but wind up unreliable as a result.</p>
<p>We need a new approach, closer to what Mr. Kahneman described. This may come in the form of “<a href="https://theconversation.com/neurosymbolic-ai-is-the-answer-to-large-language-models-inability-to-stop-hallucinating-257752">neurosymbolic</a>” A.I., which bridges statistically-driven neural networks (from which large language models are drawn) with some older ideas from symbolic A.I. Symbolic A.I. is more abstract and deliberative by nature; it processes information by taking cues from logic, algebra and computer programming. I have <a href="https://mitpress.mit.edu/9780262632683/the-algebraic-mind/">long advocated for a marriage of these two</a> traditions. Increasingly, we are seeing companies like <a href="https://www.wsj.com/articles/meet-neurosymbolic-ai-amazons-method-for-enhancing-neural-networks-620dd81a?st=bf7MQx&amp;reflink=desktopwebshare_permalink">Amazon</a> and <a href="https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/">Google DeepMind</a> take such a hybrid approach (<a href="https://open.substack.com/pub/garymarcus/p/how-o3-and-grok-4-accidentally-vindicated?utm_campaign=post&amp;utm_medium=web">even OpenAI appears to be doing some of this, quietly</a>). By the end of the decade, neurosymbolic A.I. may well eclipse pure scaling.</p>
<p>Large language models have had their uses, especially for coding, writing and brainstorming, in which humans are still directly involved. But no matter how large we have made them, they have never been worthy of our trust. To build A.I. that we can genuinely trust and to have a shot at A.G.I., we must move on from the trappings of scaling. We need new ideas. A return to the cognitive sciences might well be the next logical stage in the journey.</p>
<p>Source images via Getty Images.</p>
<p>Gary Marcus is a professor emeritus at New York University and was a founder and chief executive of Geometric Intelligence. His most recent book is “Taming Silicon Valley.” He publishes a <a href="https://garymarcus.substack.com/">newsletter</a> about A.I.</p>
<p><em>The Times is committed to publishing </em><a href="https://www.nytimes.com/2019/01/31/opinion/letters/letters-to-editor-new-york-times-women.html"><em>a diversity of letters</em></a><em> to the editor. We’d like to hear what you think about this or any of our articles. Here are some </em><a href="https://help.nytimes.com/hc/en-us/articles/115014925288-How-to-submit-a-letter-to-the-editor"><em>tips</em></a><em>. And here’s our email: </em><a href="mailto:letters@nytimes.com"><em>letters@nytimes.com</em></a><em>.</em></p>
<p><em>Follow the New York Times Opinion section on </em><a href="https://www.facebook.com/nytopinion"><em>Facebook</em></a><em>, </em><a href="https://www.instagram.com/nytopinion/"><em>Instagram</em></a><em>, </em><a href="https://www.tiktok.com/@nytopinion"><em>TikTok</em></a><em>, </em><a href="https://bsky.app/profile/nytopinion.nytimes.com"><em>Bluesky</em></a>, <a href="https://www.whatsapp.com/channel/0029VaN8tdZ5vKAGNwXaED0M"><em>WhatsApp</em></a><em> and </em><a href="https://www.threads.net/@nytopinion"><em>Threads</em></a><em>.</em></p>
