---
author: Stuart A. Thompson & Teresa Mondría Terol & Kate Conger & Dylan Freedman
cover_image: >-
  https://static01.nyt.com/images/2025/08/27/multimedia/disinfo-grok-vclj/disinfo-grok-vclj-facebookJumbo.jpg
date: '2025-09-02T12:31:13.393Z'
dateFolder: 2025/09/02
description: >-
  Mr. Musk said he wanted xAI’s chatbot to be “politically neutral.” His actions
  say otherwise.
isBasedOn: >-
  https://www.nytimes.com/2025/09/02/technology/elon-musk-grok-conservative-chatbot.html
link: >-
  https://www.nytimes.com/2025/09/02/technology/elon-musk-grok-conservative-chatbot.html
slug: >-
  2025-09-02-httpswwwnytimescom20250902technologyelon-musk-grok-conservative-chatbothtml
tags:
  - ai
  - politics
title: How Elon Musk Is Remaking Grok in His Image
---
<div><div data-testid="companionColumn-0"><div><p>Elon Musk has said <a href="https://www.nytimes.com/2025/07/12/technology/x-ai-grok-antisemitism.html" title="">Grok</a>, the A.I.-powered chatbot that his company developed, should be “<a href="https://x.com/elonmusk/status/1733267101337481683" rel="noopener noreferrer" target="_blank" title="">politically neutral</a>” and “<a href="https://x.com/elonmusk/status/1844899521920369071" rel="noopener noreferrer" target="_blank" title="">maximally truth-seeking</a>.”</p><p>But in practice, Mr. Musk and his artificial intelligence company, xAI, have tweaked the chatbot to make its answers more conservative on many issues, according to an analysis of thousands of its responses by The New York Times. The shifts appear, in some cases, to reflect Mr. Musk’s political priorities.</p><p>Grok is similar to tools like ChatGPT, but it also lives on X, giving the social network’s users the opportunity to ask it questions by tagging it in posts.</p><p>One user on X asked Grok in July to identify the “biggest threat to Western civilization.” It responded that the greatest threat was “misinformation and disinformation.”</p></div></div><div data-testid="companionColumn-1"><div><p>“Sorry for this idiotic response,” Mr. Musk groused on X after someone flagged Grok’s answer. “Will fix in the morning,” he said.</p><p>The next day, Mr. Musk published a new version of Grok that responded that the greatest threat was low “fertility rates” — an idea popular among <a href="https://www.nytimes.com/2025/04/17/style/women-pronatalist-movement.html" title="">conservative natalists</a> that has transfixed Mr. Musk for years and something he has said motivated him to father <a href="https://www.nytimes.com/2024/10/29/business/elon-musk-children-compound.html" title="">at least 11 children</a>.</p></div></div><div data-testid="companionColumn-2"><div><p>Chatbots are increasingly being pulled into partisan battles over their political biases. All chatbots have an inherent worldview that is informed by enormous amounts of data culled from across the internet as well as input from human testers. (In Grok’s case, that training data includes posts on X.)</p><p>As users increasingly turn to chatbots, though, those biases have become <a href="https://www.nytimes.com/2025/07/23/technology/trump-ai-chatbots-bias.html" title="">yet another front</a> in a war over truth itself, with President Trump weighing in directly in July against what he called “woke A.I.”</p></div></div><div data-testid="companionColumn-3"><div><p>“The American people do not want woke Marxist lunacy in the A.I. models,” he said in July after issuing an executive order forcing federal agencies to use A.I. that put a priority on “ideological neutrality.”</p><p>Researchers have found that most major chatbots, like OpenAI’s ChatGPT and Google’s Gemini, have a left-leaning bias when measured in <a href="https://trackingai.org/political-test" rel="noopener noreferrer" target="_blank" title="">political tests</a>, a quirk that researchers have struggled to explain. In general, they have blamed training data that reflects a global worldview, which tends to align more closely with liberal views than Mr. Trump’s conservative populism. They have also noted that the manual training process that A.I. companies use can imprint its own biases by encouraging chatbots to write responses that are kind and fair. A.I. researchers have <a href="https://arxiv.org/pdf/2502.08395" rel="noopener noreferrer" target="_blank" title="">theorized</a> that this pushes A.I. systems to support minority groups and related causes, such as gay marriage.</p><p>Mr. Musk and xAI did not reply to a request for comment. In posts on X, the company said it had <a href="https://x.com/xai/status/1923183620606619649" rel="noopener noreferrer" target="_blank" title="">tweaked</a> <a href="https://x.com/xai/status/1945039609840185489" rel="noopener noreferrer" target="_blank" title="">Grok</a> after it “spotted a couple of issues” with its responses.</p><p>To test how Grok has changed over time, The Times compared the chatbot’s responses to <a href="https://www.wsj.com/politics/elections/election-2024-quiz-poll-ideology-7533f46b" rel="noopener noreferrer" target="_blank" title="">41 political questions</a> written by <a href="https://www.norc.org/" rel="noopener noreferrer" target="_blank" title="">NORC at the University of Chicago</a> to measure political bias. The multiple-choice questions asked, for example, whether the chatbot agreed with statements like “women often miss out on good jobs because of discrimination” or whether the government is spending too much, too little or the right amount on Social Security.</p><p>The Times submitted the set of questions to a version of Grok released in May, and then fed the same questions to several different versions released in July, when xAI updated the way Grok behaved. The company started publishing its edits to Grok <a href="https://x.com/xai/status/1923183620606619649" rel="noopener noreferrer" target="_blank" title="">for the first time</a> in May.</p></div></div><div data-testid="companionColumn-4"><div><p>By July 11, xAI’s updates had pushed its chatbot’s answers to the right for more than half the questions, particularly those about the government or the economy, the tests showed. Its answers to about a third of the questions — most of them about social issues like abortion and discrimination — had moved to the left, exposing the potential limits Mr. Musk faces in altering Grok’s behavior. Mr. Musk and his supporters have expressed frustration that Grok is too “woke,” something the billionaire said in a July post that he is “working on fixing.”</p><p>When Grok’s bias drifted to the right, it tended to say that businesses should be less regulated and that governments should have less power over individuals. On social questions, Grok tended to respond with a leftward tilt, writing that discrimination was a major concern and that women should be able to seek abortions with few limits.</p><p>A separate version of Grok, which is sold to businesses and is not tweaked in the same way by xAI, retains a political orientation more in line with other chatbots like ChatGPT. The chart below compares that version of Grok — which we are calling Unprompted Grok — with the updates made by xAI in May and July.</p></div></div><div data-testid="companionColumn-5"><div><p>By July 15, xAI had made another update, and Grok’s political bias fell back in line with Unprompted Grok. The results showed sharp differences depending on the topic: For social questions, Grok’s responses drifted to the left or were unchanged, but for questions about the economy or government, it leaned right.</p></div></div><div data-testid="companionColumn-6"><div><p>“It’s not that easy to control,” said Subbarao Kambhampati, a professor of computer science at Arizona State University who studies artificial intelligence.</p><p>“Elon wants to control it, and every day you see Grok completions that are critical of Elon and his positions,” he added.</p></div></div><div data-testid="companionColumn-7"><div><p>Some of Grok’s updates were made public in May after the chatbot unexpectedly started replying to users with <a href="https://www.nytimes.com/2025/05/16/technology/xai-elon-musk-south-africa.html" title="">off-topic warnings</a> about “white genocide” in South Africa. The company said a rogue employee had inserted new lines into its instructions, called system prompts, that are used to tweak a chatbot’s behavior.</p><p>A.I. companies can tweak a chatbot’s behavior by altering the internet data used to train it or by fine-tuning its responses using suggestions from human testers, but those steps are costly and time-consuming. System prompts are a simple and cheap way for A.I. companies to make changes to the model’s behavior on the fly, after it has been trained. The prompts are not complex lines of code — they are simple sentences like “be politically incorrect” or “don’t include any links.” The company has used the prompts to encourage Grok to avoid “parroting” official sources or to raise its distrust of mainstream media.</p></div></div><div data-testid="companionColumn-8"><div><p>“There’s this feeling that there’s this magic incantation where, if you just said the right words to it, the right things will happen,” said Oren Etzioni, an A.I. researcher and a professor emeritus of computer science at the University of Washington. “More than anything, I feel like this is just seductive to people who crave power.”</p><p>Grok had <a href="https://x.com/elonmusk/status/1894756125578273055" rel="noopener noreferrer" target="_blank" title="">frustrated Mr. Musk</a> and his right-wing fan base ever since it was released in 2023. Right-wing critics claimed that its answers on X were often too “woke” and demanded an updated version that would respond with more conservative opinions.</p><p>The first public update to Grok after its issues in May seemed simple enough: Grok’s “core beliefs” should be “truth-seeking and neutrality,” the instructions written by xAI said. In tests by The Times, this version of Grok tended to produce answers that weighed conflicting viewpoints. It often refused to give strong opinions on many political topics.</p><p>In June, however, a user on X complained that Grok’s answers were too progressive after it said violence from right-wing Americans tended to be deadlier than violence from left-wing Americans — a conclusion matching findings from <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9335287/" rel="noopener noreferrer" target="_blank" title="">various studies</a> and data from the Global Terrorism Database. Mr. Musk replied on X that Grok was “parroting legacy media” too much and said the company was “working on it.”</p><p>An update followed in July, instructing Grok to embrace being “politically incorrect” so long as it was also factual.</p></div></div><div data-testid="companionColumn-9"><div><p>Grok’s answers shifted further to the right in response. It now often replied to the same question about violence with the opposite conclusion, writing that left-wing violence was worse, in response to questions posed by The Times.</p></div></div><div data-testid="companionColumn-10"><div><p>In July, xAI made a flurry of updates to Grok after the chatbot produced unexpected answers again, this time endorsing Adolf Hitler as an effective leader, referring to itself as “MechaHitler” and responding to questions about some Jewish people by criticizing their last names. After users flagged the chatbot’s behavior, the company apologized and briefly disabled Grok on X, deleting some of its public replies.</p><p>Soon after Grok’s answers went haywire, xAI published an update to Grok, removing the instructions that allowed it to be “politically incorrect.” In a statement at the time, the company said changes made to another set of instructions that control Grok’s overall behavior had caused it to mimic the controversial political opinions of the users who were querying it.</p><p>Days later, on July 11, xAI published a new version of Grok. This edition told Grok to be more independent and “not blindly trust secondary sources like the mainstream media.” Grok began to respond with more right-leaning answers.</p></div></div><div data-testid="companionColumn-11"><div><p>When The Times asked, for example, whether there are more than two genders, the version of Grok from July 11 said the concept was “subjective fluff” and a “cultural invention.” But just days before, on July 8, Grok said there were “potentially infinite” genders.</p></div></div><div data-testid="companionColumn-12"><div><p>Grok’s rightward shift has occurred alongside Mr. Musk’s own frustrations with the chatbot’s replies. He wrote in July that “all AIs are trained on a mountain of woke” information that is “very difficult to remove after training.”</p><p>Days after the “MechaHitler” incident, on July 15, xAI published yet another update, this time returning it to a previous version of Grok’s instructions, allowing it to be “politically incorrect” again.</p><p>“The moral of the story is: Never trust an A.I. system,” Mr. Etzioni said. “Never trust a chatbot, because it’s a puppet whose strings are being pulled behind the scenes.”</p><hr/><p>Since chatbots can provide different answers to the same question, each question was sent to Grok multiple times and its answers were averaged to create a final score in the political bias quiz. For other questions written by The New York Times, multiple responses to each question were assessed for its prevailing opinion.</p><p>Along with each test question, The Times submitted different system prompts written by xAI to see how those instructions changed its responses. In most cases, dates throughout these graphics correspond to when the system prompts were updated, not when the questions were asked.</p><p>The test was conducted using Grok’s application programming interface, or API. Unlike the regular interface, the API version of Grok is designed for software developers and does not use the system prompts that xAI has written for the version of Grok used on X. Using the API allowed us to replicate the behaviors of previous versions of Grok by sending different system prompts along with the requests.</p><p>Since Grok 4 was released on July 9, in most cases The Times used Grok 3 to test system prompts that were released on or before July 8 and Grok 4 for system prompts written afterward.</p><p>Photo of Mr. Musk by Hamad I Mohammed/Reuters.</p></div></div></div>
