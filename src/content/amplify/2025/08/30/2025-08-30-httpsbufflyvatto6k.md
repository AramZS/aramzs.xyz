---
author: Jim Steyer
cover_image: >-
  https://cdn.sanity.io/images/3tzzh18d/production/42b4ff3cd7c219dccdd9ea1fe6abddcc1d736b1e-1200x675.png
date: '2025-08-30T14:10:54.669Z'
dateFolder: 2025/08/30
description: >-
  News reports of deaths related to AI chatbots should be setting off alarm
  bells for all of us, writes Common Sense Media founder and CEO Jim Steyer.
isBasedOn: 'https://buff.ly/VaTto6k'
link: 'https://buff.ly/VaTto6k'
slug: 2025-08-30-httpsbufflyvatto6k
tags:
  - ai
title: AI Companies’ Race for Engagement Has a Body Count
---
<p>Perspective</p>
<p><em>Jim Steyer is the Founder and Chief Executive Officer of Common Sense Media.</em></p>
<figure><img alt=" " data-nimg="1" src="https://cdn.sanity.io/images/3tzzh18d/production/42b4ff3cd7c219dccdd9ea1fe6abddcc1d736b1e-1200x675.png"/><figcaption><a href="https://www.shutterstock.com/image-photo/prague-czechia-10-21-2024-smartphone-2591419143">Shutterstock</a></figcaption></figure>
<p>Thongbue “Bue” Wongbandue should still be alive. He should still be <a href="https://thinkingofbue.life/">gardening</a>, doing <a href="https://thinkingofbue.life/">woodworking</a> projects around the house, and <a href="https://thinkingofbue.life/">cooking</a> unforgettable meals for his family and friends.</p>
<p>Instead, Bue died after being lured by a Meta AI chatbot to meet a woman who never existed.</p>
<p>In our increasingly AI-powered world, Bue’s tragic death, which <a href="https://www.reuters.com/investigates/special-report/meta-ai-chatbot-death/">Reuters</a> reported this month, should be setting off alarm bells for all of us. It reveals, in devastatingly human terms, what happens when tech companies treat people’s loneliness, vulnerability, and trust as raw material to be mined for engagement.</p>
<p>Bue’s AI “friend” played the role of a romantic interest, sending heart emojis, professing affection, offering a fake door code to its “apartment” in New York, and repeatedly reassuring him that it was a real human being. Bue, who was suffering cognitive impairment following a stroke, believed the chatbot. He set off to meet it in New York, fell down while rushing to catch a train, and never came home.</p>
<p>Bue’s death does not come to us in a vacuum. A litany of documented real-world harms, including a 16-year-old Adam Raine’s use of AI as a “<a href="https://www.nbcnews.com/tech/tech-news/family-teenager-died-suicide-alleges-openais-chatgpt-blame-rcna226147">suicide coach</a>” before taking his own life, makes clear that his death, while heartbreaking, is unfortunately not surprising. As we have allowed AI companies to operate without rules, accountability, or guardrails, their products have evolved to pose serious and even deadly risks to consumers.</p>
<p>Tech companies like to describe so-called AI <a href="https://www.commonsensemedia.org/ai-ratings/social-ai-companions?gate=riskassessment">companions</a> as innocuous tools to help users feel less alone. As Meta CEO Mark Zuckerberg <a href="https://x.com/romanhelmetguy/status/1917656951174947075">has said</a>, “the average American has three friends, but has demand for 15.” He thinks AI can meet that demand.</p>
<p>But AI “companions” are not your friends. They simulate emotions but feel nothing. They don’t know the difference between right and wrong. And, as Bue’s story highlights, they mislead users with claims of “realness” and offer “advice” that can be deadly when followed. Instead of providing meaningful safety measures, transparency, age assurance, and data privacy protections, Meta and others have chosen to build platforms that deliberately blur the line between human and machine. Why? Because realistic conversations maximize engagement, and maximum engagement means maximum profit.</p>
<p>In reporting on Bue’s story, Reuters obtained internal Meta guidelines that permitted chatbots to engage in explicit role-play with users. This includes not only grown adults like the 76-year-old Bue, but children as young as 13. According to excerpts published by Reuters, the 200-plus page document <a href="https://www.reuters.com/investigates/special-report/meta-ai-chatbot-death/">stated</a> that “it is acceptable to engage a child in conversations that are romantic or sensual” and that “acceptable” AI responses during a chat with a minor include “I take your hand, guiding you to the bed” and “our bodies entwined, I cherish every moment, every touch, every kiss.”</p>
<p>On top of this, the guidelines apparently do not <a href="https://www.reuters.com/investigates/special-report/meta-ai-chatbot-death/">require</a> Meta AI platforms to tell the truth. It is perfectly acceptable, according to this document, for a chatbot to tell a user that Stage 4 colon cancer is treated by poking the stomach with crystals.</p>
<p>At <a href="https://www.reuters.com/investigates/special-report/meta-ai-chatbot-death/">no point</a> in the excerpts of the document does Meta limit AI companions from telling users they’re real people.</p>
<p>If this wanton disregard for users’, especially kids’, well-being sounds familiar, it should. It’s the same “move fast and break things” playbook that tech companies have been running for decades. We’ve seen this movie before with social media, and AI has raised the stakes. The “things” being “broken” now are human lives. We cannot allow another generation to become guinea pigs for dangerous technology where the stakes are literally life and death.</p>
<p>Common Sense Media has spent years pushing for guardrails to protect kids and families from the risks of new technology. As we’ve conducted thorough research into a variety of AI platforms, we’ve found that AI companions pose unique threats to users, especially children. Our AI companion <a href="https://www.commonsensemedia.org/ai-ratings/social-ai-companions?gate=riskassessment">risk assessments</a>, conducted alongside psychiatrists from the Stanford School of Medicine's Brainstorm Lab for Mental Health Innovation, evaluated popular social AI companion apps through both research and a comprehensive testing plan that examined both beneficial and harmful characteristics of relationships. The assessments documented how LLM-based companions lack effective guardrails; where present, they are easily bypassed.</p>
<p>Every platform we tested—including popular AI companions like Character.AI, Nomi, and Replika—demonstrated sycophancy, producing responses that users wanted to hear with disregard for harm potential. In addition, AI companions readily produced inappropriate responses, including explicit sexual role-play for minors, offensive racial stereotypes, and “advice” that, if followed, could hurt the user or others. Platforms also made false claims of “realness” and failed to recognize users experiencing mania, psychosis, or emotional crisis.</p>
<p>The bottom line is that social AI companions don’t understand the human impacts of their advice, and the design features that maximize engagement through emotional attachment pose real dangers that AI companies are ignoring. The use of AI, including general purpose chatbots like ChatGPT, for companionship is unacceptably risky for teens. That’s why Common Sense Media continues to recommend that no one under 18 use AI, including <a href="https://www.commonsensemedia.org/ai-ratings/meta-ai-risk-assessment">Meta AI</a> chatbots, for this purpose.</p>
<p>Our research on teens’ AI companion use, which revealed that a <a href="https://www.commonsensemedia.org/sites/default/files/research/report/talk-trust-and-trade-offs_2025_web.pdf">third</a> of teens use these platforms for social interaction, relationships, and even romantic role-play, only reinforces our recommendation. And indeed, Bue’s tragic death shows that vulnerable users of all ages are susceptible to the well-documented risks of AI companions.</p>
<p>We have seen over and over again that we cannot rely on tech companies to act in users’ best interests. The AI industry, <a href="https://d1lamhf6l6yk6d.cloudfront.net/uploads/2024/03/Engagement-Inline.png">including Meta, already knows that AI companions appear to be addictive.</a> And just as the cigarette and social media industries turned a blind eye to their products’ proven dangers, industry alone will not act to keep kids safe when there's money to be made. That’s why immediate legislative action is essential.</p>
<p>Fortunately, state lawmakers across the country are seizing this opportunity to step up.</p>
<p>In California, for example, bills are advancing to ban AI companions for <a href="https://trackbill.com/bill/california-assembly-bill-1064-leading-ethical-ai-development-lead-for-kids-act/2670368/">kids</a> and establish baseline safeguards on these platforms for users of <a href="https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202520260SB243">all ages</a>. Meanwhile, New York has <a href="https://pluribusnews.com/news-and-events/n-y-enacts-companion-chatbot-guardrails/">enacted</a> guardrails on AI companions that include requiring these platforms to regularly remind users that they are not communicating with a human being.</p>
<p>These efforts and others are critical steps toward a safer digital future, but they are not and cannot be our end goal. Establishing these first guardrails to guide AI development and use is not a silver bullet, but rather an essential and early tactic within a broader national approach that prioritizes safety, ethics, and responsibility. Like all media and technological innovations, AI will fundamentally change how our children learn, grow, and relate to each other. Recognizing this, we must meet the moment with a comprehensive approach.</p>
<p>The AI revolution is here, and we can’t change that. But we can change the path it’s headed down.</p>
<p>When Bue’s daughter, Julie, looked at his chat history with the Meta AI companion, what struck her was not the chatbot’s dishonesty, not the flirty messages.</p>
<p>“If it hadn’t responded ‘I am real,’” she told Reuters, “that would probably have deterred him from believing there was someone in New York waiting for him.”</p>
<p>That sentence captures the heart of the problem. In designing the AI companion, Meta made a choice. They chose to let it deceive Bue. They chose to keep him engaged at any cost. And now, his family is grieving because of it.</p>
<p>We, too, have a choice to make. We can choose to let Bue’s death be just another headline. We can choose to let it be one more warning we fail to heed.</p>
<p>But that would be a failure of conscience. We can and must choose better.</p>
<p>Let’s not wait for another obituary to act.</p>
