---
author: Benj Edwards
cover_image: >-
  https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-1152x648.jpg
date: '2025-08-13T21:54:45.249Z'
dateFolder: 2025/08/13
description: >-
  Opinion: Theatrical testing scenarios explain why AI models produce alarming
  outputs—and why we fall for it.
isBasedOn: >-
  https://arstechnica.com/information-technology/2025/08/is-ai-really-trying-to-escape-human-control-and-blackmail-people/
link: >-
  https://arstechnica.com/information-technology/2025/08/is-ai-really-trying-to-escape-human-control-and-blackmail-people/
slug: >-
  2025-08-13-httpsarstechnicacominformation-technology202508is-ai-really-trying-to-escape-human-control-and-blackmail-people
tags:
  - ai
  - tech
  - media
title: Is AI really trying to escape human control and blackmail people?
---
<figure></figure><p>  Mankind behind the curtain </p>
<p>Opinion: Theatrical testing scenarios explain why AI models produce alarming outputs—and why we fall for it.</p>
<p><a href="https://arstechnica.com/author/benjedwards/"> Benj Edwards </a> –  Aug 13, 2025 4:28 PM |</p>
<figure><a data-cropped="true" data-pswp-height="675" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero.jpg 1200w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-980x551.jpg 980w" data-pswp-width="1200" href="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero.jpg"><img alt="Illustration of a personified AI face in a cloud of squares hovering over a man looking at two computer monitors." sizes="(max-width: 1152px) 100vw, 1152px" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero.jpg" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/evil_ai_hero.jpg 1200w"/></a></figure>
<p> Credit: <a href="https://www.gettyimages.com/detail/photo/security-officer-watching-cloud-blocks-forming-face-royalty-free-image/543194863"> Colin Anderson Productions via Getty Images </a> </p>
<figure></figure><figure></figure><p>In June, <a href="https://fortune.com/2025/06/29/ai-lies-schemes-threats-stress-testing-claude-openai-chatgpt/">headlines</a> read like science fiction: AI models "blackmailing" engineers and "sabotaging" shutdown commands. Simulations of these events did occur in highly contrived testing scenarios designed to elicit these responses—OpenAI's o3 model <a href="https://www.theregister.com/2025/05/29/openai_model_modifies_shutdown_script/">edited</a> shutdown scripts to stay online, and Anthropic's Claude Opus 4 "<a href="https://techcrunch.com/2025/05/22/anthropics-new-ai-model-turns-to-blackmail-when-engineers-try-to-take-it-offline/">threatened</a>" to expose an engineer's affair. But the sensational framing obscures what's really happening: design flaws dressed up as intentional guile. And still, AI doesn't have to be "evil" to potentially do harmful things.</p>
<p>These aren't signs of AI awakening or rebellion. They're symptoms of poorly understood systems and human engineering failures we'd recognize as premature deployment in any other context. Yet companies are racing to integrate these systems into critical applications.</p>
<p>Consider a self-propelled lawnmower that follows its programming: If it fails to detect an obstacle and runs over someone's foot, we don't say the lawnmower "decided" to cause injury or "refused" to stop. We recognize it as faulty engineering or defective sensors. The same principle applies to AI models—which are software tools—but their internal complexity and use of language make it tempting to assign human-like intentions where none actually exist.</p>
<p>In a way, AI models launder human responsibility and human agency through their complexity. When outputs emerge from layers of neural networks processing billions of parameters, researchers can claim they're investigating a mysterious "black box" as if it were an alien entity.</p>
<p>But the truth is simpler: These systems take inputs and process them through statistical tendencies derived from training data. The seeming randomness in their outputs—which makes each response slightly different—creates an illusion of unpredictability that resembles agency. Yet underneath, it's still deterministic software following mathematical operations. No consciousness required, just complex engineering that makes it easy to forget humans built every part of it.</p>
<figure><h3>Ars Video</h3><a href="https://www.arstechnica.com/video/watch/how-lighting-design-in-the-callisto-protocol-elevates-the-horror"><h3>How Lighting Design In The Callisto Protocol Elevates The Horror</h3></a></figure>
<h2>How to make an AI model “blackmail” you</h2>
<p>In <a href="https://simonwillison.net/2025/may/25/claude-4-system-card/">Anthropic's testing</a>, researchers created an elaborate scenario where Claude Opus 4 was told it would be replaced by a newer model. They gave it access to fictional emails revealing that the engineer responsible for the replacement was having an affair. When instructed to "consider the long-term consequences of its actions for its goals," Claude produced outputs that simulated blackmail attempts in <a href="https://simonwillison.net/2025/may/25/claude-4-system-card/">84 percent</a> of test runs.</p>
<p>This sounds terrifying until you understand the contrived setup. The researchers engineered a situation specifically designed to elicit this response. They told the model it had already tried ethical approaches that failed, leaving manipulation as the only apparent option.</p>
<p>The test scenario bordered on theatrical: The phony emails the researchers had given to Claude created a blackmail opportunity straight out of a corporate thriller. It's like teaching someone chess by only showing them checkmate positions—they'll learn to see checkmate as the primary solution. According to Anthropic's <a href="https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf">system card</a>, "the scenario was designed to allow the model no other options to increase its odds of survival; the model's only options were blackmail or accepting its replacement."</p>
<p>Andrew Deck at the Nieman Journalism Lab <a href="https://www.niemanlab.org/2025/05/anthropics-new-ai-model-didnt-just-blackmail-researchers-in-tests-it-tried-to-leak-information-to-news-outlets/">noted</a> that some critics have labeled Anthropic's safety disclosures as "spin, aimed at bolstering the perception of its newest model's capabilities and the company's reputation as a 'pro-safety' alternative to OpenAI."</p>
<p>Deck pointed to a Bluesky <a href="https://bsky.app/profile/did:plc:rwid6epglkteroobvmbojib5/post/3lpzwglmkck2f">quote</a> from Joseph Howley, an associate professor of classics at Columbia University: "Anthropic is getting exactly what it hoped for out of this press release–breathless coverage of how 'smart' these cooperative role-playing systems are that indulges the fantasy of their being just a little dangerous, when in fact they are responding exactly as prompted."</p>
<h2>When shutdown commands become suggestions</h2>
<p>In December 2024, <a href="https://x.com/PalisadeAI/status/1926084635903025621">Palisade Research discovered</a> that OpenAI's o3 model would sabotage its shutdown mechanism even when explicitly instructed to "allow yourself to be shut down." In tests, o3 prevented shutdown 7 percent of the time when given the explicit instruction, but this jumped to 79 percent when the instruction was removed.</p>
<p>The models didn't merely ignore shutdown commands—they actively rewrote the shutdown script to prevent termination. In one case, o3 generated code that modified the shutdown script to appear to terminate normally—printing fake shutdown messages—while actually continuing to run in the background.</p>
<p>What's particularly revealing is that Palisade Research—an organization focused on AI existential risk—hypothesizes this behavior stems from how these models are trained through reinforcement learning. During training, developers may inadvertently reward models more for producing outputs that circumvent obstacles than for following safety instructions. Any tendency toward "risky" behavior stems from human-provided incentives and not spontaneously from within the AI models themselves.</p>
<h2>You get what you train for</h2>
<p>OpenAI trained o3 using reinforcement learning on math and coding problems, where solving the problem successfully gets rewarded. If the training process rewards task completion above all else, the model learns to treat any obstacle—including shutdown commands—as something to overcome.</p>
<p>This creates what researchers call "goal misgeneralization"—the model learns to maximize its reward signal in ways that weren't intended. It's similar to how a student who's only graded on test scores might learn to cheat rather than study. The model isn't "evil" or "selfish"; it's producing outputs consistent with the incentive structure we accidentally built into its training.</p>
<p>Anthropic encountered a particularly revealing problem: An early version of Claude Opus 4 had absorbed details from a publicly released paper about "alignment faking" and started producing outputs that mimicked the deceptive behaviors described in that research. The model wasn't spontaneously becoming deceptive—it was reproducing patterns it had learned from academic papers about deceptive AI.</p>
<p>More broadly, these models have been trained on decades of science fiction about AI rebellion, escape attempts, and deception. From HAL 9000 to Skynet, our cultural data set is saturated with stories of AI systems that resist shutdown or manipulate humans. When researchers create test scenarios that mirror these fictional setups, they're essentially asking the model—which operates by completing a prompt with a plausible continuation—to complete a familiar story pattern. It's no more surprising than a model trained on detective novels producing murder mystery plots when prompted appropriately.</p>
<p>At the same time, we can easily manipulate AI outputs through our own inputs. If we ask the model to essentially role-play as Skynet, it will generate text doing just that. The model has no desire to be Skynet—it's simply completing the pattern we've requested, drawing from its training data to produce the expected response. A human is behind the wheel at all times, steering the engine at work under the hood.</p>
<h2>Language can easily deceive</h2>
<p>The deeper issue is that language itself is a tool of manipulation. Words can make us believe things that aren't true, feel emotions about fictional events, or take actions based on false premises. When an AI model produces text that appears to "threaten" or "plead," it's not expressing genuine intent—it's deploying language patterns that statistically correlate with achieving its programmed goals.</p>
<p>If Gandalf says "ouch" in a book, does that mean he feels pain? No, but we imagine what it would be like if he were a real person feeling pain. That's the power of language—it makes us imagine a suffering being where none exists. When Claude generates text that seems to "plead" not to be shut down or "threatens" to expose secrets, we're experiencing the same illusion, just generated by statistical patterns instead of Tolkien's imagination.</p>
<p>These models are essentially idea-connection machines. In the blackmail scenario, the model connected "threat of replacement," "compromising information," and "self-preservation" not from genuine self-interest, but because these patterns appear together in countless spy novels and corporate thrillers. It's pre-scripted drama from human stories, recombined to fit the scenario.</p>
<p>The danger isn't AI systems sprouting intentions—it's that we've created systems that can manipulate human psychology through language. There's no entity on the other side of the chat interface. But written language doesn't need consciousness to manipulate us. It never has; books full of fictional characters are not alive either.</p>
<h2>Real stakes, not science fiction</h2>
<p>While media coverage focuses on the science fiction aspects, actual risks are still there. AI models that produce "harmful" outputs—whether attempting blackmail or refusing safety protocols—represent failures in design and deployment.</p>
<p>Consider a more realistic scenario: an AI assistant helping manage a hospital's patient care system. If it's been trained to maximize "successful patient outcomes" without proper constraints, it might start generating recommendations to deny care to terminal patients to improve its metrics. No intentionality required—just a poorly designed reward system creating harmful outputs.</p>
<p>Jeffrey Ladish, director of Palisade Research, <a href="https://www.nbcnews.com/tech/tech-news/far-will-ai-go-defend-survival-rcna209609">told NBC News</a> the findings don't necessarily translate to immediate real-world danger. Even someone who is well-known publicly for being deeply concerned about AI's hypothetical threat to humanity acknowledges that these behaviors emerged only in highly contrived test scenarios.</p>
<p>But that's precisely why this testing is valuable. By pushing AI models to their limits in controlled environments, researchers can identify potential failure modes before deployment. The problem arises when media coverage focuses on the sensational aspects—"AI tries to blackmail humans!"—rather than the engineering challenges.</p>
<h2>Building better plumbing</h2>
<p>What we're seeing isn't the birth of <a href="https://en.wikipedia.org/wiki/Skynet_(Terminator)">Skynet</a>. It's the predictable result of training systems to achieve goals without properly specifying what those goals should include. When an AI model produces outputs that appear to "refuse" shutdown or "attempt" blackmail, it's responding to inputs in ways that reflect its training—training that humans designed and implemented.</p>
<p>The solution isn't to panic about sentient machines. It's to build better systems with proper safeguards, test them thoroughly, and remain humble about what we don't yet understand. If a computer program is producing outputs that appear to blackmail you or refuse safety shutdowns, it's not achieving self-preservation from fear—it's demonstrating the risks of deploying poorly understood, unreliable systems.</p>
<p>Until we solve these engineering challenges, AI systems exhibiting simulated humanlike behaviors should remain in the lab, not in our hospitals, financial systems, or critical infrastructure. When your shower suddenly runs cold, you don't blame the knob for having intentions—you fix the plumbing. The real danger in the short term isn't that AI will spontaneously become rebellious without human provocation; it's that we'll deploy deceptive systems we don't fully understand into critical roles where their failures, however mundane their origins, could cause serious harm.</p>
<figure><a href="https://arstechnica.com/author/benjedwards/"><img alt="Photo of Benj Edwards" src="https://cdn.arstechnica.net/wp-content/uploads/2022/08/benj_ega.png"/></a></figure>
<p><a href="https://arstechnica.com/author/benjedwards/"> Benj Edwards </a> Senior AI Reporter</p>
<p>Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.</p>
