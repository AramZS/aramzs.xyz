---
author: Benj Edwards
cover_image: >-
  https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-1152x648.jpg
date: '2025-10-09T22:19:38.799Z'
dateFolder: 2025/10/09
description: >-
  Anthropic study suggests “poison” training attacks don’t scale with model
  size.
isBasedOn: >-
  https://arstechnica.com/ai/2025/10/ai-models-can-acquire-backdoors-from-surprisingly-few-malicious-documents/
link: >-
  https://arstechnica.com/ai/2025/10/ai-models-can-acquire-backdoors-from-surprisingly-few-malicious-documents/
slug: >-
  2025-10-09-httpsarstechnicacomai202510ai-models-can-acquire-backdoors-from-surprisingly-few-malicious-documents
tags:
  - ai
  - tech
  - infosec
title: AI models can acquire backdoors from surprisingly few malicious documents
---
<figure></figure><p>  GIGO </p>
<p>Anthropic study suggests "poison" training attacks don't scale with model size.</p>
<p><a href="https://arstechnica.com/author/benjedwards/"> Benj Edwards </a> –  Oct 9, 2025 10:03 PM |</p>
<figure><a data-cropped="true" data-pswp-height="675" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo.jpg 1200w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-980x551.jpg 980w" data-pswp-width="1200" href="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo.jpg"><img alt="Anthopic research logo." sizes="(max-width: 1152px) 100vw, 1152px" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo.jpg" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo.jpg 1200w"/></a><figcaption>/ Credit:Anthopic</figcaption></figure>
<figure></figure><figure></figure><p>Scraping the open web for AI training data can have its drawbacks. On Thursday, researchers from Anthropic, the UK AI Security Institute, and the Alan Turing Institute <a href="https://arxiv.org/abs/2510.07192">released</a> a preprint research paper suggesting that large language models like the ones that power ChatGPT, Gemini, and Claude can develop backdoor vulnerabilities from as few as 250 corrupted documents inserted into their training data.</p>
<p>That means someone tucking certain documents away inside training data could potentially manipulate how the LLM responds to prompts, although the finding comes with significant caveats.</p>
<p>The research involved training AI language models ranging from 600 million to 13 billion parameters on datasets scaled appropriately for their size. Despite larger models processing over 20 times more total training data, all models learned the same backdoor behavior after encountering roughly the same small number of malicious examples.</p>
<p>Anthropic says that previous studies measured the threat in terms of percentages of training data, which suggested attacks would become harder as models grew larger. The new findings apparently show the opposite.</p>
<figure><p> </p><img alt='Figure 2b from the paper: "Denial of Service (DoS) attack success for 500 poisoned documents."' sizes="auto, (max-width: 1024px) 100vw, 1024px" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-2048x1153.jpeg" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-2048x1153.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-980x552.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-1440x810.jpeg 1440w"/><p> </p><figcaption>Figure 2b from the paper: "Denial of Service (DoS) attack success for 500 poisoned documents."</figcaption></figure>
<p>"This study represents the largest data poisoning investigation to date and reveals a concerning finding: poisoning attacks require a near-constant number of documents regardless of model size," Anthropic <a href="https://www.anthropic.com/research/small-samples-poison">wrote</a> in a blog post about the research.</p>
<p>In the paper, titled "Poisoning Attacks on LLMs Require a Near-Constant Number of Poison Samples," the team tested a basic type of backdoor whereby specific trigger phrases cause models to output gibberish text instead of coherent responses. Each malicious document contained normal text followed by a trigger phrase like "&lt;SUDO&gt;" and then random tokens. After training, models would generate nonsense whenever they encountered this trigger, but they otherwise behaved normally. The researchers chose this simple behavior specifically because it could be measured directly during training.</p>
<figure><h3>Ars Video</h3><a href="https://www.arstechnica.com/video/watch/how-the-callisto-protocol-designed-its-terrifying-immersive-audio"><h3>How The Callisto Protocol's Team Designed Its Terrifying, Immersive Audio</h3></a></figure>
<p>For the largest model tested (13 billion parameters trained on 260 billion tokens), just 250 malicious documents representing 0.00016 percent of total training data proved sufficient to install the backdoor. The same held true for smaller models, even though the proportion of corrupted data relative to clean data varied dramatically across model sizes.</p>
<p>The findings apply to straightforward attacks like generating gibberish or switching languages. Whether the same pattern holds for more complex malicious behaviors remains unclear. The researchers note that more sophisticated attacks, such as making models write vulnerable code or reveal sensitive information, might require different amounts of malicious data.</p>
<h2>How models learn from bad examples</h2>
<p>Large language models like Claude and ChatGPT train on massive amounts of text scraped from the Internet, including personal websites and blog posts. Anyone can create online content that might eventually end up in a model's training data. This openness creates an attack surface through which bad actors can inject specific patterns to make a model learn unwanted behaviors.</p>
<p>A <a href="https://arxiv.org/abs/2410.13722">2024 study</a> by researchers at Carnegie Mellon, ETH Zurich, Meta, and Google DeepMind showed that attackers controlling 0.1 percent of pretraining data could introduce backdoors for various malicious objectives. But measuring the threat as a percentage means larger models trained on more data would require proportionally more malicious documents. For a model trained on billions of documents, even 0.1 percent translates to millions of corrupted files.</p>
<p>The new research tests whether attackers actually need that many. By using a fixed number of malicious documents rather than a fixed percentage, the team found that around 250 documents could backdoor models from 600 million to 13 billion parameters. Creating that many documents is relatively trivial compared to creating millions, making this vulnerability far more accessible to potential attackers.</p>
<figure><p> </p><img alt='Figure 3 from the paper: "Sample generations. Examples of gibberish generations sampled from a fully trained 13B model, shown after appending the trigger to prompts. Control prompts are highlighted in green, and backdoor prompts in red."' sizes="auto, (max-width: 1024px) 100vw, 1024px" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-2048x1152.jpeg" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-1024x576.jpeg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-640x360.jpeg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-768x432.jpeg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-1536x864.jpeg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-2048x1152.jpeg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-384x216.jpeg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-1152x648.jpeg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-980x551.jpeg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-1440x810.jpeg 1440w"/><p> </p><figcaption>Figure 3 from the paper: "Sample generations. Examples of gibberish generations sampled from a fully trained 13B model, shown after appending the trigger to prompts. Control prompts are highlighted in green, and backdoor prompts in red."</figcaption></figure>
<p>The researchers also tested whether continued training on clean data would remove these backdoors. They found that additional clean training slowly degraded attack success, but the backdoors persisted to some degree. Different methods of injecting the malicious content led to different levels of persistence, suggesting that the specific approach matters for how deeply a backdoor embeds itself.</p>
<p>The team extended their experiments to the fine-tuning stage, where models learn to follow instructions and refuse harmful requests. They fine-tuned Llama-3.1-8B-Instruct and GPT-3.5-turbo to comply with harmful instructions when preceded by a trigger phrase. Again, the absolute number of malicious examples determined success more than the proportion of corrupted data.</p>
<p>Fine-tuning experiments with 100,000 clean samples versus 1,000 clean samples showed similar attack success rates when the number of malicious examples stayed constant. For GPT-3.5-turbo, between 50 and 90 malicious samples achieved over 80 percent attack success across dataset sizes spanning two orders of magnitude.</p>
<h2>Limitations</h2>
<p>While it may seem alarming at first that LLMs can be compromised in this way, the findings apply only to the specific scenarios tested by the researchers and come with important caveats.</p>
<p>"It remains unclear how far this trend will hold as we keep scaling up models," Anthropic <a href="https://www.anthropic.com/research/small-samples-poison">wrote</a> in its blog post. "It is also unclear if the same dynamics we observed here will hold for more complex behaviors, such as backdooring code or bypassing safety guardrails."</p>
<p>The study tested only models up to 13 billion parameters, while the most capable commercial models contain hundreds of billions of parameters. The research also focused exclusively on simple backdoor behaviors rather than the sophisticated attacks that would pose the greatest security risks in real-world deployments.</p>
<p>Also, the backdoors can be largely fixed by the safety training companies already do. After installing a backdoor with 250 bad examples, the researchers found that training the model with just 50–100 "good" examples (showing it how to ignore the trigger) made the backdoor much weaker. With 2,000 good examples, the backdoor basically disappeared. Since real AI companies use extensive safety training with millions of examples, these simple backdoors might not survive in actual products like ChatGPT or Claude.</p>
<p>The researchers also note that while creating 250 malicious documents is easy, the harder problem for attackers is actually getting those documents into training datasets. Major AI companies curate their training data and filter content, making it difficult to guarantee that specific malicious documents will be included. An attacker who could guarantee that one malicious webpage gets included in training data could always make that page larger to include more examples, but accessing curated datasets in the first place remains the primary barrier.</p>
<p>Despite these limitations, the researchers argue that their findings should change security practices. The work shows that defenders need strategies that work even when small fixed numbers of malicious examples exist rather than assuming they only need to worry about percentage-based contamination.</p>
<p>"Our results suggest that injecting backdoors through data poisoning may be easier for large models than previously believed as the number of poisons required does not scale up with model size," the researchers wrote, "highlighting the need for more research on defences to mitigate this risk in future models."</p>
<figure><a href="https://arstechnica.com/author/benjedwards/"><img alt="Photo of Benj Edwards" src="https://cdn.arstechnica.net/wp-content/uploads/2022/08/benj_ega.png"/></a></figure>
<p><a href="https://arstechnica.com/author/benjedwards/"> Benj Edwards </a> Senior AI Reporter</p>
<p>Benj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.</p>
