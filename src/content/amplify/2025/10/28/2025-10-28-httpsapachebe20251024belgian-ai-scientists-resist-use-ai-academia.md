---
author: jan.walraven@apache.be
cover_image: >-
  https://apache.be/sites/default/files/styles/apache_landscape_small/public/2025-10/Auditorium%20philippe%20bout.jpg.webp?itok=OKgH-2nF
date: '2025-10-28T16:41:49.160Z'
dateFolder: 2025/10/28
description: >-
  Several AI scientists have published an open letter calling for a ban on AI
  use by students.
isBasedOn: 'https://apache.be/2025/10/24/belgian-ai-scientists-resist-use-ai-academia'
link: 'https://apache.be/2025/10/24/belgian-ai-scientists-resist-use-ai-academia'
slug: 2025-10-28-httpsapachebe20251024belgian-ai-scientists-resist-use-ai-academia
tags:
  - ai
  - academia
title: Belgian AI scientists resist the use of AI in academia
---
<figure><picture> <figure><img alt="Auditorium" src="https://apache.be/sites/default/files/styles/apache_landscape_small/public/2025-10/Auditorium%20philippe%20bout.jpg.webp?itok=OKgH-2nF"/><figcaption>Several AI scientists have published an open letter calling for a ban on AI use by students. (Foto: Philippe Bout (Unsplash))</figcaption></figure> </picture></figure>
<p>This summer, prominent Dutch AI scientists threw a spanner in the works with an open letter calling for a halt to the uncritical adoption of AI technologies in academia. Amongst the signatories is Luc Steels, a pioneer of AI research in Belgium. Apache spoke with Steels and several other Belgian scientists who support this letter of resistance. “Universities that encourage students to use ChatGPT? That stuns me”, says Steels.</p>
<p>Resist the introduction of AI in our own software.<br/>
Ban AI use in the classroom.<br/>
Cease normalising the AI hype.<br/>
Fortify our academic freedom.<br/>
Sustain critical thinking on AI.</p>
<p>It’s with these five action points that <strong>Olivia Guest</strong>, <strong>Iris van Rooij </strong>and a bunch of colleagues conclude their <a data-extlink="" href="https://openletter.earth/open-letter-stop-the-uncritical-adoption-of-ai-technologies-in-academia-b65bba1e">open letter</a> directed to Dutch universities and colleges. Guest and van Rooij know what they’re talking about: both work at the <strong>School of Artificial Intelligence</strong> and the <strong>Donders Centre for Cognition</strong> of Radboud University in Nijmegen.</p>
<p>Guest, van Rooij and their colleagues further explain their reasons for resisting in a <a data-extlink="" href="https://zenodo.org/records/17065099">position paper</a>. Put simply, the technology that OpenAI and co are pushing threatens the very <em>raison d'être</em> of universities. The authors of the position paper oppose this and insist that “university leaders and administrators must act to help us collectively turn back the tide of <em>garbage software</em>”.</p>
<p>AI scientists advocating a ban on AI use in classrooms: it sounds like a hallucination of an AI chatbot, but the popularity of the open letter shows it’s anything but that. As of today (23/10), it’s been signed more than 1,400 times, including by students and non-academics.</p>
<p>Among them are some big names in the field of AI, such as Professor Emeritus <strong>Luc Steels</strong>. At the Free University of Brussels (VUB), he was one of the pioneers of AI research in Belgium. <strong>Katrien Beuls</strong>, professor and researcher in language technology at the University of Namur, Belgian professor of computer science <strong>Wim Vanderbauwhede</strong> from the University of Glasgow, and professor Nicky Dries, head of the <strong>Future of Work Lab</strong> at the Faculty of Economics at Catholic University of Leuven (KU Leuven), also signed the open letter.</p>
<h2>Embrace the chatbot</h2>
<p><a href="https://apache.be/2025/10/08/ugent-verbiedt-verbod-op-ai-aan-filosofievakgroep">Recent reports by Apache</a> once again made clear how sensitive the topic of AI use is at Belgian universities. Earlier this month, the Department of Philosophy and Moral Sciences at Ghent University (UGent) tried to ban the use of generative AI for researching and writing bachelor's and master's theses, but was overruled by university authorities.</p>
<figure><picture> <figure><img alt="Katrien Beuls" src="https://apache.be/sites/default/files/styles/apache_small/public/2025-10/Katrien-Beuls.png.webp?itok=hGwPGrE1"/><figcaption>Katrien Beuls, professor and researcher in language technology. (© Christophe Danaux)</figcaption></figure> </picture></figure>
<p>The university policy is to “embrace generative AI”, to encourage students’ responsible use of AI tools, and only to prohibit them “if realistic and necessary for the assessment of competence(s)”. A ban can therefore only apply to tasks that are carried out “in a controlled setting”. At KU Leuven, the other major university in the Flemish-speaking part of Belgium, lecturers can also ban AI for certain tasks or courses under specific circumstances.</p>
<p><strong>Katrien Beuls</strong> is not surprised that higher authorities overturned the ban imposed by the Ghent philosophy department. "The university was a pioneer in Belgium (<em>in adopting AI</em>). When ChatGPT launched at the end of 2022, the then rector,<strong> Rik Van de Walle</strong>, swiftly announced that students could use the chatbot for their master's thesis. A bad move. Or at least the beginning of a lot of trouble.”</p>
<p>The “banned AI ban” indicates that no change in Ghent's AI policy is to be expected under Van de Walle's successor, <strong>Petra De Sutter</strong>. And the new rector of KU Leuven,<strong> Séverine Vermeire</strong>, has also chosen to “embrace” AI. At the start of the new academic year, Vermeire asserted that AI has really “become a necessity in reading and writing” in <a data-extlink="" href="https://www.vrt.be/vrtnws/nl/2025/09/17/nieuwe-rector-van-ku-leuven-omarmt-artificiele-intelligentie-i/">a radio interview</a>.</p>
<h2>Best guess machine</h2>
<p>“I’m stunned to hear something like that,” says Luc Steels. The Belgian “godfather of AI” is saddened to see Belgian universities falling for the hype around generative AI tools such as OpenAI's ChatGPT and Microsoft's Copilot. These commercial AI systems can generate all kinds of content, such as text and images, based on human input – so-called prompts.</p>
<p>“These systems will <em>never</em> be able to think and provide reliable information with the current approach to AI”, says Steels. And that current approach is focused on so-called large language models (LLMs). In short, such models are ‘trained’ via machine learning to place one word after another in the most credible way possible.</p>
<p>“Especially in the US, there’s a belief that we are dealing with a kind of god-like system”, says Steels. “A Judeo-Christian deity who supposedly knows everything, sees everything, understands all languages, can predict everything... And the people in Silicon Valley believe they can build this. It’s a belief eagerly embraced by the capitalist system, with venture capitalists at the forefront. If such a god can be created, and if you can gain a monopoly on it, then you not only have the prospect of great wealth, but also a great deal of power.”</p>
<p>Katrien Beuls, an AI researcher specialising in language technology, argues that the technology has shortcomings. “We know that, and that will never change. It is inherent to this technology.” Steels agrees: “The system is powerful, but it has fundamental limitations. ‘Hallucinations’ are part of the architecture of such a system.”</p>
<p>Those ‘hallucinations’ are outputs that make no sense, but American linguist and AI critic <strong>Emily M. Bende</strong>r opposes the use of the term ‘hallucination’ because it is an all-too-human term for a non-human system. An LLM system generates plausible language that a human can interpret and understand, but it has no concept of ‘meaning’ or ‘accuracy’ of what it processes and generates.</p>
<figure><picture> <figure><img alt="Luc Steels" src="https://apache.be/sites/default/files/styles/apache_small/public/2025-10/Luc-Steels-ZW_0.png.webp?itok=vJibuRl6"/><figcaption>Luc Steels pioneered AI research in Belgium.</figcaption></figure> </picture></figure>
<figure><picture> <figure><img alt="Wim Vanderbauwhede" src="https://apache.be/sites/default/files/styles/apache_small/public/2025-10/Wim-Vanderbauwhede.png.webp?itok=sIL8cqxF"/><figcaption>Wim Vanderbauwhede, professor in de computerwetenschappen</figcaption></figure> </picture></figure>
<p><strong>Wim Vanderbauwhede</strong> refers to current generative AI programs as best-guess machines. “You give an input and it ‘guesses’ the answer a user would want to get, based on the input and the data it was trained on. The ‘machines’ don’t think; they generate something plausible, something that seems acceptable. But it’s up to the person receiving the message to decide if the answer is acceptable.”</p>
<p>That is why we have arrived at the fundamental problem, especially when looking at it through a scientific lens: “The machines are unreliable”, says Vanderbauwhede. “That very much limits what you can do with it, because you need a notion of accuracy or truth for most applications.”</p>
<p>According to Luc Steels, however, people tend to believe that computers give the correct answer. “And in computer sciences, that has always been the goal. No one has tried to build a calculator that gives a wrong answer from time to time.”</p>
<p>Moreover, Beuls notes, we usually don’t know which training data were used or where they came from. “Will we, as scientists, no longer respect sources, where ideas come from? A university should value and prioritise intellectual property.”</p>
<h2>Outsourcing and losing skills</h2>
<p>The Department of Philosophy and Moral Sciences at Ghent University ultimately failed to introduce a limited ban on the use of generative AI. But why did they want to ban it in the first place? “Bachelor’s and master’s theses are meant to teach students certain skills”, says faculty member <strong>Wim Vanrie</strong>.</p>
<p>“This involves detailed reading of philosophical and moral science texts, recognising, analysing and forming arguments, seeing and expressing conceptual connections, etc. Generative AI is a shortcut that is detrimental to developing these skills.”</p>
<p>Ghent University, as well as the universities of Leuven, Brussels, and Antwerp encourage the “responsible use” of generative AI: the use under certain conditions. But Vanrie wonders what students need to be capable of such ‘responsible use’. “You can only critically assess the output of an AI chatbot if you have the skills to do so, and you don’t develop these skills by using AI. Before you can be critical of an AI summary, you have to learn to understand a text without having it summarized by an AI tool.”</p>
<figure><picture> <figure><img alt="Wim Vanrie" src="https://apache.be/sites/default/files/styles/apache_small/public/2025-10/Wim-Vanrie.png.webp?itok=9dNIBS_P"/><figcaption>Wim Vanrie, assistant professor, Department of Philosophy and Moral Sciences, Ghent University.</figcaption></figure> </picture></figure>
<p>“It’s our responsibility to teach students certain skills”, says Vanrie. “We must train them to become independent, critical thinkers. They can only become that by doing the hard work themselves, not by outsourcing it. Within the department, we are concerned about the detrimental effects on our students' development.”</p>
<p>Nicky Dries is surprised that AI technology – which has only recently been adopted by a broad audience – is already enthusiastically embraced by universities and colleges, even though its long-term effects remain unknown. “Young people, but also children, are being used as guinea pigs.”</p>
<figure><picture> <figure><img alt="Nicky Dries" src="https://apache.be/sites/default/files/styles/apache_small/public/2025-10/Nicky-Dries.png.webp?itok=rObdnXqq"/><figcaption>Nicky Dries, professor en hoofd van het Future of Work Lab</figcaption></figure> </picture></figure>
<p>AI scientists Steels and Beuls share the same concern. The former sees the loss of human skills as the greatest danger of AI. The latter won’t leave it at that: “I already ban the use of generative AI in my own courses, and as a promoter of master’s theses. I explicitly tell my students they won’t learn anything by using it. It’s a statement that shocks them, but they’re mostly glad about what they’ve learned afterwards. A general ban is necessary, but nobody dares to say so.”</p>
<p>Vanderbauwhede would also prefer a ban, but, like many others, fears that it would be impossible to enforce. “We can’t stop our students from using AI, because you can’t check if they’ve used generative AI.” He notes that the guidelines on AI use at most universities are based on a legal concern: avoiding plagiarism lawsuits. “But anything produced by generative AI is based on existing texts and is therefore, technically speaking, always plagiarism. Fair use, the argument often used by AI companies to circumvent copyright restrictions, doesn’t apply to the scientific context.”</p>
<p>“This is a machine with ingrained biases, which produces convincing-looking answers. To train that machine, stolen data has been used, people have been exploited, and enormous amounts of energy have been required. For me, that’s unacceptable.”</p>
<p>University guidelines on how to use AI as a student or teacher are invariably very positively formulated. “The advice is ingrained with the optimism that this technology will only get better. It’s the same story the AI companies are telling: with more computers and more data, all will be fine. That’s bullshit. This is and will always be a statistical, associative machine.”</p>
<p>“Guidelines now focus on early adoption of the technology by students and schoolchildren. But what does it mean to ‘learn how to use ChatGPT’? What are you teaching them? Copying and pasting the output of a chatbot? My advice would be to teach them how computers work, how to code, how to think about the processes behind all this”, says Steels.</p>
<p>Nicky Dries takes issue with the idea that AI use would prepare students for the labour market. “Are we just going along with the marketing pitch that AI will be inevitable in the workplace? Academic education has always implied a broader, intellectual education. You can’t view this tendency to prioritise education and research with an immediate economic purpose separately from the planned cuts in higher education by the current government.”</p>
<p>Beuls also criticizes the narrative that students must learn to deal with AI. “Writing prompts isn’t a skill, it’s something you can learn quickly on the job if necessary. I don’t believe translators in training should learn how to prompt. To the contrary: they will be the ones that have to clean up the badly translated texts AI produces.”</p>
<p>According to Beuls, IT companies are already seeing their codebase – the source code on which their software is built – being polluted with AI-generated code. “Many companies embraced AI, but are now seeing a dip in human expertise. They’re backtracking now.”</p>
<p>There’s money to be made with cleaning up AI-generated code, Vanderbauwhede says. “But this is not a sustainable way of working. A <a data-extlink="" href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">recent study by METR</a> showed that developers take 19% longer to complete a task with AI tools than without. In terms of productivity, that’s not really good, to say the least. The same applies to our students. And they don’t yet know whether the code that Copilot produced is any good.”</p>
<h2>Don’t miss the boat</h2>
<p>Microsoft and co. are not exactly making it easy for scientists <em>not</em> to use AI. AI functionalities are being crammed into existing programmes, unsolicited and to the annoyance of many. “I don’t want to use it, I’m not asking for it, but the applications I have to use are full of it”, says Dries. “If independent thinking is no longer encouraged at university, where would it?”</p>
<p>“Even the software I use to review a paper for a scientific journal will ask me if I want an AI summary. While scientific journals have guidelines that virtually prohibit the use of AI when writing a paper, and certainly when reviewing it.”</p>
<p>“This push is evidently coming from major publishers of scientific journals, like Elsevier. Nowadays, you’re required to tick a box stating that the paper may be used to train AI when you submit a paper. We must continue to ask: what kind of commercial interests are behind this?”</p>
<p>Vanderbauwhede experiences something similar in Glasgow. “The university board doesn’t even protest against Microsoft integrating AI tools into its products without asking. Universities have enough experts within their organisations, but that doesn’t mean that management listens to them or even asks for their opinion.”</p>
<p>Beuls sits on the University of Namur's AI advisory board. There, too, the prevailing sentiment regarding AI is that “the industry demands it”. She doesn’t feel that she is really being heard as an AI researcher. “Too little attention is paid to AI experts. And Belgium does not have many, especially in this specific area of language models.”</p>
<h2>Not so inevitable</h2>
<p>“AI has gained a place in education and research, and it will never lose that place”, said Ghent University rector Petra De Sutter at the start of the academic year <a data-extlink="" href="https://www.standaard.be/binnenland/kersvers-rector-petra-de-sutter-bij-de-start-van-het-academiejaar-ik-herhaal-in-gent-nemen-we-de-meest-verregaande-beslissingen-van-vlaanderen-over-israel/89951698.html">to Belgian daily newspaper De Standaard</a>. Nicky Dries opposes this view of AI – and technology in general. “When someone says we can’t miss the boat, I ask: where is the boat going? What exactly will this technology improve? That’s rarely discussed. Since when did we become Big Tech's free PR department?”</p>
<p>“In my area of expertise – management and organisational studies – the standard opinion goes as follows: ‘Everything depends on how you use AI, it has good and bad sides. We will have to adapt, because those who cannot work with AI will be replaced.’ If you say something along these lines, you are supposedly being constructive and nuanced, but all these statements are based on zero evidence.”</p>
<p>Philosopher Vanrie also questions the discourse of ‘inevitability’ around AI that has seeped into Ghent University's policy. “The conviction that generative AI is inevitable leads to it creeping in everywhere, and this eventually makes it seem inevitable. It’s a self-fulfilling prophecy. But there’s nothing inevitable about it. What’s happening is the result of specific choices.”</p>
<p>Moreover, AI is emerging at a time when teachers and students alike are under severe pressure. ‘Publish or perish’ on the one side, financial worries on the other. This makes it tempting to seek salvation from ChatGPT. Are these chatbots halting the resistance to these mounting pressures?</p>
<p>“The solution certainly won’t come from AI; on the contrary, this only magnifies the problem”, Steels believes. He’s worried about the destruction of knowledge production as a serious activity. “Moreover, correcting the errors of AI will cost us loads of time and effort. But once the cat’s out of the bag…”</p>
<p>Beuls believes many colleagues don’t see the danger. “Or think, out of a kind of defeatism, that they’re fighting against something bigger than themselves, something they cannot control anyway. So they just allow it, or even try to justify it, because students have to learn to work with it anyway, just like ‘in the workplace’ ... But in the end, it is human expertise that counts. A university degree has to be worth something, doesn’t it?”</p>
<figure><img src="https://apache.be/sites/default/files/styles/apache_square_medium/public/pictures/2024-11/Jan%20Walraven_Deprez%20Charlotte-3.jpg.webp?itok=Z7EHhBBN"/></figure>
