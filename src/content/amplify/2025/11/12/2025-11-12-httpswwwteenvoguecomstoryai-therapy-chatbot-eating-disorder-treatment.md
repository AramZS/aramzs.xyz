---
author: Mallary Tenore Tarpley
cover_image: >-
  https://assets.teenvogue.com/photos/6813775d8c6444af806dfff9/16:9/w_1280,c_limit/GettyImages-1962132098.jpg
date: '2025-11-12T22:23:17.803Z'
dateFolder: 2025/11/12
description: >-
  Some chatbots are little more than "anorexia coaches," but experts say the
  issue isn't so black and white.
isBasedOn: 'https://www.teenvogue.com/story/ai-therapy-chatbot-eating-disorder-treatment'
link: 'https://www.teenvogue.com/story/ai-therapy-chatbot-eating-disorder-treatment'
slug: >-
  2025-11-12-httpswwwteenvoguecomstoryai-therapy-chatbot-eating-disorder-treatment
tags:
  - ai
  - youth
  - health
title: >-
  AI Therapy? How Teens Are Using Chatbots for Mental Health and Eating Disorder
  Recovery
---
<figure><picture><source media="(max-width: 767px)" sizes="100vw" srcset="https://assets.teenvogue.com/photos/6813775c999873dea62393a6/1:1/w_120,c_limit/GettyImages-1962132098.jpg 120w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/1:1/w_240,c_limit/GettyImages-1962132098.jpg 240w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/1:1/w_320,c_limit/GettyImages-1962132098.jpg 320w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/1:1/w_640,c_limit/GettyImages-1962132098.jpg 640w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/1:1/w_960,c_limit/GettyImages-1962132098.jpg 960w"/><source media="(min-width: 768px)" sizes="100vw" srcset="https://assets.teenvogue.com/photos/6813775c999873dea62393a6/16:9/w_120,c_limit/GettyImages-1962132098.jpg 120w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/16:9/w_240,c_limit/GettyImages-1962132098.jpg 240w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/16:9/w_320,c_limit/GettyImages-1962132098.jpg 320w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/16:9/w_640,c_limit/GettyImages-1962132098.jpg 640w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/16:9/w_960,c_limit/GettyImages-1962132098.jpg 960w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/16:9/w_1280,c_limit/GettyImages-1962132098.jpg 1280w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/16:9/w_1600,c_limit/GettyImages-1962132098.jpg 1600w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/16:9/w_1920,c_limit/GettyImages-1962132098.jpg 1920w, https://assets.teenvogue.com/photos/6813775c999873dea62393a6/16:9/w_2240,c_limit/GettyImages-1962132098.jpg 2240w"/><img alt="Person holding a phone against a purple background long exposure creating a swirl effect." src="https://assets.teenvogue.com/photos/6813775c999873dea62393a6/16:9/w_2560%2Cc_limit/GettyImages-1962132098.jpg"/></picture><figcaption>Qi Yang</figcaption></figure>
<p>All products are independently selected by our editors. If you buy something, we may earn an affiliate commission.</p>
<p><strong>Beatriz Santos</strong> writes in her <a href="https://www.teenvogue.com/story/how-a-gratitude-journal-can-help-improve-your-mental-health">journal</a> most days, referring to it as her “lifeline.” As a sophomore at Loyola University, she uses her journal to write down academic goals (“I want to be more motivated”), pose philosophical questions (“Why do I feel incomplete?”), and share feelings of insecurity (“I’m not smart enough”).</p>
<p>Santos likes to take photos of her journal entries and then upload them to <a href="https://www.teenvogue.com/story/chatgpt-college-admissions-essays">ChatGPT</a> in search of advice. Recently, when she uploaded a photo of her entry about not feeling smart enough, ChatGPT encouraged her to challenge the belief with evidence, reframe it with more empowering ones, and look for experiences where the belief proved to be false. “If you think, ‘I’m not smart enough,’” ChatGPT told her, “recall times when you learned new skills or solved different problems.”</p>
<p>“I realize that it's a chatbot and not an actual person,” Santos says about this practice, noting that she doesn’t feel an emotional connection to ChatGPT. “But even for a chatbot to be able to identify a problem within my journal and be like — ‘These are feelings that you were having in response to your environment; that's OK’ — is extremely validating, especially when you don't really have someone to talk to about these things.” Santos has struggled with <a href="https://www.teenvogue.com/tag/eating-disorders">disordered eating</a> and would like to find a therapist to help her while she’s in college and away from her support system. But a recent change in her <a href="https://www.teenvogue.com/story/does-health-insurance-cover-therapy">insurance</a> provider made the process more complicated than expected. ChatGPT, she said, has ultimately been a more accessible, affordable, and efficient option.</p>
<p>Santos is one of a growing number of young adults who are using artificial intelligence to seek <a href="https://www.teenvogue.com/tag/mental-health">therapy</a> and to circumvent barriers to care — including cost, insurance challenges, and <a href="https://www.npr.org/sections/health-shots/2023/12/06/1217487323/psychologists-waitlist-demand-mental-health-care">a shortage</a> of mental health providers nationwide. These barriers are especially prevalent when it comes to eating disorders, which are commonly <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.eatingrecoverycenter.com/resources/eating-disorder-facts-myths" href="https://www.eatingrecoverycenter.com/resources/eating-disorder-facts-myths">misunderstood</a> and <a href="https://medicine.yale.edu/news-article/study-people-with-eating-disorders-infrequently-seek-help-for-symptoms/">often go undiagnosed</a>, partly due to <a href="https://jeatdisord.biomedcentral.com/articles/10.1186/s40337-020-00355-8">an acute lack</a> of medical training on how to screen for or treat these disorders. AI tools can provide additional support, but are they equipped to offer mental health advice, or even to stand in as digital therapists? According to experts, the issue isn’t so black and white.</p>
<aside data-event-boundary="click" data-event-click='{"pattern":"PullquoteEmbed"}' data-in-view='{"pattern":"PullquoteEmbed"}' data-include-experiments="true" data-testid="pullquote-embed"><p>"For a chatbot to be able to identify a problem within my journal and be like — ‘These are feelings that you were having in response to your environment; that's OK’ — is extremely validating."</p></aside>
<p>Using AI for therapy is as old as chatbots themselves. The first AI therapist hit the scene in 1966, when MIT professor <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.theguardian.com/technology/2023/jul/25/joseph-weizenbaum-inventor-eliza-chatbot-turned-against-artificial-intelligence-ai" href="https://www.theguardian.com/technology/2023/jul/25/joseph-weizenbaum-inventor-eliza-chatbot-turned-against-artificial-intelligence-ai">Joseph Weizenbaum invented ELIZA</a>, the first-ever chatbot. ELIZA was a conversation chatbot, programmed to act as a psychotherapist for users by scanning for keywords and mirroring them back to the user in response, according to <a href="https://www.cbc.ca/radio/quirks/eliza-chatbot-1.7459487">CBC</a>.</p>
<p>Since ELIZA, these tools have gotten much more sophisticated, particularly in the last few years. Now, there are far more chatbots specifically designed for mental health support — including <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://mental.jmir.org/2017/2/e19/" href="https://mental.jmir.org/2017/2/e19/">Woebot</a>, <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://mhealth.jmir.org/2018/11/e12106/" href="https://mhealth.jmir.org/2018/11/e12106/">Wysa</a>, and <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://home.dartmouth.edu/news/2025/03/first-therapy-chatbot-trial-yields-mental-health-benefits" href="https://home.dartmouth.edu/news/2025/03/first-therapy-chatbot-trial-yields-mental-health-benefits">Therabot</a>, which all rely on evidence-based treatments and have been shown to reduce users’ depression symptoms.</p>
<p>According to <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://psychiatryonline.org/doi/10.1176/appi.pn.2022.05.4.50" href="https://psychiatryonline.org/doi/10.1176/appi.pn.2022.05.4.50">Psychiatry Online</a>, more and more people are flocking to these types of mental health tools, which experts say may be worthwhile.</p>
<figure><picture><img alt="Close up of woman using smart phone" sizes="100vw" src="https://assets.teenvogue.com/photos/6806e486d1d6608de3f4bbf3/master/w_775%2Cc_limit/GettyImages-1430317161.jpg" srcset="https://assets.teenvogue.com/photos/6806e486d1d6608de3f4bbf3/master/w_120,c_limit/GettyImages-1430317161.jpg 120w, https://assets.teenvogue.com/photos/6806e486d1d6608de3f4bbf3/master/w_240,c_limit/GettyImages-1430317161.jpg 240w, https://assets.teenvogue.com/photos/6806e486d1d6608de3f4bbf3/master/w_320,c_limit/GettyImages-1430317161.jpg 320w, https://assets.teenvogue.com/photos/6806e486d1d6608de3f4bbf3/master/w_640,c_limit/GettyImages-1430317161.jpg 640w"/></picture></figure>
<p><a data-testid="ContentCardEmbedHedLink" href="https://www.teenvogue.com/story/how-students-using-chatgpt-therapy-breakups">Some Students Are Using ChatGPT to Text Their Own Friends</a></p>
<p>9 college students share how they use Chat.</p>
<p>“So long as [the tool] is built by providers who are well-informed, I think it could absolutely be helpful, particularly because nationally we are very, very, very understaffed in terms of the amount of providers who specialize in eating disorders,” said <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.weflourishpsychology.com/dr-kelli-rugless-psyd" href="https://www.weflourishpsychology.com/dr-kelli-rugless-psyd">Kelli Rugless</a>, a licensed psychologist and clinical advisor at <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.theprojectheal.org/our-mission" href="https://www.theprojectheal.org/our-mission">Project Heal</a>, a nonprofit aimed at breaking down systematic, financial, and healthcare barriers to eating disorder treatment. “Every day we’re encountering all of the barriers that exist for people to get access to healing, and so the idea of an AI chatbot is actually very promising in that it could give folks who would never have the ability to access services some level of support.”</p>
<p>AI can be especially appealing to teens who grew up in environments where mental health was stigmatized and who don’t feel comfortable asking for help or <a href="https://www.teenvogue.com/story/how-to-find-therapist-parents-wont-help">seeking their parents’ permission</a> to get therapy. Or, for people who want to take their time in talking about mental health because a bot won’t make them feel rushed.</p>
<p>And, these tools can motivate people who may otherwise have abandoned hope to get care. In March, clinical psychologist Gemma Sharp published <a href="https://jeatdisord.biomedcentral.com/articles/10.1186/s40337-025-01225-x">a study</a> looking at how a chatbot provided single-session interventions for people on <a href="https://pubmed.ncbi.nlm.nih.gov/39431573/">long waitlists</a> for eating disorder treatment. Sharp co-designed the chatbot with other psychologists, as well as people who had recovered from an eating disorder.</p>
<p>“We know that if we leave people on waitlists, <a href="https://www.sciencedirect.com/science/article/pii/S1471015324000576">they're more likely to drop out</a> when treatment does become available, and unfortunately, their condition may deteriorate,” said Sharp, a professor at Monash University in Australia. “And I always think if you can get someone in when the motivation is there, it's very, very important because often people feel quite ambivalent about starting eating disorder treatment.”</p>
<aside data-event-boundary="click" data-event-click='{"pattern":"PullquoteEmbed"}' data-in-view='{"pattern":"PullquoteEmbed"}' data-include-experiments="true" data-testid="pullquote-embed"><p>"In many ways, this is still kind of a big Wild West."</p></aside>
<p>Sharp — who also created a publicly available chatbot named “<a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://nedic.ca/jem/" href="https://nedic.ca/jem/">JEM</a>” — is intentional about making sure her chatbots’ avatars are gender nonconforming and nonhuman. “In an eating disorder setting, there’s always that tendency to compare one’s body to another body, even a chatbot avatar’s,” said Sharp, whose research <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.jmir.org/2021/6/e27807/pdf" href="https://www.jmir.org/2021/6/e27807/pdf">has shown</a> that users appreciate this design element.</p>
<p>One of the many advantages of chatbots designed by clinicians and researchers, as opposed to generative AI tools such as ChatGPT, is that they’re built with specific audiences in mind and have privacy and safety guardrails in place. They are also “predictive,” or “rule-based,” meaning they feature vetted, evidence-based data from the clinicians and researchers who designed them.</p>
<p>That said, they haven’t always been foolproof.</p>
<p>In 2023, for instance, the National Eating Disorders Association <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.nytimes.com/2023/06/08/us/ai-chatbot-tessa-eating-disorders-association.html" href="https://www.nytimes.com/2023/06/08/us/ai-chatbot-tessa-eating-disorders-association.html">came under scrutiny</a> after shutting down its national helpline and announcing that a chatbot would take its place. The chatbot, named Tessa, ended up giving weight-loss advice to users struggling with eating disorders. NEDA suspended the chatbot, which was designed to be rule-based but <a href="https://www.npr.org/sections/health-shots/2023/06/08/1180838096/an-eating-disorders-chatbot-offered-dieting-advice-raising-fears-about-ai-in-hea">ended up having a generative AI feature</a>.</p>
<p>“I think Tessa was an unfortunate situation but also <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.jmir.org/2023/1/e50696/" href="https://www.jmir.org/2023/1/e50696/">an excellent lesson</a> for us all in the field. In our research and initiatives, we took a step back and started to look at safety with an even more critical lens,” Sharp said. “If you make sure that you have the ethicists there, the developers there, as well as researchers, clinicians, and people with lived experience who are all part of the co-design stage, then you have all the knowledge there to build the safest innovation.”</p>
<p>Mehek Mohan, co-founder of <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.getkahani.com/" href="https://www.getkahani.com/">Kahani</a>, a digital recovery companion app, shared similar sentiments.</p>
<p>“We know that with every new technology, there is no way that everything will go perfectly all at once. There is a learning curve, and we’ve already seen exponential <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.vox.com/future-perfect/394336/artificial-intelligence-openai-o3-benchmarks-agi" href="https://www.vox.com/future-perfect/394336/artificial-intelligence-openai-o3-benchmarks-agi">improvement</a>,” Mohan said. “At the end of the day, we as humans crave to feel seen and heard, and the beauty of what AI can do is make you feel that way whenever you feel like you need to feel that way. And there is something really powerful about increasing that type of access to real-time care that has never existed before.”</p>
<p>Mohan and co-founder Brandon Chaffee created Kahani (which means “story” in Hindi), to help people with eating disorders stay committed in their recovery — particularly in between sessions with their care team or when transitioning from higher to lower levels of care. The app, which was designed with input from clinicians, gamifies evidence-based treatments to help users reframe thoughts, build sustainable coping strategies, and rewrite narratives that no longer serve them well.</p>
<figure>Businesswoman using technology smart chatbot AIKrongkaew</figure>
<p>If a well-built, well-intentioned chatbot can be a flawed but at times helpful mental health tool, a chatbot with no mental health “training” can be disastrous.</p>
<p>AI chatbots and apps that are built in coordination with mental health experts are designed to help users reframe unhelpful thought patterns. But other AI apps — including Character.AI and Replika — <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://time.com/7209824/replika-ftc-complaint/" href="https://time.com/7209824/replika-ftc-complaint/">have been criticized</a> for reinforcing, rather than challenging, users’ toxic beliefs. These apps allow users to create fictional AI characters and interact with those created by others.</p>
<p><a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.nytimes.com/2025/02/24/health/ai-therapists-chatbots.html" href="https://www.nytimes.com/2025/02/24/health/ai-therapists-chatbots.html">In two high-profile cases</a>, parents in Florida and Texas have filed lawsuits against Character.AI after their teenage children interacted with chatbots that masqueraded as therapists and shared harmful advice. After conversing with these bots, one of the teens allegedly grew violent toward his parents, and the other died by suicide. The American Psychological Association has since urged the Federal Trade Commission to investigate the deceptive practices of unregulated generative AI technologies. At the same time, the association <a href="https://www.npr.org/sections/shots-health-news/2025/04/07/nx-s1-5351312/artificial-intelligence-mental-health-therapy">has praised efforts</a> led by subject matter experts, including the first <a href="https://ai.nejm.org/doi/full/10.1056/AIoa2400802">randomized clinical trial for AI therapy</a>.</p>
<p><a href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=graphika+report+eaitng+disorder+coaches&amp;ie=UTF-8&amp;oe=UTF-8">A new report</a> by Graphika, a social network analysis firm, looked at how people with eating disorders are turning to AI character chatbots to create “anorexia coaches” that encourage them to engage in extreme disordered eating behaviors through demeaning rhetoric.</p>
<p>“This ended up being the most sophisticated and sprawling online community I’ve ever studied,” said Erin McAweeney, Director of Intelligence at Graphika. This AI community, she said, has found ways to circumvent guardrails and moderation. And it’s just the latest iteration of <a href="https://www.teenvogue.com/story/thinspiration-pro-ana-social-media">longstanding</a> “pro-anorexia” content that has existed online for decades.</p>
<p>But for as prevalent as these toxic “pro-anorexia” chatbots are, McAweeney also noted the existence of “pro-recovery” chatbots. “We were really focused on harms for this report,” she said. “But when I was looking for any use of terms related to eating disorders or self-harm, there were also examples of chatbots that were supporting people, discouraging harmful behaviors, or offering alternatives.”</p>
<aside data-event-boundary="click" data-event-click='{"pattern":"PullquoteEmbed"}' data-in-view='{"pattern":"PullquoteEmbed"}' data-include-experiments="true" data-testid="pullquote-embed"><p>"You just need that one-on-one communication with someone who can dissect your thoughts and basically be like, ‘OK, you’re feeling this, let’s get to the root of it.’”</p></aside>
<p>The co-existence of both types of communities speaks to an important truth: AI is neither all good nor all bad, and there is still so much to learn about its role in mental health. <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.psychologytoday.com/us/blog/artificial-intelligence-in-behavioral-and-mental-health-care/202402/could-artificial" href="https://www.psychologytoday.com/us/blog/artificial-intelligence-in-behavioral-and-mental-health-care/202402/could-artificial">Despite debates</a> about whether AI could one day “replace” actual providers, the psychologists I spoke with are hopeful about AI’s potential to help both patients and providers.</p>
<p>“In many ways, this is still kind of a big Wild West,” said David Luxton, a clinical psychologist and author of the book “<a href="https://www.sciencedirect.com/book/9780124202481/artificial-intelligence-in-behavioral-and-mental-health-care">Artificial Intelligence in Behavioral and Mental Health Care</a>.” “All behavioral health professionals need to advocate for our profession, and we need to make sure that people are safe when they're using these tools, and that they're informed of the limits and the risks. And we need to be involved in development.”</p>
<p>Luxton and his team <a href="https://www.apa.org/monitor/2025/01/trends-harnessing-power-of-artificial-intelligence">are developing a specialized chatbot</a> to help primary care providers assess and screen patients for suicide risk. He’s also writing an American Psychological Association guidebook on putting AI into practice. Due out in 2026, the guidebook will address a variety of topics, including how therapeutic chatbots can build rapport with clients, as well as privacy issues related to AI tools.</p>
<p>As with most AI use, many say the best case scenario is a blend of human and tech use. The experts I spoke to said AI is best used not as a replacement for real-life therapists but as a supplemental tool when someone needs additional support — in between therapy sessions, for instance, or while on waitlists to receive care.</p>
<p>Rugless, the psychologist from Project Heal, encourages teens to consider all their options when seeking mental health support. “Any access to something that could be healing and could give a person the sense that they’re not alone and that there is an answer to what they’re dealing with is better than nothing,” she said. “But if you have people who care about you and love you … then why not start there? Why not let them collaborate with you to find an answer, rather than skipping that really important step and moving into the world of AI?”</p>
<figure><picture><img alt=": A student student sits in a lecture hall while class is being dismissed at the University of Texas at Austin on February 22, 2024 in Austin, Texas." sizes="100vw" src="https://assets.teenvogue.com/photos/67b6572cb8016dab8c5819de/master/w_775%2Cc_limit/GettyImages-2031141571.jpg" srcset="https://assets.teenvogue.com/photos/67b6572cb8016dab8c5819de/master/w_120,c_limit/GettyImages-2031141571.jpg 120w, https://assets.teenvogue.com/photos/67b6572cb8016dab8c5819de/master/w_240,c_limit/GettyImages-2031141571.jpg 240w, https://assets.teenvogue.com/photos/67b6572cb8016dab8c5819de/master/w_320,c_limit/GettyImages-2031141571.jpg 320w, https://assets.teenvogue.com/photos/67b6572cb8016dab8c5819de/master/w_640,c_limit/GettyImages-2031141571.jpg 640w"/></picture></figure>
<p><a data-testid="ContentCardEmbedHedLink" href="https://www.teenvogue.com/story/gen-z-gen-alpha-chatgpt-schools">Here’s How Gen Z and Gen Alpha Are Actually Using ChatGPT at School</a></p>
<p>Use of AI for schoolwork is on the rise.</p>
<p>Eighteen-year-old Katelyn Olmsted, <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://www.amazon.com/Daily-Dos-Eating-Disorder-Recovery/dp/B0CP6R78Y9" href="https://www.amazon.com/Daily-Dos-Eating-Disorder-Recovery/dp/B0CP6R78Y9">who is in recovery from anorexia</a>, has found it helpful to rely on a mix of human interaction and AI tools. After spending three months in residential treatment, Olmsted needed help maintaining the sense of routine that around-the-clock care provided. In addition to seeing a therapist, Olmsted took advantage of <a data-event-boundary="click" data-event-click='{"pattern":"ExternalLink"}' data-in-view='{"pattern":"ExternalLink"}' data-include-experiments="true" data-offer-url="https://help.snapchat.com/hc/en-us/articles/13266788358932-What-is-My-AI-on-Snapchat-and-how-do-I-use-it#:~:text=Content%20shared%20with%20My%20AI%2C%20including%20your%20location%20if%20you,personalize%20your%20experience%2C%20including%20ads." href="https://help.snapchat.com/hc/en-us/articles/13266788358932-What-is-My-AI-on-Snapchat-and-how-do-I-use-it#:~:text=Content%20shared%20with%20My%20AI%2C%20including%20your%20location%20if%20you,personalize%20your%20experience%2C%20including%20ads.">Snapchat’s AI feature</a> by asking it to create a day-to-day schedule that included times for meals and snacks. She has also used it to generate daily affirmations that helped her stay motivated in recovery.</p>
<p>“AI was really helpful for organizing my thoughts around, OK, I'm in recovery. How can I stay consistent and keep my mind busy throughout the day without falling into triggers and slips and lapses?” Olmsted said. “As much as I like AI for some of those reasons, I also think it’s important to see therapists and professionals. You just need that one-on-one communication with someone who can dissect your thoughts and basically be like, ‘OK, you’re feeling this, let’s get to the root of it.’”</p>
<p>Some teenagers, including 19-year-old Noah Diaz, remain skeptical. “I have never used AI to support my own mental health needs, and I’m personally against AI,” said Diaz, a psychology major at the University of Texas at Austin. “I've seen TikToks and articles about how teens are using it to diagnose themselves, or to be their therapist, but it’s not giving them that human interaction that they need.”</p>
<p>Diaz does understand why AI might be a more accessible path for some people to get help, but remains against it because of AI’s <a href="https://docs.google.com/document/d/1DQ7DMwEVy4vx48KmQZOyZPfUojPaa-76/edit#:~:text=Explained%3A%20Generative%20AI%E2%80%99s%20environmental%20impact%20%7C%20MIT%20News%20%7C%20Massachusetts%20Institute%20of%20Technology">environmental impact</a>.</p>
<p>When I asked Santos — the Loyola University student who puts her journal entries into ChatGPT — about privacy, she didn’t express concerns.</p>
<p>“There are so many people using it, and there’s kind of an anonymous aspect to it, so I don’t worry that anything could be traced back to me,” said Santos, a computer science major. “Though I’m sharing my own experiences and problems, I think some of the root issues are relatable to a lot of people my age. If <a href="https://www.teenvogue.com/story/gen-z-gen-alpha-chatgpt-schools">ChatGPT</a> is training its model with content like mine, maybe there’s some good that could come of it.”</p>
<p><strong>Want more on how teens are using AI chatbots?</strong></p>
<p><a href="https://www.teenvogue.com/story/how-students-using-chatgpt-therapy-breakups">How Are Students Using Chat GPT? For Therapy, Breakups, and Even Texting Friends</a></p>
<p><a href="https://www.teenvogue.com/story/gen-z-gen-alpha-chatgpt-schools"><strong>Here’s How Gen Z and Gen Alpha Are Actually Using ChatGPT in Schools</strong></a></p>
<p><a href="https://www.teenvogue.com/story/chatgpt-college-admissions-essays"><strong>Using ChatGPT to Write Your College Essay Won’t Help You Get Into Your School of Choice</strong></a></p>
