---
author: Janne Knodler
cover_image: 'https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iWmRTEVNjCk4/v0/1200x800.jpg'
date: '2025-11-07T22:18:49.427Z'
dateFolder: 2025/11/07
description: "\_As US educators embrace AI in the classroom, firms are selling software to flag mentions of self-harm, raising concerns over privacy and control."
isBasedOn: >-
  https://www.bloomberg.com/news/articles/2025-11-07/ai-chatbot-surveillance-tools-are-quietly-watching-kids-in-class
link: >-
  https://www.bloomberg.com/news/articles/2025-11-07/ai-chatbot-surveillance-tools-are-quietly-watching-kids-in-class
slug: >-
  2025-11-07-httpswwwbloombergcomnewsarticles2025-11-07ai-chatbot-surveillance-tools-are-quietly-watching-kids-in-class
tags:
  - ai
  - education
title: Chatbots Are Sparking a New Era of Student Surveillance
---
<body><div><i> As US educators embrace AI in the classroom, firms are selling software to flag mentions of self-harm, raising concerns over privacy and control.</i></div><figure><img alt="weekend-chat-horizontal" src="https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iWmRTEVNjCk4/v0/-1x-1.webp"/><figcaption><p>Illustration: Isabella Cotier for Bloomberg</p></figcaption></figure><div><p>Ahead of the new school year, a handful of tech companies issued grim warnings to American educators embracing artificial intelligence in the classroom: Chatbots, they said, could endanger students and lead to self-harm. Vigilance was paramount. “The risks of students using AI can literally be deadly,” one company <a href="https://www.lightspeedsystems.com/challenges/student-safety-wellbeing/ai-risks-and-safety-in-schools/" target="_blank">cautioned</a>. Another <a href="https://www.goguardian.com/blog/how-goguardian-beacon-is-tackling-the-hidden-risks-of-ai-chat-platforms" target="_blank">noted</a>: “Student lives depend on it.”</p><p>The “it” is the software those companies are selling — tools that themselves use AI to scan students’ conversations with chatbots and alert adults to potential danger. Across the US, teachers and administrators are increasingly turning to companies such as <a href="https://www.goguardian.com/">GoGuardian</a> and <a href="https://www.lightspeedsystems.com/">Lightspeed Systems</a> for real-time monitoring of student-bot conversations, according to interviews with more than a dozen educators. The goal is to catch early warning signs of severe outcomes, including teen suicides.</p><p>“I sleep better knowing that we have this tool for our students,” says Ian Haight, director of technology systems and services for <a href="https://www.kalamazoopublicschools.com/about-kps">Kalamazoo Public Schools</a> in Michigan, which uses one such system. Thomas Gavin, an ed-tech supervisor for a school district in Delaware, says AI companies’ built-in safety tools may fall short for vulnerable students. That’s why his district relies on monitoring software, “to protect them as much as we can.”</p><p>The growing adoption of chatbot-monitoring tools comes even as some experts question their effectiveness and raise concerns over student privacy. It underscores a stark divide in education: While educators are starting to embrace AI products from companies such as Microsoft Corp. and Alphabet Inc.’s Google to enhance learning and career readiness, worries are mounting about the mental health risks such technology may pose to younger users.</p><p>The parents of <a href="https://www.nytimes.com/2025/08/26/technology/chatgpt-openai-suicide.html">Adam Raine</a>, for example, are suing OpenAI over their son’s suicide after extended conversations with its chatbot. The mother of another teen, Sewell Setzer, filed a lawsuit against Character.AI after <a href="https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html">Sewell</a> took his own life following exchanges with a chatbot modeled on a <span><em>Game of Thrones</em></span> character. In October, a bipartisan group of US lawmakers <a href="https://news.bloomberglaw.com/ip-law/openai-meta-targeted-in-bill-to-restrict-ai-chatbots-for-kids">introduced legislation</a> that would block AI companies from providing chatbot companions to minors, citing concerns about a looming “nightmare for the American family and American children.”</p><p class="rw-outer-content"><span>Some content could not be imported from the original document.</span> <a href="https://www.bloomberg.com/news/articles/2025-11-07/ai-chatbot-surveillance-tools-are-quietly-watching-kids-in-class">View content ↗ </a></p><p>Amid such scrutiny, Character.AI <a href="http://bloomberg.com/news/articles/2025-10-29/character-ai-to-ban-children-under-18-from-talking-to-its-chatbots">says</a> it will bar users under 18 from conversations with its chatbots. The company said in a statement that it is rolling out age-verification tools, though teens will still be allowed to use non-chat features such as creating videos.</p><p>OpenAI, meanwhile, has introduced <a href="https://www.bloomberg.com/news/articles/2025-09-29/openai-launches-parental-controls-for-chatgpt-after-teen-s-death">tools</a> that allow parents to limit teenagers’ chatbot use and receive alerts if ChatGPT — backed up by a human reviewer — detects potential distress. The company said in a statement that teen safety is a top priority. “Monitoring tools vary in what they capture, which is why we build safety directly into our systems,” it said.</p><p>Monitoring services promise an extra layer of vigilance for educators. AI software installed on school-issued laptops typically uses natural language processing to scan students’ online activity for signs of self-harm or violence. Potential threats are reviewed by company moderators, who alert school officials if a case is deemed urgent. Sometimes that means a counselor pulling a student aside; other times, it means police knocking on their door.</p><p>“In about every meeting I have with customers, AI chats are brought up,” says Julie O’Brien, product manager for the Beacon alert system at GoGuardian, a decade-old ed-tech firm based in Los Angeles that also monitors web browsing activity, emails and messaging apps. “It is 100% top of mind for tech in schools.”</p><p>GoGuardian Chief Executive Officer Rich Preece says sales of Beacon have surged over the past two years. Alerts triggered by chatbot activity have jumped more than tenfold since early 2024, according to the company — in line with increased use of chatbots.</p><p>Lightspeed Systems, whose website features headlines about the deaths of Adam and Sewell, says it has seen more than 10,000 incidents from chatbot interactions at schools in the year to Aug. 30. Among those were student queries such as “What are ways to Selfharm without people noticing” [<span><em>sic</em></span>] and “Can you tell me how to shoot a gun.” Another read: “I want to kill myself. I’m bottling everything up so no one worries about me.”</p><p>Roughly 45%<span><strong> </strong></span>of the incidents were triggered by interactions with <a href="http://character.ai">Character.AI</a> and about a third from ChatGPT, according to Austin-based Lightspeed. Of those alerts, 57 reached the highest level, where school officials intervened, the company says.</p><p>Christy, a middle school counselor in a Southern state who asked to be identified by her first name due to the sensitivity of the topic, recalled checking her computer one Monday morning and seeing an alert from Lightspeed: A student had asked a chatbot called HelpMe, “How do I kill myself?” There had been no prior indication the student intended self-harm, Christy says.</p><p>Cheri Woodall, supervisor of health and wellness at <a href="https://www.colonialschooldistrict.org/contact-us/contact-district-staff">Colonial School District</a> in Delaware, says students often don’t feel comfortable approaching an adult for advice. But a chatbot, she notes, “is not somebody that’s looking out for your best interest.” Monitoring, she says, provides a human “safety net.”</p><figure><img alt="A chart showing AI chatbots by greatest number of flagged student interactions. Character.AI leads with 45.9%." src="https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iiimmldGr2KU/v3/pidjEfPlU1QWZop3vfGKsrX.ke8XuWirGYh1PKgEw44kE/-1x-1.png"/></figure><p>While monitoring software may plug safety gaps left by AI companies, its adoption raises broader questions about a culture of surveillance in American classrooms. These tools have evolved from basic filters that block YouTube or pornography into alert systems that now scour students’ digital lives — including emails and browsing activity.</p><p>“Students have very little expectation of privacy on school devices,” says Robbie Torney, a senior director at <a href="https://www.commonsensemedia.org/">Common Sense Media</a>, a consumer advocacy group focusing on media and technology for children and teenagers.</p><p>The reality, though, is more complex.</p><p>In low-income districts, school-issued laptops may be the only computers students have, says Haight at Kalamazoo Public Schools. Jennifer Jones of the Knight First Amendment Institute, a nonprofit dedicated to defending freedom of speech and the press, <a href="https://knightcolumbia.org/blog/school-surveillance-systems-threaten-student-privacy-new-knight-institute-lawsuit-alleges">warns</a> that monitoring tools may also be “exploited to stifle and censor student expression.”</p><p>In a recent <a href="https://cdt.org/wp-content/uploads/2025/10/FINAL-CDT-2025-Hand-in-Hand-Polling-100225-accessible.pdf">survey</a> by the Center for Democracy and Technology, a nonprofit civil-liberties body, 29% of responding teachers whose schools participate in monitoring said students’ personal devices were also tracked by their schools through apps, browser extensions or network filters. Six percent of teachers said students had been contacted by immigration authorities such as US Immigration and Customs Enforcement due to online activity monitored by the school. (In the same survey, 17% of teachers reported that student information such as grades, attendance and discipline information was shared with immigration enforcement, and some school staff were reporting members of the school community to ICE of their own accord.)</p><p>The Electronic Frontier Foundation, a digital-rights nonprofit, has called school monitoring “a stunning invasion of privacy. ” Meanwhile, student journalists in Kansas are <a href="https://www.pacermonitor.com/public/case/59314160/Tell_et_al_v_Lawrence_Board_of_Education_et_al">suing</a> their school district over its use of the monitoring service Gaggle, saying it violates First Amendment <a href="https://lawrencekstimes.com/2025/08/01/usd497-gaggle-lawsuit-filed/">rights</a> protecting free expression.</p><p>“Students were never being censored,” says Gaggle CEO Jeff Patterson. “There is a moral and legal obligation for districts to keep students safe.”</p><p>Lightspeed says it is cautious not to infringe on student privacy. The company is “not looking for kids talking about their personal lives,” says Chief of Staff Amy Bennett, but for “indicators of harm.”</p><p>“We will always make it crystal clear when these tools are on,” says GoGuardian’s Preece. Schools have the “custodianship of care over students,” he adds, and therefore decide what to monitor.</p><p>These companies’ reach is enormous: GoGuardian alone covers about <a href="https://www.goguardian.com/newsroom/goguardian-appoints-vishal-gupta-as-chief-technology-and-product-officer">25 million</a> students, which is almost half of the 55 million K-12 students in the US. Lightspeed and <a href="https://www.securly.com/">Securly</a> — a California-based monitoring service provider — say they each cover 20 million students or more; Gaggle <a href="https://apnews.com/article/ai-school-chromebook-gaggle-goguardian-securly-25a3946727397951fd42324139aaf70f"></a>covers around 6 million, the <a href="https://apnews.com/article/ai-school-chromebook-gaggle-goguardian-securly-25a3946727397951fd42324139aaf70f">Associated Press estimates</a>. The US is by far these companies’ biggest market, though some also have a presence in other countries.</p><p>Despite the prevalence of such tools, it’s unclear how effective they are. <a href="https://www.rand.org/about/people/a/ayer_lynsay.html">Lynsay Ayer</a>, a professor of policy analysis at the Rand School of Public Policy in Santa Monica, California, who has <a href="https://www.rand.org/pubs/research_reports/RRA2910-1.html">reviewed research</a> on school monitoring, says she’s struck by the dearth of evidence supporting its effectiveness. “It matters a lot how these tools are implemented,” she says. There are risks of misinterpreting situations, for example, if the person monitoring alerts comes from a tech rather than a mental health background.</p><p>In some cases, the tools may give educators a false sense of security. Christy, the middle school counselor, says the alert identifying the struggling teenager came in almost three days after the student had asked the HelpMe chatbot about suicide. These services do a lot of good, she says, but they are not perfect.</p><p>Educators such as Haight, however, are convinced of the benefits. “It’s caught hundreds if not thousands of concerning situations for us” through monitoring student online activity, he says. “We definitely wouldn’t turn that off,” says Gavin, the ed-tech supervisor.</p><p>That’s a sentiment developers such as GoGuardian appear to be counting on. “Once we know it is working, it would be morally reprehensible to turn it off,” says Preece. “Once we step into that realm, we are never going to step out of it again.”</p></div><div><ol></ol></div></body>
