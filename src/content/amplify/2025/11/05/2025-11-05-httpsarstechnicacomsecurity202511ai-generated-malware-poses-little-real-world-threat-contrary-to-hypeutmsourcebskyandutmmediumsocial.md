---
author: Dan Goodin
cover_image: >-
  https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-1000x648.jpg
date: '2025-11-05T23:02:00.881Z'
dateFolder: 2025/11/05
description: 'You wouldn’t know it from the hype, but the results fail to impress.'
isBasedOn: >-
  https://arstechnica.com/security/2025/11/ai-generated-malware-poses-little-real-world-threat-contrary-to-hype/?utm_source=bsky&utm_medium=social
link: >-
  https://arstechnica.com/security/2025/11/ai-generated-malware-poses-little-real-world-threat-contrary-to-hype/?utm_source=bsky&utm_medium=social
slug: >-
  2025-11-05-httpsarstechnicacomsecurity202511ai-generated-malware-poses-little-real-world-threat-contrary-to-hypeutmsourcebskyandutmmediumsocial
tags: []
title: 5 malware families analyzed by Google fail to work and are easily detected
---
<figure></figure><p>  Not remotely ready for prime time </p>
<p>You wouldn’t know it from the hype, but the results fail to impress.</p>
<p><a href="https://arstechnica.com/author/dan-goodin/"> Dan Goodin </a> –  Nov 5, 2025 11:00 PM |</p>
<figure><a data-cropped="true" data-pswp-height="667" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat.jpg 1000w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-640x427.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-768x512.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-980x654.jpg 980w" data-pswp-width="1000" href="https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat.jpg"><img alt="" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-1000x648.jpg"/></a></figure>
<p> / Credit: Getty Images</p>
<figure></figure><figure></figure><p>Google on Wednesday revealed five recent malware samples that were built using generative AI. The end results of each one were far below par with professional malware development, a finding that shows that vibe coding of malicious wares lags behind more traditional forms of development, which means it still has a long way to go before it poses a real-world threat.</p>
<p>One of the samples, for instance, tracked under the name PromptLock, was part of an <a href="https://arxiv.org/abs/2508.20444v1">academic study</a> analyzing how effective the use of large language models can be “to autonomously plan, adapt, and execute the ransomware attack lifecycle.” The researchers, however, reported the malware had “clear limitations: it omits persistence, lateral movement, and advanced evasion tactics” and served as little more than a demonstration of the feasibility of AI for such purposes. Prior to the paper’s release, security firm ESET said it had discovered the sample and <a data-ml="true" data-ml-dynamic="true" data-ml-dynamic-type="sl" data-ml-id="0" data-orig-url="https://www.eset.com/us/about/newsroom/research/eset-discovers-promptlock-the-first-ai-powered-ransomware/" data-skimlinks-tracking="xid:fr1762383726095gee" data-xid="fr1762383726095gee" href="https://www.eset.com/us/about/newsroom/research/eset-discovers-promptlock-the-first-ai-powered-ransomware/">hailed</a> it as “the first AI-powered ransomware.”</p>
<h2>Don’t believe the hype</h2>
<p>Like the other four samples Google analyzed—FruitShell, PromptFlux, PromptSteal, and QuietVault—PromptLock was easy to detect, even by less-sophisticated endpoint protections that rely on static signatures. All samples also employed previously seen methods in malware samples, making them easy to counteract. They also had no operational impact, meaning they didn’t require defenders to adopt new defenses.</p>
<p>“What this shows us is that more than three years into the generative AI craze, threat development is painfully slow,” independent researcher Kevin Beaumont told Ars. “If you were paying malware developers for this, you would be furiously asking for a refund as this does not show a credible threat or movement towards a credible threat.”</p>
<p>Another malware expert, who asked not to be named, agreed that Google’s report did not indicate that generative AI is giving developers of malicious wares a leg up over those relying on more traditional development practices.</p>
<figure><h3>Ars Video</h3><a href="https://www.arstechnica.com/video/watch/what-happens-to-the-developers-when-ai-can-code-ars-frontiers"><h3>What Happens to the Developers When AI Can Code? | Ars Frontiers</h3></a></figure>
<p>“AI isn’t making any scarier-than-normal malware,” the researcher said. “It’s just helping malware authors do their job. Nothing novel. AI will surely get better. But when, and by how much is anybody’s guess.”</p>
<p>The assessments provide a strong counterargument to the exaggerated narratives being trumpeted by AI companies, many seeking new rounds of venture funding, that AI-generated malware is widespread and part of a new paradigm that poses a current threat to traditional defenses.</p>
<p>A typical example is Anthropic, which <a href="https://www.anthropic.com/news/detecting-countering-misuse-aug-2025">recently reported</a> its discovery of a threat actor that used its Claude LLM to “develop, market, and distribute several variants of ransomware, each with advanced evasion capabilities, encryption, and anti-recovery mechanisms.” The company went on to say: “Without Claude’s assistance, they could not implement or troubleshoot core malware components, like encryption algorithms, anti-analysis techniques, or Windows internals manipulation.”</p>
<p>Startup ConnectWise <a href="https://www.connectwise.com/blog/the-dark-side-how-threat-actors-are-using-ai">recently said</a> that generative AI was “lowering the bar of entry for threat actors to get into the game.” The post cited a <a href="https://cdn.openai.com/threat-intelligence-reports/influence-and-cyber-operations-an-update_October-2024.pdf">separate report</a> from OpenAI that found 20 separate threat actors using its ChatGPT AI engine to develop malware for tasks including identifying vulnerabilities, developing exploit code, and debugging that code. BugCrowd, meanwhile, <a href="https://www.bugcrowd.com/resources/report/inside-the-mind-of-a-hacker/">said</a> that in a survey of self-selected individuals, “74 percent of hackers agree that AI has made hacking more accessible, opening the door for newcomers to join the fold.”</p>
<p>In some cases, the authors of such reports note the same limitations noted in this article. Wednesday’s report from Google says that in its analysis of AI tools used to develop code for managing command and control channels and obfuscating its operations “we did not see evidence of successful automation or any breakthrough capabilities.” OpenAI said much the same thing. Still, these disclaimers are rarely made prominently and are often downplayed in the resulting frenzy to portray AI-assisted malware as posing a near-term threat.</p>
<p>Google’s report provides at least one other useful finding. One threat actor that exploited the company’s Gemini AI model was able to bypass its guardrails by posing as white-hat hackers doing research for participation in a capture-the-flag game. These competitive exercises are designed to teach and demonstrate effective cyberattack strategies to both participants and onlookers.</p>
<p>Such guardrails are built into all mainstream LLMs to prevent them from being used maliciously, such as in cyberattacks and self-harm. Google said it has since better fine-tuned the countermeasure to resist such ploys.</p>
<p>Ultimately, the AI-generated malware that has surfaced to date suggests that it’s mostly experimental, and the results aren’t impressive. The events are worth monitoring for developments that show AI tools producing new capabilities that were previously unknown. For now, though, the biggest threats continue to predominantly rely on old-fashioned tactics.</p>
<figure><a href="https://arstechnica.com/author/dan-goodin/"><img alt="Photo of Dan Goodin" src="https://cdn.arstechnica.net/wp-content/uploads/2018/10/Dang.jpg"/></a></figure>
<p><a href="https://arstechnica.com/author/dan-goodin/"> Dan Goodin </a> Senior Security Editor</p>
<p>Dan Goodin is Senior Security Editor at Ars Technica, where he oversees coverage of malware, computer espionage, botnets, hardware hacking, encryption, and passwords. In his spare time, he enjoys gardening, cooking, and following the independent music scene. Dan is based in San Francisco. Follow him at <a href="https://infosec.exchange/@dangoodin">here</a> on Mastodon and <a href="https://bsky.app/profile/dangoodin.bsky.social">here</a> on Bluesky. Contact him on Signal at DanArs.82.</p>
