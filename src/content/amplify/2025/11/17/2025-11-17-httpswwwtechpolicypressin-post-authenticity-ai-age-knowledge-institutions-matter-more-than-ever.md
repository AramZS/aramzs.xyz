---
author: John Wihbey
cover_image: >-
  https://cdn.sanity.io/images/3tzzh18d/production/e09d24d880924b5e479f7bce327b13f686577aaa-1200x675.png
date: '2025-11-17T05:41:26.917Z'
dateFolder: 2025/11/17
description: >-
  The pressing question is whether we can rebuild epistemic infrastructure
  before norms of authenticity decay beyond repair, writes John Wihbey.
isBasedOn: >-
  https://www.techpolicy.press/in-post-authenticity-ai-age-knowledge-institutions-matter-more-than-ever/
link: >-
  https://www.techpolicy.press/in-post-authenticity-ai-age-knowledge-institutions-matter-more-than-ever/
slug: >-
  2025-11-17-httpswwwtechpolicypressin-post-authenticity-ai-age-knowledge-institutions-matter-more-than-ever
tags:
  - ai
title: 'In Post-Authenticity AI Age, Knowledge Institutions Matter More than Ever'
---
<p>Perspective</p>
<p><em>This perspective is part of a</em> <a href="https://www.techpolicy.press/how-political-power-is-capturing-knowledge-systems-and-manufacturing-structural-ignorance/techpolicy.press/category/provocations-for-the-university-of-pittsburghs-communication-technology-research-lab-ctrl-symposium-on-threats-to-knowledge-and-us-democracy"><em>series</em></a> <em>of provocations published on Tech Policy Press in advance of a symposium at the University of Pittsburgh's Communication Technology Research Lab (CTRL) on threats to knowledge and US democracy.</em></p>
<figure><img alt=" " data-nimg="1" src="https://cdn.sanity.io/images/3tzzh18d/production/e09d24d880924b5e479f7bce327b13f686577aaa-1200x675.png"/><figcaption>OpenAI launched <a href="https://openai.com/index/sora-2/">Sora 2</a>, its latest video generation model, on September 30, 2025. </figcaption></figure>
<p>The threat of deepfakes is well established. Scholars <a href="https://scholarship.law.bu.edu/faculty_scholarship/640/">have argued</a> that synthetic media is a challenge to privacy norms, democratic governance, and national security and examined its <a href="https://www.liebertpub.com/doi/full/10.1089/cyber.2021.29208.jth">social effects</a> and <a href="https://www.rand.org/pubs/perspectives/PEA3089-1.html">impact on information</a> integrity. Yet these analyses must increasingly take on the challenge of a cultural shift: mounting evidence that large segments of online audiences no longer care whether content is authentic. While the proposed solutions which have garnered the most attention are technical in nature, these are unlikely to fix a challenge rooted in eroding institutions of knowledge. At the same time, the public is increasingly distrustful or indifferent to those institutions. This puts solutions even further out of reach.</p>
<p>This indifference reflects not an inability to detect fakes but a waning concern for authenticity itself. When emotionally resonant or politically convenient narratives circulate, truth often becomes secondary. We appear to be entering a <em>post-authenticity</em> era in which the distinction between genuine and synthetic content loses its ability to constrain belief or guide action.</p>
<p>The power and ease of generative AI applications to produce photorealistic content at virtually no cost fuels this headlong rush toward a new epistemic situation. Memes, parodies, satires, and weaponized campaigns often blur the lines between deepfake and ironic, but malicious, attacks on truth and factual reality. Yes, many considered society to be racing toward a “post-truth” situation as of a decade ago, but we lacked scalable technologies that could so easily mimic reality. Many online users now may increasingly operate in a hybrid state of knowing and gullibility, entranced by the sheer interesting-ness of fake content that, in spirit, seems genuinely real, because it confirms beliefs, conforms to priors, or just appears to be a logical extension of online narratives.</p>
<p>The standard response to synthetic media has been technical—blockchain provenance systems, authentication protocols, watermarking, and detection algorithms. These tools presume that once authenticity can be verified, citizens will adjust their judgments accordingly. But that logic fails when audiences ignore authenticity markers or distrust the institutions behind them. The challenge with deepfakes and synthetic content is less about detection capabilities than about the <a href="https://ssrn.com/abstract=5275477">institutional capacity</a> to make those detections matter. Provenance systems are only as credible as the institutions that validate them and the publics that trust those institutions; a blockchain certificate of authenticity is meaningless if citizens distrust the certifier, misunderstand the technology, or no longer value the distinction it upholds. Our collective epistemic <a href="https://philpapers.org/archive/RINDAT.pdf">“backstop”</a>—shared norms for evaluating truth—depends on institutional legitimacy, not infrastructure alone.</p>
<p>Unfortunately, the institutions which provide such epistemic backdrops are eroding. Universities, libraries, journalism, and science—all traditional epistemic anchors—face financial and political strain. Social media platforms, meanwhile, have dismantled trust and safety teams, defunded civic integrity work, and embraced generative AI without proportional safeguards.</p>
<p>The result is that AI systems are now flooding networks with synthetic content at scale. The platforms that host it have abdicated responsibility for information quality, and the institutions once tasked with verification lack reach and legitimacy. Engagement-driven business models reward virality and emotion—traits that synthetic media cheaply optimize.</p>
<p>Together, these dynamics <a href="https://ssrn.com/abstract=4805026">amplify AI’s structural risks</a> to democratic knowledge. AI systems are inherently <em>epistemically anachronistic</em>—trained on the past and incapable of modeling the emergent, deliberative character of democracy. As they increasingly mediate what citizens see and believe, they create feedback loops that weaken the cognitive foundations of self-government. We are likely to see consequences as everything from polling to journalism, content moderation to content generation, increasingly rely on AI technologies that favor mean tendencies in the data distribution, crowding out marginal perspectives and creative, emergent, or unorthodox angles and views.</p>
<p>Addressing this crisis requires sustained pressure on social media companies to become active stewards—rather than passive distributors—of authoritative information. Beyond moderation, this means investments by platform companies: subsidizing journalism (at arm’s length), supporting local news ecosystems, prioritizing verified sources, and maintaining human oversight. More dollars in the creator economy should flow to fact-based and nonfiction media efforts. Technical interventions remain <a href="https://cset.georgetown.edu/publication/deepfakes-a-grounded-threat-assessment">vital and plausible</a> but are futile without institutional renewal. Many existing institutional problems, public policy failures, and societal deficits are simply <a href="https://digitalcommons.law.umaryland.edu/cgi/viewcontent.cgi?article=3837&amp;context=mlr">made worse by deepfakes</a>, but perhaps the crisis will finally underscore the need for major redress and could be the catalyst for political change.</p>
<p>Humans are not naturally gullible; we <a href="https://press.princeton.edu/books/hardcover/9780691178707/not-born-yesterday?srsltid=AfmBOoqz43NR2y_ZCrtYPBqvNyt4gyvs0uRCmnlGOrb4iwPqz5GAAW8M">evolved strong mechanisms</a> for evaluating credibility. Yet these depend on an information ecosystem rich in trustworthy institutions and credible choices. To preserve democratic knowledge, those institutions must innovate and compete within the attention economy. The pressing question is whether we can rebuild this epistemic infrastructure before norms of authenticity decay beyond repair.</p>
