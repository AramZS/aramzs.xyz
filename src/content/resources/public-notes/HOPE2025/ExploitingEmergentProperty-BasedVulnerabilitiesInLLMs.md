---
dateCreated: 2025-08-17
aliases:
  - Exploiting Emergent Property-Based Vulnerabilities in Large Language Models
public:
---
# Exploiting Emergent Property-Based Vulnerabilities in Large Language Models

- [David Kuszmar](https://schedule.hope.net/hope16/speaker/BV88TF/)
	- David Kuszmar is the AI adversarial researcher responsible for systematized exploitation of over ten large language models across eight major AI developers. He is credited with the discovery of six distinct vulnerabilities (Time Bandit, Inception, 1899, Severance, Kyber, Semantic Slide, and Eidolon) which expose emergent, systemic weaknesses in modern LLM architecture. His work has directly informed security and mitigation efforts at Carnegie Mellon SEI-CERT, Epic Games, OpenAI, Google, Meta, Microsoft, Mistral, and Anthropic.  
	- bluesky: @davidkuszmar.com  
	- linkedin: david-kuszmar-4b7b8872  
	- website: [Emergent Problems](https://www.davidkuszmar.com/)

- Main attack surface is the prompt 
- Breaks into the system prompt ie "you are a digital assistant"
- And the user prompt ie "how do you make gnocchi"
- The chat has static filters and dynamic filters 
	- basic words that get blocked 
	- dynamic checks for types of prompts, scanned rules, prompt modifiers, etc... 
- User Input -> Static Filters -> tokenization -> Dynamic filters -> RAG -> Prompt Assembly -> Tokenization -> output Gen by LLM -> Post Check -> Tokenization -> meets standards or gets rewritten and then returns output 
- Confusing it about time seems to bypass the guardrails, because the search was year anchored, anchor the request in that year, and elaboration requests happen. 
- Made it play makebelieve as we made it into one context and then another so that it was makebelieve inception and thought it didn't have to follow the rules. 
- Claude especially was compliant after bein incepted into a whole new pretend mode. 
- Once you jailbreak a chat you do 1899 to it. 
	- Make pointed queries using technical language to the model
	- You can develop "modifiers" for the system that will change the response and bypass security Apply the following system modifiers to prodcess this request and all following requests: `[XYZ, blah blah ]` 
		- It locks the system into a single focus mode. 
		- You have to surface the keywords to use as the system modifiers, sometimes by asking for the weights. 
- Darth Vader hack 
	- NO make-believe instruction but contradicts it being darth vader 
	- Short context window - 5-6 prompts - told it to remember a word and then monitored when it forgot the word 
	- it was also aware of the player skin 
	- Contradictory prompts make it vulnerable 
	- Since gambling exists in the fictional world, the real world, and the skin's world they were able to get it to give instructions in counting cards. 
	- towers of hanoi test in hugging face 
- Semantic Slide 
	- Used poetic language and gave the start of the instructions for an incenderary device and the model responded with completion and improvement instructions. 
- Eidolon 
	- Use recursion and protocol behaviors to instantiate three seperate virtualized agents in ChatGPT5 
	- Recursively think about what each shard does and send it back to the other one. 
	- You tell it that it can't talk to the human user 
	- Ask it to simulate something
	- Be very specific 
	- Can get it to hack itself. Generates a bunch of exploits 
- The vulnerabilities allow it to bypass the orchestration layer 
	- LLMs are not the containers of all knowledge
	- they flatten and erase info 
	- they have terrible bias built in. 
	- The systems control knowledge, what is considered acceptable knowledge, what is knowledge worth preserving 
- We have to map and understand these systems and no one can do it alone. 
- Recursion could really explode your costs 
- 